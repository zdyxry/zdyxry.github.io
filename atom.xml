<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yiran&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zdyxry.github.io/"/>
  <updated>2019-07-28T04:16:47.995Z</updated>
  <id>https://zdyxry.github.io/</id>
  
  <author>
    <name>yiran</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Heap In Python &amp; Golang</title>
    <link href="https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/"/>
    <id>https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/</id>
    <published>2019-07-28T04:11:45.000Z</published>
    <updated>2019-07-28T04:16:47.995Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近使用到了 heap 这个数据结构，记录一下在 Python 和 Golang 中最基本的使用方法～ </p><blockquote><p>堆（英语：Heap）是计算机科学中的一種特別的樹狀数据结构。若是滿足以下特性，即可稱為堆積：「給定堆積中任意節點P和C，若P是C的母節點，那麼P的值會小於等於（或大於等於）C的值」。若母節點的值恆小於等於子節點的值，此堆積稱為最小堆積（min heap）；反之，若母節點的值恆大於等於子節點的值，此堆積稱為最大堆積（max heap）。在堆積中最頂端的那一個節點，稱作根節點（root node），根節點本身沒有母節點（parent node）。</p></blockquote><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="create"><a href="#create" class="headerlink" title="create"></a>create</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: heapq.heapify(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: a</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: b = []</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: heapq.heappu</span><br><span class="line">heapq.heappush     heapq.heappushpop</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: heapq.heappush(b, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: heapq.heappush(b, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: heapq.heappush(b, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: heapq.heappush(b, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: heapq.heappush(b, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: b</span><br><span class="line">Out[<span class="number">11</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h3 id="read"><a href="#read" class="headerlink" title="read"></a>read</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">28</span>]: b</span><br><span class="line">Out[<span class="number">28</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: heapq.nlargest(<span class="number">3</span>, b)</span><br><span class="line">Out[<span class="number">29</span>]: [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: heapq.nsmallest(<span class="number">2</span>, b)</span><br><span class="line">Out[<span class="number">30</span>]: [<span class="number">1</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><h3 id="update"><a href="#update" class="headerlink" title="update"></a>update</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">32</span>]: heapq.heappush(b, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: b</span><br><span class="line">Out[<span class="number">33</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: heapq.heapreplace(b, <span class="number">7</span>)</span><br><span class="line">Out[<span class="number">39</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: b</span><br><span class="line">Out[<span class="number">40</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: a = [<span class="number">6</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">42</span>]: heapq.heapify(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: b</span><br><span class="line">Out[<span class="number">43</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: c = heapq.merge(a,b)</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: list(c)</span><br><span class="line">Out[<span class="number">46</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">50</span>]: b</span><br><span class="line">Out[<span class="number">50</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">51</span>]: heapq.heappop(b)</span><br><span class="line">Out[<span class="number">51</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: b</span><br><span class="line">Out[<span class="number">52</span>]: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="Golang"><a href="#Golang" class="headerlink" title="Golang"></a>Golang</h2><p>Golang 中没有 heapq 这种封装好的库可以直接使用，不过有 <code>container/heap</code> ，提供了同样的方法，只是我们需要先对我们的操作对象实现搜有的 <code>heap.Interface</code>  方法。</p><h2 id="Constructor"><a href="#Constructor" class="headerlink" title="Constructor"></a>Constructor</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// An IntHeap is a min-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> IntHeap []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Len</span><span class="params">()</span> <span class="title">int</span></span>           &#123; <span class="keyword">return</span> <span class="built_in">len</span>(h) &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h[i] &lt; h[j] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Swap</span><span class="params">(i, j <span class="keyword">int</span>)</span></span>      &#123; h[i], h[j] = h[j], h[i] &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Push</span><span class="params">(x <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="comment">// Push and Pop use pointer receivers because they modify the slice's length,</span></span><br><span class="line"><span class="comment">// not just its contents.</span></span><br><span class="line">*h = <span class="built_in">append</span>(*h, x.(<span class="keyword">int</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取最后一个元素，在 `container/heap.Pop` 中，将堆顶的元素放置在最后，然后调用 `container/heap.Down` 将当前堆顶元素下沉到适当位置。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Pop</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">old := *h</span><br><span class="line">n := <span class="built_in">len</span>(old)</span><br><span class="line">x := old[n<span class="number">-1</span>]</span><br><span class="line">*h = old[<span class="number">0</span> : n<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// // An IntHeap is a max-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> MaxHeap <span class="keyword">struct</span> &#123;</span><br><span class="line">IntHeap</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h MaxHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h.IntHeap[i] &gt; h.IntHeap[j] &#125;</span><br></pre></td></tr></table></figure><h3 id="create-1"><a href="#create-1" class="headerlink" title="create"></a>create</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// This example inserts several ints into an IntHeap, checks the minimum,</span></span><br><span class="line"><span class="comment">// and removes them in order of priority.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"minimum: %d\n"</span>, (*h)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="read-1"><a href="#read-1" class="headerlink" title="read"></a>read</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"minimum: %d\n"</span>, (*h)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">h1 := &amp;MaxHeap&#123;[]<span class="keyword">int</span>&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;&#125;</span><br><span class="line">heap.Init(h1)</span><br><span class="line">heap.Push(h1, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"maximum: %d\n"</span>, h1.IntHeap[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h1.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h1))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="update-1"><a href="#update-1" class="headerlink" title="update"></a>update</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="delete-1"><a href="#delete-1" class="headerlink" title="delete"></a>delete</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// An IntHeap is a min-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> IntHeap []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Len</span><span class="params">()</span> <span class="title">int</span></span>           &#123; <span class="keyword">return</span> <span class="built_in">len</span>(h) &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h[i] &lt; h[j] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Swap</span><span class="params">(i, j <span class="keyword">int</span>)</span></span>      &#123; h[i], h[j] = h[j], h[i] &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Push</span><span class="params">(x <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="comment">// Push and Pop use pointer receivers because they modify the slice's length,</span></span><br><span class="line"><span class="comment">// not just its contents.</span></span><br><span class="line">*h = <span class="built_in">append</span>(*h, x.(<span class="keyword">int</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Pop</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">old := *h</span><br><span class="line">n := <span class="built_in">len</span>(old)</span><br><span class="line">x := old[n<span class="number">-1</span>]</span><br><span class="line">*h = old[<span class="number">0</span> : n<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This example inserts several ints into an IntHeap, checks the minimum,</span></span><br><span class="line"><span class="comment">// and removes them in order of priority.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Println(h)</span><br><span class="line">heap.Remove(h, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Heap 的使用场景有：优先级队列、TopK 等等。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://godoc.org/container/heap" target="_blank" rel="noopener">https://godoc.org/container/heap</a></li><li><a href="https://docs.python.org/3.7/library/heapq.html" target="_blank" rel="noopener">https://docs.python.org/3.7/library/heapq.html</a></li><li><a href="https://ieevee.com/tech/2018/01/29/go-heap.html" target="_blank" rel="noopener">https://ieevee.com/tech/2018/01/29/go-heap.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近使用到了 heap 这个数据结构，记录一下在 Python 和 Golang 中最基本的使用方法～ &lt;/p&gt;
&lt;blockquote&gt;

      
    
    </summary>
    
    
      <category term="Python" scheme="https://zdyxry.github.io/tags/Python/"/>
    
      <category term="Golang" scheme="https://zdyxry.github.io/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>CentOS定制-软件源错误</title>
    <link href="https://zdyxry.github.io/2019/07/21/CentOS%E5%AE%9A%E5%88%B6-%E8%BD%AF%E4%BB%B6%E6%BA%90%E9%94%99%E8%AF%AF/"/>
    <id>https://zdyxry.github.io/2019/07/21/CentOS定制-软件源错误/</id>
    <published>2019-07-21T11:58:43.000Z</published>
    <updated>2019-07-21T12:00:33.172Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无法安装 OS 的问题，记录一下解决方式。</p><h2 id="定制-OS"><a href="#定制-OS" class="headerlink" title="定制 OS"></a>定制 OS</h2><p>关于定制 OS，在之前的博客中已经提到过几次了，CentOS 是比较容易改动的一个发行版，因为有着 RHEL 红（爸）帽（爸），有着完善的文档可以参考。</p><p>主要需要注意的是两点：  </p><ol><li>分区方式</li><li>软件包选择</li></ol><p>今天遇到的问题是第二点。</p><p>先说下前提，由于是 2B 产品，所以对于每次的 BaseOS 版本升级都非常谨慎，每次 BaseOS 版本都会进行各种测试。但是如果仅仅是升级部分所需要的软件包，就不用这么麻烦了，我们可以定制自己所需要的软件组（group），来进行安装/升级。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这次接到的需要是升级 Kernel、libiscsi、qemu 三个软件，后两个是虚拟化相关的，相关依赖较少；kernel 是跟 BaseOS 版本关联性很大的。</p><p>比如 CentOS 7.6 中，kernel 版本为：kernel-3.10.0-957.el7.x86_64.rpm，这个版本对 selinux 等相关软件是有依赖要求的，我在这里翻车了。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>像往常一样，将对应的 rpm 放置到了对应的 yum 源中，更新 yum 源，制作 ISO，在安装过程中报错：</p><img src="/2019/07/21/CentOS定制-软件源错误/os1.png" title="OS1"><p>报错显示是软件源出了问题，但是没有更多的信息了，这时候我们可以通过 console 连接到其他的 pty 中，查看对应的日志，比如 CentOS 默认的日志在： <code>/tmp/packaging.log</code> 中：</p><img src="/2019/07/21/CentOS定制-软件源错误/os2.png" title="OS2"><p>我们可以看到日志中提示 kernel 与当前软件源中的 selinux-policy-targeted 冲突，因为安装 OS 所用的软件源就是 ISO ，所以这里肯定是我们打包 ISO 时遗漏了依赖关系导致的，我们将对应的 Kernel 所需依赖更新，重新构建 ISO 就可以了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-踩坑记录（持续更新）</title>
    <link href="https://zdyxry.github.io/2019/07/13/Kubernetes-%E5%AE%9E%E6%88%98-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>https://zdyxry.github.io/2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/</id>
    <published>2019-07-13T01:34:45.000Z</published>
    <updated>2019-07-18T22:59:27.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更新。</p><h3 id="更新记录"><a href="#更新记录" class="headerlink" title="更新记录"></a>更新记录</h3><ul><li>2019.07.13 02:00 来自加班中的 yiran</li><li>2019.07.19 06:52 早起不想去公司的 yiran</li></ul><h2 id="coredns-无法解析域名"><a href="#coredns-无法解析域名" class="headerlink" title="coredns 无法解析域名"></a>coredns 无法解析域名</h2><p>在 Kubernetes 环境中，使用 kubeadm 工具部署的集群，会自动部署 coredns 作为集群的域名服务，每当我们创建了自己的 service，都可以通过域名直接访问，不用再考虑自己多个 Pod 的 IP 不同如何连接的问题。</p><p>最近遇到多个环境出现无法解析域名的问题，具体现象如下：</p><ol><li>集群部署完成后，部署 daemonset 资源，每个节点均运行一个 busybox；</li><li>在 busybox 中对 <code>kubernetes</code> 默认域名进行解析，查看解析结果。</li></ol><p>正常情况应该是所有的 busybox 都可以正常解析才对，但是最近几个环境中均出现了 3 个node 中1个node 上的 pod 无法解析的问题，示例代码如下：</p><p>daemonset.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:1.28.4"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 21:28:40 ~]<span class="variable">$for</span> i <span class="keyword">in</span> `kubectl get pod  -o wide  |grep ds | awk <span class="string">'&#123;print $1&#125;'</span>`;<span class="keyword">do</span> kubectl <span class="built_in">exec</span> <span class="variable">$i</span> nslookup kubernetes;<span class="built_in">echo</span> ;<span class="keyword">done</span></span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10</span><br><span class="line"></span><br><span class="line">nslookup: can<span class="string">'t resolve '</span>kubernetes<span class="string">'</span></span><br><span class="line"><span class="string">command terminated with exit code 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br></pre></td></tr></table></figure><p>在第一个节点的 Pod 解析时失效，最后命令执行 1min 超时退出。</p><p>经过查看发现节点的 NetFilter 相关系统配置未生效，导致 iptables 相关功能失效，具体可以参考 <a href="https://github.com/kubernetes/kubernetes/issues/21613" target="_blank" rel="noopener">issue</a>。</p><p>解决方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br></pre></td></tr></table></figure><h2 id="Flannel-OOM"><a href="#Flannel-OOM" class="headerlink" title="Flannel OOM"></a>Flannel OOM</h2><p>在配置好集群业务后，发现业务时不时的出现中断情况，最开始排查业务自身问题，未发现 Pod 出现重启或异常的日志，开始排查 k8s 状态，发现在节点 <code>/var/log/messages</code> 日志中，Flannel 一直处于 OOM 状态，惨不忍睹。</p><p>之前还略微惊奇，Flannel 默认的计算资源中，内存只要 50MiB，且上限也是 50MiB，没有给自己留一丝余地，看到 <a href="https://github.com/coreos/flannel/issues/963" target="_blank" rel="noopener">issue</a> 中的描述，感觉这个不是一个偶发事件，最终我将 Flannel 的内存调整为 250MiB 后，未出现 OOM 情况。</p><p>issue 中提到的 <code>kubectl patch</code> 命令未自动生效，我通过更新 ds 配置，然后依次手动删除节点上的 Flannel Pod 使其生效。</p><h2 id="Nginx-Ingress"><a href="#Nginx-Ingress" class="headerlink" title="Nginx Ingress"></a>Nginx Ingress</h2><p>Nginx Ingress 有多个版本，在编写 Ingress 规则的时候一定要看清自己集群中的 Nginx Ingress 版本，我最开始就是因为这个看错了文档。。</p><p>主要的版本有： <code>kubernetes/ingress-nginx</code> , <code>nginxinc/kubernetes-ingress with NGINX</code> 和 <code>nginxinc/kubernetes-ingress with NGINX PLUS</code> ，具体的对比规则可以在 <a href="https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md" target="_blank" rel="noopener">Github</a> 中了解。</p><p>在 <code>kubernetes/ingress-nginx</code> 中，默认 <code>ssl-redirect</code> 参数是 <code>true</code> ，如果自己的服务不支持 https，那么需要显示的声明该参数为 false 才可以，这里需要注意一下。</p><p>在配置 nginx 参数的时候，要注意语法，正确的书写方式如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      &#123;"apiVersion":"networking.k8s.io/v1beta1","kind":"Ingress","metadata":&#123;"annotations":&#123;"kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/proxy-read-timeout":"3600","nginx.ingress.kubernetes.io/proxy-send-timeout":"3600","nginx.ingress.kubernetes.io/ssl-redirect":"true","nginx.ingress.kubernetes.io/use-regex":"true","nginx.org/websocket-services":"websockify"&#125;,"name":"websockify","namespace":"default"&#125;,"spec":&#123;"rules":[&#123;"http":&#123;"paths":[&#123;"backend":&#123;"serviceName":"websockify","servicePort":8000&#125;,"path":"/websockify"&#125;]&#125;&#125;]&#125;&#125;</span></span><br><span class="line"><span class="string">    kubernetes.io/ingress.class: nginx</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"</span></span><br></pre></td></tr></table></figure><p>相关 issue 链接： <a href="https://github.com/kubernetes/ingress-nginx/issues/2007" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/issues/2007</a> </p><h2 id="Docker-稳定性"><a href="#Docker-稳定性" class="headerlink" title="Docker 稳定性"></a>Docker 稳定性</h2><p>在修改 Docker 配置后，需要重启 Docker.service 使配置生效，在一次重启操作中，直接导致物理节点宕机，自动重启了。。。</p><p>重启后观察物理节点日志，未发现异常日志，目前待复现调查，很坑很诡异。</p><h2 id="Helm-values-为空更新错误"><a href="#Helm-values-为空更新错误" class="headerlink" title="Helm values 为空更新错误"></a>Helm values 为空更新错误</h2><p>今天在给应用编写 Helm Charts 的时候，在 Values 中通过 resources.requests.cpu 方式指定了 cpu 和内存，在测试的时候忘记填写具体数值了，像这面这样：</p><p>values:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br></pre></td></tr></table></figure></p><p>在 helm templates 中定义 daemonset，指定使用 resources 字段。</p><p>直接执行 helm 命令安装成功了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:44:09 <span class="built_in">test</span>]<span class="variable">$helm</span> install . --name-template <span class="built_in">test</span></span><br><span class="line">NAME: <span class="built_in">test</span></span><br><span class="line">LAST DEPLOYED: 2019-07-15 20:44:18.151073881 +0800 CST m=+0.092592446</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods -l <span class="string">"app=test,release=test"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br></pre></td></tr></table></figure><p>我们查看创建出来的 daemonset 资源状态：</p><p>daemonset/test</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">Name:</span>           <span class="string">test</span></span><br><span class="line"><span class="attr">Selector:</span>       <span class="string">app=test</span></span><br><span class="line"><span class="attr">Node-Selector:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="string">Pods</span> <span class="attr">Status:</span>  <span class="number">3</span> <span class="string">Running</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Waiting</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Succeeded</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Failed</span></span><br><span class="line"><span class="string">Pod</span> <span class="attr">Template:</span></span><br><span class="line"><span class="attr">  Labels:</span>  <span class="string">app=test</span></span><br><span class="line"><span class="attr">  Containers:</span></span><br><span class="line"><span class="attr">   test:</span></span><br><span class="line"><span class="attr">    Image:</span>      <span class="string">harbor.zdyxry.com/test/test:0.1.2</span></span><br><span class="line"><span class="attr">    Port:</span>       <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line">    <span class="string">Host</span> <span class="attr">Port:</span>  <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line"><span class="attr">    Command:</span></span><br><span class="line">      <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">      -</span><span class="string">c</span></span><br><span class="line"><span class="attr">    Args:</span></span><br><span class="line">      <span class="string">gunicorn</span> <span class="bullet">-b</span> <span class="string">:10402</span> <span class="bullet">-k</span> <span class="string">gevent</span> <span class="string">test.main:flask_app</span> <span class="bullet">-w</span> <span class="number">2</span> <span class="bullet">--timeout</span> <span class="number">40</span> <span class="bullet">--pid</span> <span class="string">/var/run/test.pid</span></span><br><span class="line"><span class="attr">    Limits:</span></span><br><span class="line"><span class="attr">      cpu:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>  <span class="number">0</span></span><br><span class="line"><span class="attr">    Requests:</span></span><br><span class="line"><span class="attr">      cpu:</span>        <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">    Environment:</span>  <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure><p>这时候我在检查资源的时候发现自己忘记设置资源了，我计划通过更新 values 数值来更新 daemonset：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br></pre></td></tr></table></figure><p>执行 helm upgrade 时候报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:49:36 <span class="built_in">test</span>]<span class="variable">$helm</span> upgrade <span class="built_in">test</span> .</span><br><span class="line">Error: UPGRADE FAILED: error validating <span class="string">""</span>: error validating data: [unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.memory, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.memory]</span><br></pre></td></tr></table></figure><p>根据报错信息可以看到这个字段之前是 <code>nil</code> ，现在我们要更新为有效类型更新失败，只能通过 <code>helm uninstall</code> 卸载后再次安装修复该问题。</p><p>这个问题只在 daemonset 类型下会出现。</p><h2 id="Flannel-网卡丢失"><a href="#Flannel-网卡丢失" class="headerlink" title="Flannel 网卡丢失"></a>Flannel 网卡丢失</h2><p>在通常情况下，我们的 k8s 节点都只有单一的网络环境，也就是有一块网卡，在部署 Flannel 插件的时候，默认会找默认路由所在的网卡，并将其绑定在上面。</p><p>由于内部测试环境较为特殊，我将其绑定在一个 ovs port 上，这个具体配置在 flannel yaml 中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">kube-flannel</span></span><br><span class="line"><span class="attr">  image:</span> <span class="string">quay.io/coreos/flannel:v0.11.0-amd64</span></span><br><span class="line"><span class="attr">  command:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">/opt/bin/flanneld</span></span><br><span class="line"><span class="attr">  args:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--ip-masq</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--kube-subnet-mgr</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--iface=port-storage</span>  <span class="comment"># 在这里我强制指定了 iface</span></span><br></pre></td></tr></table></figure><p>正常运行时时没有问题的，但是对 ovs port 进行了 <code>ifdown</code> 操作后，在 OS 层面就无法找到这个 ovs port 了，flannel 默认的 <code>flannel.1</code> 这个 link 也丢失了，当我尝试 <code>ifup</code> ovs port，这个 port 正常恢复工作了，但是 <code>flannel.1</code> 无法自动恢复，目前找到的办法是手动重建 flannel pod。</p><p>猜测这个动作在 flannel 的init 相关步骤执行的，在之后 container 正常运行时没有考虑 <code>flannel.1</code> 不存在的情况。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用经验通常是踩了一个又一个坑过来的~ </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>关于Ansible的一点经验</title>
    <link href="https://zdyxry.github.io/2019/07/05/%E5%85%B3%E4%BA%8EAnsible%E7%9A%84%E4%B8%80%E7%82%B9%E7%BB%8F%E9%AA%8C/"/>
    <id>https://zdyxry.github.io/2019/07/05/关于Ansible的一点经验/</id>
    <published>2019-07-05T12:55:19.000Z</published>
    <updated>2019-07-05T12:57:01.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>先介绍下 Kubespray，Kubespray 是 K8S SIG 下的项目，目标是帮助用户创建 <code>生产环境级别</code> 的 k8s 集群。</p><p>是通过 Ansible Playbook 实现的，是的，这又是一个 Ansible 项目，其中 YAML 文件就有 15k 行，名副其实的大项目。</p><p>花费了几天时间陆陆续续看完了整个项目，大概了解了其中的工作流程，具体内容不提，感觉 Ansible 90% 的使用例子都可以在这个项目中找到，是一个值得阅读的项目。</p><p>之前写过一篇当时理解的最佳实践，今天趁此机会再总结下最近使用 Ansible 的一些经验。</p><h2 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h2><p>使用 tag 对 ansible task 进行划分，比如在重启某些服务的时候，我们只希望在初次安装的时候重启，在后续升级的时候不进行重启，那么我们就可以对这个重启服务的 task 进行tag 区分。</p><p>tag 使用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:35:56 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">1 directory, 4 files</span><br><span class="line">[root@node111 16:35:58 ansible]<span class="variable">$cat</span> test.yaml </span><br><span class="line">- hosts: cluster</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: root</span><br><span class="line">  become_method: sudo</span><br><span class="line">  tasks:</span><br><span class="line">  - yum:</span><br><span class="line">      name: <span class="string">"&#123;&#123; item &#125;&#125;"</span></span><br><span class="line">      state: present</span><br><span class="line">    loop:</span><br><span class="line">    - httpd</span><br><span class="line">    - memcached</span><br><span class="line">    tags:</span><br><span class="line">    - packages</span><br><span class="line">  </span><br><span class="line">  - template:</span><br><span class="line">      src: templates/src.j2</span><br><span class="line">      dest: /etc/foo.conf</span><br><span class="line">    tags:</span><br><span class="line">    - configuration</span><br><span class="line">[root@node111 16:36:01 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml --tags configuration -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [template] *****************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=1    changed=0    unreachable=0    failed=0</span><br></pre></td></tr></table></figure><h2 id="roles-meta-管理依赖"><a href="#roles-meta-管理依赖" class="headerlink" title="roles/meta 管理依赖"></a>roles/meta 管理依赖</h2><p>在 playbook 中存在多个 roles，且其中有相互依赖关系时，合理使用 meta 配置，填写其所依赖的 roles。注意，被依赖的 roles 会优先执行，示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:49:12 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── roles</span><br><span class="line">│   ├── a</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── b</span><br><span class="line">│       ├── defaults</span><br><span class="line">│       ├── files</span><br><span class="line">│       ├── handlers</span><br><span class="line">│       ├── meta</span><br><span class="line">│       ├── tasks</span><br><span class="line">│       │   └── main.yaml</span><br><span class="line">│       ├── templates</span><br><span class="line">│       └── vars</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">18 directories, 7 files</span><br><span class="line">[root@node111 16:49:18 ansible]<span class="variable">$cat</span> roles/a/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: a</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"a"</span></span><br><span class="line">[root@node111 16:49:25 ansible]<span class="variable">$cat</span> roles/a/meta/main.yaml </span><br><span class="line">---</span><br><span class="line">dependencies:</span><br><span class="line">  - &#123; role: b &#125;</span><br><span class="line">[root@node111 16:49:33 ansible]<span class="variable">$cat</span> roles/b/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: b</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"b"</span></span><br><span class="line">[root@node111 16:49:40 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [b : b] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TASK [a : a] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line"></span><br><span class="line">You have new mail <span class="keyword">in</span> /var/spool/mail/root</span><br><span class="line">[root@node111 16:50:05 ansible]$</span><br></pre></td></tr></table></figure><h2 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h2><p>在一个大型项目中，我们无论是服务的数量还是各个服务对应的参数数量都是极其惊人的，那么我们就要合理的管理相应参数，这里 kubespray 项目给出了一个很好的示例，先来看下 kubespray 的组织结构（简化版）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">inventory/</span><br><span class="line">├── <span class="built_in">local</span></span><br><span class="line">│   └── group_vars -&gt; ../sample/group_vars</span><br><span class="line">└── sample</span><br><span class="line">    └── group_vars</span><br><span class="line">        ├── all</span><br><span class="line">        └── k8s-cluster</span><br><span class="line">roles</span><br><span class="line">├── container-engine</span><br><span class="line">│   ├── containerd</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   ├── docker</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── meta</span><br><span class="line">├── kubespray-defaults</span><br><span class="line">│   ├── defaults</span><br><span class="line">│   ├── meta</span><br><span class="line">│   └── tasks</span><br></pre></td></tr></table></figure><p>在 kubespray 中，参数定义有三个位置：</p><ol><li>inventory/sample/group_vars</li><li>roles/kubespray-defaults/defaults</li><li>roles/<common>/defaults</common></li><li>使用 <code>set_fact</code> 关键字声明</li></ol><p>上述三个位置是按照参数粒度划分，参数粒度越细，越靠后。</p><p>举个例子：</p><ul><li><code>bin_dir</code> 这个参数，是一个全局参数，管理下载的二进制文件路径，它在 <code>inventory/samle/group_vars</code> 中定义； </li><li><code>etcd_kubeadm_enabled</code> 参数，决定是否通过 kubeadm 来创建 etcd 集群，是集群粒度的，在 <code>roles/kubesray-defaults/defaults</code> 中定义；</li><li><code>docker_fedora_repo_base_url</code> 是 docker 在下载镜像时指定的 repo，是一个容器运行时粒度的参数，那么它就在 <code>roles/container-ngine/docker/defaults</code> 中定义</li><li>执行某些命令后，我们希望对命令结果进行过滤或判断，那么我们通常会使用 regster，但是 register 的声明周期仅限于该 playbook，而且他们的优先级也是不同的，具体可以看下<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#ansible-variable-precedence" target="_blank" rel="noopener">官方文档</a></li></ul><p>可能跟大多数项目不同，kubespray 中涉及的参数数量很多，不知道是否是这个原因，专门使用了 <code>roles/kubespray-defaults</code> 这个 role 来声明参数。不过它的这种参数划分方式是我们值得学习的。</p><h2 id="Handler-触发"><a href="#Handler-触发" class="headerlink" title="Handler 触发"></a>Handler 触发</h2><p>当我们更新了配置文件之后，我们想要重启相应服务，这时候可以使用 notify 配合 handlers 来完成相应操作，而且 handler<br>的方式也可以保证我们代码最大程度的重用。</p><h2 id="loop-control"><a href="#loop-control" class="headerlink" title="loop_control"></a>loop_control</h2><p>当我们在 playbook 中定义了某个变量为 list 类型，我们想要遍历变量，并且针对每个变量的操作都进行错误处理，我们可以采用 loop_control 关键字配合 include 来实现，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- include_tasks: docker_plugin.yml</span><br><span class="line">  loop: &quot;&#123;&#123; docker_plugins &#125;&#125;&quot;</span><br><span class="line">  loop_control:</span><br><span class="line">    loop_var: docker_plugin</span><br></pre></td></tr></table></figure><p>循环 <code>device_plugins</code> ，每个变量为 <code>docker_plugin</code> ，将 <code>docker_plugin</code> 传递到 <code>docker_plugin.yml</code> 。</p><h2 id="不建议的操作"><a href="#不建议的操作" class="headerlink" title="不建议的操作"></a>不建议的操作</h2><h3 id="在-ansible-中使用复杂的语法规则"><a href="#在-ansible-中使用复杂的语法规则" class="headerlink" title="在 ansible 中使用复杂的语法规则"></a>在 ansible 中使用复杂的语法规则</h3><p>在 kubespray 中，随处可见一些很复杂的语法夹杂在 playbook 中，看到一个比较头疼的：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">279</span></span><br><span class="line"><span class="number">280</span><span class="attr">dashboard_image_repo:</span> <span class="string">"gcr.io/google_containers/kubernetes-dashboard-<span class="template-variable">&#123;&#123; image_arch &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">281</span><span class="attr">dashboard_image_tag:</span> <span class="string">"v1.10.1"</span></span><br><span class="line"><span class="number">282</span></span><br><span class="line"><span class="number">283</span><span class="attr">image_pull_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker pull"</span></span><br><span class="line"><span class="number">284</span><span class="attr">image_info_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker images -q | xargs <span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker inspect -f \"<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoTags <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoTags 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoDigests <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;,<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoDigests 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;\" | tr '\n' ','"</span></span><br><span class="line"><span class="number">285</span></span><br><span class="line"><span class="number">286</span><span class="attr">downloads:</span></span><br><span class="line"><span class="number">287</span>  <span class="attr">netcheck_server:</span></span><br><span class="line"><span class="number">288</span>    <span class="attr">enabled:</span> <span class="string">"<span class="template-variable">&#123;&#123; deploy_netchecker &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">289</span>    <span class="attr">container:</span> <span class="literal">true</span></span><br><span class="line"><span class="number">290</span>    <span class="attr">repo:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_repo &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">291</span>    <span class="attr">tag:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">292</span>    <span class="attr">sha256:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_digest_checksum|default(None) &#125;&#125;</span>"</span></span><br></pre></td></tr></table></figure><p>284 行获取 <code>image_info_command</code> ，这里真的是一点可维护性都没有，初步看很难看出这里到底是 ansible 语法，还是 shell 语法，还是 go template 语法，如果我们要通过这么复杂的方式来获取一个参数，为什么不干脆写一个脚本来完成这个事情呢？ 搞不懂。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>看完 kubespray 了解了 k8s 集群部署的步骤之外，Ansible 的一些高级用法或者说经验是之后需要改善的，收获多多。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;先介绍下 Kubespray，Kubespray 是 K8S SIG 下的项目，目标是帮助用户创建 &lt;code&gt;生产环境级别&lt;/code&gt; 
      
    
    </summary>
    
    
      <category term="Ansible" scheme="https://zdyxry.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Pod 可用性</title>
    <link href="https://zdyxry.github.io/2019/06/26/Kubernetes-%E5%AE%9E%E6%88%98-Pod-%E5%8F%AF%E7%94%A8%E6%80%A7/"/>
    <id>https://zdyxry.github.io/2019/06/26/Kubernetes-实战-Pod-可用性/</id>
    <published>2019-06-26T13:57:31.000Z</published>
    <updated>2019-06-27T12:52:37.478Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性相关知识。</p><h2 id="K8S-可用性相关参数"><a href="#K8S-可用性相关参数" class="headerlink" title="K8S 可用性相关参数"></a>K8S 可用性相关参数</h2><p>k8s 核心组件有 kubelet,kube-apiserver,kube-scheduler,kube-controller-manager，通过阅读官方文档中相关参数说明，我摘取了认为跟可用性相关的参数，具体列表如下：</p><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><h4 id="–housekeeping-interval-duration"><a href="#–housekeeping-interval-duration" class="headerlink" title="–housekeeping-interval duration"></a>–housekeeping-interval duration</h4><p>Default: 10s</p><p>Interval between container housekeepings.</p><p>kubelet 主动检测容器资源是否达到阈值的周期。</p><h4 id="–node-status-update-frequency-duration"><a href="#–node-status-update-frequency-duration" class="headerlink" title="–node-status-update-frequency duration"></a>–node-status-update-frequency duration</h4><p>Default: 10s</p><p>Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller. </p><p>kubelet 上报到 kube-apiserver 频率。</p><h3 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h3><h4 id="–node-eviction-rate-float32"><a href="#–node-eviction-rate-float32" class="headerlink" title="–node-eviction-rate float32"></a>–node-eviction-rate float32</h4><p>Default: 0.1 </p><p>Number of nodes per second on which pods are deleted in case of node failure when a zone is healthy (see –unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.</p><p>当 kube-controller-manager 判定节点故障，开始迁移（重建）pod 的速度，默认是 0.1，也就是 1Pod/10s 。</p><h4 id="–node-monitor-grace-period-duration"><a href="#–node-monitor-grace-period-duration" class="headerlink" title="–node-monitor-grace-period duration"></a>–node-monitor-grace-period duration</h4><p>Default: 40s </p><p>Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet’s nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.</p><p>kube-controller-manager 标记 kubelet(node) 为不健康的周期。</p><h4 id="–node-monitor-period-duration"><a href="#–node-monitor-period-duration" class="headerlink" title="–node-monitor-period duration"></a>–node-monitor-period duration</h4><p>Default: 5s </p><p>The period for syncing NodeStatus in NodeController.</p><p>kube-controller-manager 定期检查 kubelet(node) 状态周期。</p><h4 id="–node-startup-grace-period-duration"><a href="#–node-startup-grace-period-duration" class="headerlink" title="–node-startup-grace-period duration"></a>–node-startup-grace-period duration</h4><p>Default: 1m0s </p><p>Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.</p><p>kube-controller-manager 在标记节点为不健康之前允许无响应时间。</p><h4 id="–pod-eviction-timeout-duration"><a href="#–pod-eviction-timeout-duration" class="headerlink" title="–pod-eviction-timeout duration"></a>–pod-eviction-timeout duration</h4><p>Default: 5m0s </p><p>The grace period for deleting pods on failed nodes.</p><p>kube-controller-manager 判定节点故障，重建 Pod 的超时时间。</p><h2 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h2><p>看完了相关参数，我们来看下 kubelet 和 kube-controller-manager 是如何相互关联工作来保证 Pod 可用性的。</p><ol><li><p>kubelet 启动，若启动时间超过 node-startup-grace-period，则 kube-controller-manager 将其置为 unhealthy</p></li><li><p>kubelet 按照 –node-status-update-frequency 周期，定时与 kube-apiserver 通信将其状态记录到 etcd</p></li><li><p>若 kubelet 无法连接到 kube-apiserver，那么 kubelet 会尝试 nodeStatusUpdateRetry 次更新状态信息</p></li><li><p>kube-controller-manager 按照 –node-monitor-period 周期，定时从 etcd 中获取 kubelet 状态</p></li><li><p>若 kubelet 在 –node-monitor-grace-period 周期内均为非健康状态（这里如果 kubelet 未更新状态，等同），则该节点更新为 NotReady，将需要迁移（重建）的资源放置到队列中</p></li><li><p>当 kubelet NotReady 的 –pod-eviction-timeout 时间后， kube-controller-manager 开始进行 Pod 驱逐动作</p></li><li><p>驱逐速度为 –node-eviction-rate ，即每10s 迁移（重建）1个 Pod</p></li></ol><p>需要注意的是，上述 kubelet 和 kube-controller-manager 的操作是异步的，中间任何一个更新步骤都有可能出现延迟的情况，所以真实情况会比上述流程复杂（诡异）的多。比如 lijieao 的<a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html" target="_blank" rel="noopener">最新博客</a>中碰到的因为磁盘 IO 压力导致 kubelet NotReady 的情况，都是有可能出现的。</p><p>按照上面的流程，可以看到，当我们节点故障后，要花费 5min 的时间才会重建我们的业务，这种情况下大部分对稳定性要求较高的业务都无法忍受的，所以有同学可能会考虑修改默认配置，来减少业务宕机时间。</p><p>上述提到的参数的默认值，肯定是 k8s 社区经过多年的架构设计（或者经验？）的。之前看过社区中关于这部分的一篇文章提到，不建议修改默认的配置，担心修改了默认配置可能因为其他一些比较微小的故障导致 Pod 频繁的迁移，这是我们不想看到的。</p><p>上面提到的情况都是说：当节点发生故障后，我们希望我们的 Pod 能够第一时间迁移到正常运行的节点，那么有没有反过来的，不想迁移的呢？</p><p>其实是有的，比如有状态服务配合 kubelet 服务故障场景。我们的节点正常运行的情况下，容器运行时也都正常工作，唯独 kubelet 故障了，那么我们想想此时发生了什么？ </p><p>kube-controller-manager 在一段时间后标记节点故障，开始迁移（重建） Pod 操作，但是要注意，此时因为是有状态服务，而节点上的容器又在正常运行，kubelet 由于故障了，k8s 无法直接操作节点上的容器，只能在其他节点进行重建。这时候问题就出现了，集群中存在2个 Pod 同时读写一个 PV，这种错误是致命的。</p><p>在这种场景下，我们想要追求的是哪怕节点故障了，我们也要尽量的不迁移 Pod，来保证我们数据的读写正常。</p><p>那么上述情况我们应该怎么解决呢？ </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Operator 是一种方式。在 Operator 出现之前，有状态服务的运维工作都是靠着探针或者其他的基础运维工具来完成的，很多工作即使做到了自动化也不完美，现在有了 Operator 结合 CRD，我们完全可以自己在应用层面监控服务状态，从而主动的触发 Pod 的迁移（重建），保证我们业务的稳定。</p><p>另一种解决方式是 <code>Taint based Evictions</code>，在 1.13 版本中增加了该配置，我们可以在创建资源的时候，指定规则配置Pod 容忍节点异常的时间，加快触发 Pod 重建，这大大减缓了对集群配置的要求。</p><h2 id="测试-YAML"><a href="#测试-YAML" class="headerlink" title="测试 YAML"></a>测试 YAML</h2><p>在测试 Pod 可用性的时候，我们必须创建 Deployment、RS 类型资源，单纯的 Pod 资源是不被保证可用性的。同时我们也想观察 DaemonSet Pod 在节点故障场景下的表现，所以一同创建了。可以使用以下 YAML 用来创建测试环境。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@m1</span> <span class="string">ha]#</span> <span class="string">cat</span> <span class="string">ha.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/unreachable"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/not-ready"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">sleep</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">"apps/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">dp</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">          done</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string"></span><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md" target="_blank" rel="noopener">https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md</a></p><p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md</a></p><p><a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81" target="_blank" rel="noopener">https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Helm 包管理器</title>
    <link href="https://zdyxry.github.io/2019/06/21/Kubernetes-%E5%AE%9E%E6%88%98-Helm-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8/"/>
    <id>https://zdyxry.github.io/2019/06/21/Kubernetes-实战-Helm-包管理器/</id>
    <published>2019-06-21T12:40:24.000Z</published>
    <updated>2019-06-22T01:40:50.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Helm 就是<code>k8s 的包管理器</code> 。常见的包管理器有：yum,apt,pip…</p><p>包管理器基础功能有：</p><ul><li>安装<ul><li>依赖安装</li></ul></li><li>升级</li><li>回滚</li><li>卸载</li><li>源管理</li><li>搜索</li><li>…</li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>Helm: Kubernetes的包管理工具，命令行同名</p></li><li><p>Tiller: Helmv2 的服务端，用于接收并处理 Helm 发送的请求，默认以 Deployment 形式部署在 k8s 集群中</p></li><li><p>Chart: Helm 包管理的基础单元，等同于 RPM</p></li><li><p>Repoistory: Helm的软件源仓库，是一个 Web 服务器，路径下除了响应的软件 Chart 包之外，维护了一个 index.yaml 用于索引</p></li><li><p>Release: Helm 安装在 Kubernetes 集群中的 Chart 实例</p></li></ul><h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><p>helm 截至06月20日最新稳定版本为 v2.14.1。</p><p>在05月16日发布了 v3.0 alpha 版本，根据相关文档描述，v2 无法平滑升级到 v3 版本。</p><p>注：存在部分小版本无法平滑升级情况。</p><p>helm v3 版本改进：</p><ol><li>在 v2 版本设计中，需要单独创建属于 Tiller 的 ServiceAccount，授权 clusteradmin 权限，以为着只要你有 helm 权限，那么你有操作 k8s全集群所有权限。在 v3 版本中删除 Tiller，直接与 k8s api 进行通信，权限管理更清晰</li><li>helm 提供 libary</li><li>模板引擎切换为 Lua</li><li>目前通过 Hook 方式创建的资源，helm 后续不会管理，在 v3 会增加管理 Hook 资源功能</li><li>目前所有配置保存在 cm 中，后续考虑保存到 secret</li><li>v2 需要单独维护仓库，v3 中可以将 Chart 推送到 Docker 镜像仓库中，提供 helm push/login 功能</li><li>…</li></ol><h2 id="Helm2-基本使用"><a href="#Helm2-基本使用" class="headerlink" title="Helm2 基本使用"></a>Helm2 基本使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在 Helm Github <a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">Release</a> 下载最新版本二进制文件，并在本地解压。</p><p>在 k8s master 节点，执行 <code>helm init --server-account tiller</code> 将 Tiller 部署在 k8s 集群中，指定 Service Account 为 Tiller。</p><p>在 k8s 1.6 版本之后，需要创建对应的 ServiceAccount 资源给 Tiller ，便于 Tiller 后续创建资源，参考官方文档：</p><p>rbac-config.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount <span class="string">"tiller"</span> created</span><br><span class="line">clusterrolebinding <span class="string">"tiller"</span> created</span><br></pre></td></tr></table></figure><p>通过 <code>helm version</code> 验证是否部署成功，成功会显示 client 和 server 对应版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="Chart构建"><a href="#Chart构建" class="headerlink" title="Chart构建"></a>Chart构建</h4><p>本地创建一个测试软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm create yiran-test</span></span><br><span class="line">Creating yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts                            <span class="comment"># yiran-test 所依赖的 Chart，此处为空</span></span><br><span class="line">├── Chart.yaml                        <span class="comment"># yiran-test 的基本信息，包含：名称，apiversion, appversion, version</span></span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml               <span class="comment"># 配置模板路径</span></span><br><span class="line">│   ├── _helpers.tpl                  <span class="comment"># 用于修改kubernetes objcet配置的模板</span></span><br><span class="line">│   ├── ingress.yaml                  <span class="comment"># </span></span><br><span class="line">│   ├── NOTES.txt                     <span class="comment"># 类似于 Chart README</span></span><br><span class="line">│   ├── service.yaml                  <span class="comment"># </span></span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml      <span class="comment"># 测试模板</span></span><br><span class="line">└── values.yaml                       <span class="comment"># 用于渲染模板的具体值</span></span><br><span class="line"></span><br><span class="line">3 directories, 8 files</span><br></pre></td></tr></table></figure><p>有了基础示例，我们可以先通过 <code>dry-run</code> 方式跑一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install --dry-run --debug ./</span></span><br><span class="line">[debug] Created tunnel using <span class="built_in">local</span> port: <span class="string">'39257'</span></span><br><span class="line"></span><br><span class="line">[debug] SERVER: <span class="string">"127.0.0.1:39257"</span></span><br><span class="line"></span><br><span class="line">[debug] Original chart version: <span class="string">""</span></span><br><span class="line">[debug] CHART PATH: /root/helm/yiran-test</span><br><span class="line"></span><br><span class="line">NAME:   torrid-rodent</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Thu Jun 20 19:10:08 2019</span><br><span class="line">CHART: yiran-test-0.1.0</span><br><span class="line">USER-SUPPLIED VALUES:</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">COMPUTED VALUES:</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line">fullnameOverride: <span class="string">""</span></span><br><span class="line">image:</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">imagePullSecrets: []</span><br><span class="line">ingress:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  enabled: <span class="literal">false</span></span><br><span class="line">  hosts:</span><br><span class="line">  - host: chart-example.local</span><br><span class="line">    paths: []</span><br><span class="line">  tls: []</span><br><span class="line">nameOverride: <span class="string">""</span></span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line">replicaCount: 1</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  port: 80</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># torrid-rodent-yiran-test-test-connection</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="string">"torrid-rodent-yiran-test-test-connection"</span></span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: yiran-test</span><br><span class="line">    helm.sh/chart: yiran-test-0.1.0</span><br><span class="line">    app.kubernetes.io/instance: torrid-rodent</span><br><span class="line">    app.kubernetes.io/version: <span class="string">"1.0"</span></span><br><span class="line">    app.kubernetes.io/managed-by: Tiller</span><br><span class="line">  annotations:</span><br><span class="line">    <span class="string">"helm.sh/hook"</span>: <span class="built_in">test</span>-success</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: wget</span><br><span class="line">      image: busybox</span><br><span class="line">      <span class="built_in">command</span>: [<span class="string">'wget'</span>]</span><br><span class="line">      args:  [<span class="string">'torrid-rodent-yiran-test:80'</span>]</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">MANIFEST:</span><br></pre></td></tr></table></figure><h4 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h4><p>可以看到生成的 YAML 文件就是拿 values.yaml 值渲染模板生成的，那么我们来安装一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install ./</span></span><br><span class="line">NAME:   precise-bear</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:12:01 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS             RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Running   0          7s</span><br></pre></td></tr></table></figure><p>注意，此时我们已经创建好了对应的资源，可以通过 <code>kubectl</code> 来查看状态。</p><h4 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h4><p>我们来修改一下 <code>yiran-test/Chart.yaml</code> ，调整下版本，进行升级：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># cat Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">heNAME          REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm upgrade precise-bear ./</span></span><br><span class="line">Release <span class="string">"precise-bear"</span> has been upgraded.</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:18:30 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  1/1    1           1          6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS   RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  1/1    Running  0         6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   6m30s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default</span><br></pre></td></tr></table></figure><h4 id="回滚"><a href="#回滚" class="headerlink" title="回滚"></a>回滚</h4><p>通过上述步骤，我们已经把我们的应用 <code>yiran-test</code> 从 <code>0.1.0</code> 版本升级到了 <code>0.2.0</code> 版本，那么我们现在来尝试下回滚。</p><p>helm 回滚操作需要指定 Release 名称和目标版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm rollback precise-bear 1</span></span><br><span class="line">Rollback was a success.</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    3           Thu Jun 20 19:23:04 2019    DEPLOYED    yiran-test-0.1.0    1.0         default</span><br></pre></td></tr></table></figure><p>嗯，可以看到，我们指定了回滚的目标 REVISION，回滚成功了，<code>yiran-test</code> 回到了 <code>0.1.0</code> 版本，但是，最恶心的来了，Release 的当前版本变成了 <code>3</code> ，而不是设想中的 <code>1</code> 。</p><h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><p>如果我们想要从 k8s 上卸载对应的软件，也就是我们的 <code>yiran-test</code> ，我们可以直接执行 <code>delete</code> 命令，会直接把相关资源全部删除掉。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">NAME            REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">precise-bear    3               Thu Jun 20 19:23:04 2019        DEPLOYED        yiran-test-0.1.0        1.0             default</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">kuNAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   1/1     Running   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes                ClusterIP   10.68.0.1       &lt;none&gt;        443/TCP   36d</span><br><span class="line">precise-bear-yiran-test   ClusterIP   10.68.142.106   &lt;none&gt;        80/TCP    139m</span><br><span class="line">[root@node1 ~]<span class="comment"># helm delete precise-bear</span></span><br><span class="line">release <span class="string">"precise-bear"</span> deleted</span><br><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">NAME                                       READY   STATUS        RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Terminating   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.68.0.1    &lt;none&gt;        443/TCP   36d</span><br></pre></td></tr></table></figure><h4 id="软件源管理"><a href="#软件源管理" class="headerlink" title="软件源管理"></a>软件源管理</h4><p>上面我们所有的操作都是针对本地包（路径），那么我们怎么才能通过网络下载别人已经构建好的软件包呢？ </p><p>helm 使用 <code>repo</code> 命令来管理软件源，使用上也很简单，简单列举下相应命令实用说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME    URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>   http://127.0.0.1:8879/charts</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable-mirror https://burdenbear.github.io/kube-charts-mirror/</span></span><br><span class="line"><span class="string">"stable-mirror"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable https://kubernetes-charts.storage.googleapis.com</span></span><br><span class="line">helm repo list</span><br><span class="line"><span class="string">"stable"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>           http://127.0.0.1:8879/charts</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo remove local</span></span><br><span class="line"><span class="string">"local"</span> has been removed from your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br></pre></td></tr></table></figure><h4 id="软件搜索"><a href="#软件搜索" class="headerlink" title="软件搜索"></a>软件搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm search mongo</span></span><br><span class="line">NAME                                            CHART VERSION   APP VERSION     DESCRIPTION</span><br><span class="line">stable-mirror/mongodb                           5.20.0          4.0.10          NoSQL document-oriented database that stores JSON-like <span class="keyword">do</span>...</span><br><span class="line">stable-mirror/mongodb-replicaset                3.9.6           3.6             </span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="打包与本地软件源构建"><a href="#打包与本地软件源构建" class="headerlink" title="打包与本地软件源构建"></a>打包与本地软件源构建</h4><p>Helm v2 命令支持本地创建软件源，命令关键字是 <code>helm serve</code> ，下面来演示下相关操作：</p><p>启动本地源，并指定 IP 地址和 repo 路径：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">rbac-config.yaml  yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># helm serve --address 192.168.27.231:8879 --repo-path /root/helm/ </span></span><br><span class="line">Regenerating index. This may take a moment.</span><br><span class="line">Now serving you on 192.168.27.231:8879</span><br></pre></td></tr></table></figure></p><p>新开终端，通过修改 <code>yiran-test</code> 的 Chart.yaml 文件打包两个版本的 Chart：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: <span class="string">"1.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.1.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.1.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># vi yiran-test/Chart.yaml </span></span><br><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">index.yaml  rbac-config.yaml  yiran-test  yiran-test-0.1.0.tgz  yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure><p>此时访问浏览器，应该只能看到空的列表，我们更新一下软件源索引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo index --url=http://192.168.27.231:8879 .</span></span><br><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># cat index.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">entries:</span><br><span class="line">  yiran-test:</span><br><span class="line">  - apiVersion: v2</span><br><span class="line">    appVersion: <span class="string">"2.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220764856+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: af54cfdc5f8e6463a91311496bab8fafd7364c3588b85b5e676fb930cd4e2754</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.2.0.tgz</span><br><span class="line">    version: 0.2.0</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    appVersion: <span class="string">"1.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220059406+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: 64b94c30827aab8e52f57cd3950645bb14ae2deb44e3100d48ecd64f1e706ea5</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.1.0.tgz</span><br><span class="line">    version: 0.1.0</span><br><span class="line">generated: <span class="string">"2019-06-21T13:43:41.218753007+08:00"</span></span><br></pre></td></tr></table></figure><p>可以看到在 repo 路径下生成了一个 index.yaml 文件，这个文件就是 repo 的索引文件，我们可以直接通过浏览器访问 <code>http://192.168.27.231:8879</code> 来浏览或下载所需软件包。</p><p>也可以将本地 repo 添加到 helm 中，使用 helm 命令将软件包部署到 k8s 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         URL                                             </span><br><span class="line">stable       https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirrorhttps://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo add local http://192.168.27.231:8879</span></span><br><span class="line"><span class="string">"local"</span> has been added to your repositories</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         URL                                             </span><br><span class="line">stable       https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirrorhttps://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line"><span class="built_in">local</span>        http://192.168.27.231:8879                      </span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            CHART VERSIONAPP VERSIONDESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test0.2.0        2.0        A Helm chart <span class="keyword">for</span> Kubernetes</span><br></pre></td></tr></table></figure><p>尝试从本地源安装应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            CHART VERSIONAPP VERSIONDESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test0.2.0        2.0        A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">[root@node1 helm]<span class="comment"># helm install local/yiran-test</span></span><br><span class="line">NAME:   orange-chicken</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 13:49:45 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">orange-chicken-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                        READY  STATUS             RESTARTS  AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                       TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">orange-chicken-yiran-test  ClusterIP  10.68.85.197  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=orange-chicken"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME          REVISIONUPDATED                 STATUS  CHART           APP VERSIONNAMESPACE</span><br><span class="line">orange-chicken1       Fri Jun 21 13:49:45 2019DEPLOYEDyiran-test-0.2.02.0        default </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp   1/1     Running   0          62s</span><br></pre></td></tr></table></figure><h4 id="Chart-依赖管理"><a href="#Chart-依赖管理" class="headerlink" title="Chart 依赖管理"></a>Chart 依赖管理</h4><p>包管理器一个比较钟要的功能就是依赖管理，当我安装 A，A 依赖于 B，那么 B 应该会自动安装完成。</p><p>我们在 <code>yiran-test</code> 中添加两个依赖，并构建：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/requirements.yaml   # 添加 apache 和 mysql 依赖</span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: apache</span><br><span class="line">    version: 4.3.2</span><br><span class="line">    repository: https://charts.bitnami.com</span><br><span class="line">  - name: mysql</span><br><span class="line">    version: 1.2.0</span><br><span class="line">    repository: https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 当前目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm dep up yiran-test/ # 下载依赖到本地</span></span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Successfully got an update from the <span class="string">"local"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"bitnami"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable-mirror"</span> chart repository</span><br><span class="line">Update Complete.</span><br><span class="line">Saving 2 charts</span><br><span class="line">Downloading apache from repo https://charts.bitnami.com</span><br><span class="line">Downloading mysql from repo https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">Deleting outdated charts</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 完整目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">│   ├── apache-4.3.2.tgz</span><br><span class="line">│   └── mysql-1.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 12 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure><p>可以看到 helm 对依赖的管理方式是将自己所依赖的所有 Chart，均下载到 <code>yiran-test/charts/</code> 路径下，我们打包的时候其实已经包含了所有依赖了。</p><p>再创建一个 Chart，依赖于 <code>yiran-test</code> ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># tree nested/</span></span><br><span class="line">nested/</span><br><span class="line">├── charts</span><br><span class="line">│   └── yiran-test-0.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 11 files</span><br><span class="line">[root@node1 helm]<span class="comment"># cat nested/requirements.yaml </span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: yiran-test</span><br><span class="line">    version: 0.2.0</span><br><span class="line">    repository: http://192.168.27.231:8879</span><br></pre></td></tr></table></figure><p>实际安装 <code>nested</code> ，来看下安装结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm install nested/</span></span><br><span class="line">NAME:   alliterating-gopher</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 18:22:52 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                            DATA  AGE</span><br><span class="line">alliterating-gopher-mysql-test  1     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                            READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-nested      0/1    1           0          1s</span><br><span class="line">alliterating-gopher-yiran-test  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/PersistentVolumeClaim</span><br><span class="line">NAME                       STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">alliterating-gopher-mysql  Pending  1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                             READY  STATUS             RESTARTS  AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg        0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm       0/1    Pending            0         1s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq      0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME                       TYPE    DATA  AGE</span><br><span class="line">alliterating-gopher-mysql  Opaque  2     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                            TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE</span><br><span class="line">alliterating-gopher-apac        LoadBalancer  10.68.35.233   &lt;pending&gt;    80:21195/TCP,443:26200/TCP  1s</span><br><span class="line">alliterating-gopher-mysql       ClusterIP     10.68.70.173   &lt;none&gt;       3306/TCP                    1s</span><br><span class="line">alliterating-gopher-nested      ClusterIP     10.68.190.252  &lt;none&gt;       80/TCP                      1s</span><br><span class="line">alliterating-gopher-yiran-test  ClusterIP     10.68.217.171  &lt;none&gt;       80/TCP                      1s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-apac   0/1    1           0          1s</span><br><span class="line">alliterating-gopher-mysql  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=nested,app.kubernetes.io/instance=alliterating-gopher"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME               REVISIONUPDATED                 STATUS  CHART       APP VERSIONNAMESPACE</span><br><span class="line">alliterating-gopher1       Fri Jun 21 18:22:52 2019DEPLOYEDnested-0.1.01.0        default  </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg         0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm        0/1     Pending   0          5s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq       0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm   1/1     Running   0          5s</span><br></pre></td></tr></table></figure><p>我们可以看到 nested 已经安装完成了，同时 nested 依赖得 yiran-test 及 yiran-test 依赖得 mysql &amp; apache 也已经安装了。</p><p>具体得依赖关系直接引用官方文档示例：</p><p>假设名为 “A” 的 chart 创建以下 Kubernetes 对象</p><blockquote><p>namespace “A-Namespace”<br>statefulset “A-StatefulSet”<br>service “A-Service”  </p></blockquote><p>此外，A 依赖于创建对象的 chart B.</p><blockquote><p>namespace “B-Namespace”<br>replicaset “B-ReplicaSet”<br>service “B-Service”  </p></blockquote><p>安装/升级 chart A 后，会创建/修改单个 Helm 版本。该版本将按以下顺序创建/更新所有上述 Kubernetes 对象：</p><blockquote><p>A-Namespace<br>B-Namespace<br>A-StatefulSet<br>B-ReplicaSet<br>A-Service<br>B-Service  </p></blockquote><p>这是因为当 Helm 安装 / 升级 charts 时，charts 中的 Kubernetes 对象及其所有依赖项都是如下</p><ol><li>聚合成一个单一的集合; </li><li>按类型排序，然后按名称排序; </li><li>按该顺序创建/更新。</li></ol><p>因此，单个 release 是使用 charts 及其依赖关系创建的所有对象，这意味着 Helm 不管理依赖服务的相关启动顺序，要由上层应用自己控制（比如创建响应探针）。</p><p>具体的资源创建顺序可以看 Tiller <a href="https://github.com/helm/helm/blob/master/pkg/tiller/kind_sorter.go#L29" target="_blank" rel="noopener">相关代码</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说 Helm 基础功能使用还是很方便的，关键在于我们如何划分我们应用的粒度，比如 openstack 是以组件为粒度划分的：nova Chart 包含 api、scheduler、conductor 等服务；比如 TiDB Operator 项目是以具体功能来划分的：tidb-backup、tidb-cluster、tidb-operator 。我们应该根据自己的业务需求，合理划分。</p><p>还有一点需要注意的是，Helm 项目还处于快速发展阶段（貌似涉及 k8s 的都变化太快），尤其是最近发布了 Helm3 alpha，如果是生产系统，需要考虑后续是否能够平滑升级的影响。</p><p>如果实在不喜欢 Helm Tiller 方式，单纯使用 Helm template 也是可以的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Helm 就是&lt;code&gt;k8s 的包管理器&lt;/code&gt; 。常见的包管理器有：yum,apt,pip…&lt;/p&gt;
&lt;p&gt;包管理器基础功能有：
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>调整 arp 参数提高网络稳定性</title>
    <link href="https://zdyxry.github.io/2019/06/17/%E8%B0%83%E6%95%B4-arp-%E5%8F%82%E6%95%B0%E6%8F%90%E9%AB%98%E7%BD%91%E7%BB%9C%E7%A8%B3%E5%AE%9A%E6%80%A7/"/>
    <id>https://zdyxry.github.io/2019/06/17/调整-arp-参数提高网络稳定性/</id>
    <published>2019-06-17T13:20:04.000Z</published>
    <updated>2019-06-17T13:32:36.915Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在机房网络规划中，地区 A 和地区 B 是通过 OpenVPN 连接的，也就是说每个地区的网关是一台虚拟机，提供 DHCP 服务。<br>今天地区 A 的机器又无法连接地区 B 了，我登陆网关尝试从网关 ping 目标主机，发现直接提示 :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No buffer space available</span><br></pre></td></tr></table></figure><p>根据这个提示，感觉像是某些系统参数配置的小了，于是查了一下，发现跟 arp 有关。什么是 arp ？ </p><p>相信对网络稍微有些概念的同学都不陌生，这里我直接引用维基百科：</p><blockquote><p>地址解析协议（英语：Address Resolution Protocol，缩写：ARP）。在以太网协议中规定，同一局域网中的一台主机要和另一台主机进行直接通信，必须要知道目标主机的MAC地址。而在TCP/IP协议中，网络层和传输层只关心目标主机的IP地址。这就导致在以太网中使用IP协议时，数据链路层的以太网协议接到上层IP协议提供的数据中，只包含目的主机的IP地址。于是需要一种方法，根据目的主机的IP地址，获得其MAC地址。这就是ARP协议要做的事情。所谓地址解析（address resolution）就是主机在发送帧前将目标IP地址转换成目标MAC地址的过程。</p></blockquote><blockquote><p>另外，当发送主机和目的主机不在同一个局域网中时，即便知道对方的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为委托ARP或ARP代理（ARP Proxy）。</p></blockquote><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>知道了原因，那么我们来调整参数就好：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gc_thresh1 (since Linux 2.2)</span><br><span class="line">The minimum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will not run <span class="keyword">if</span> there are fewer than this number of entries <span class="keyword">in</span> the cache. Defaults to 128.</span><br><span class="line">gc_thresh2 (since Linux 2.2)</span><br><span class="line">The soft maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will allow the number of entries to exceed this <span class="keyword">for</span> 5 seconds before collection will be performed. Defaults to 512.</span><br><span class="line">gc_thresh3 (since Linux 2.2)</span><br><span class="line">The hard maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will always run <span class="keyword">if</span> there are more than this number of entries <span class="keyword">in</span> the cache. Defaults to 1024.</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yiran@<span class="built_in">test</span>:~$ cat /etc/sysctl.conf |grep -v ^<span class="comment"># |grep -v ^$</span></span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">yiran@<span class="built_in">test</span>:~$ sudo sysctl -p</span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。&lt;/p&gt;
&lt;h2 id=&quot;问题&quot;
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-高可用集群部署</title>
    <link href="https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://zdyxry.github.io/2019/06/15/Kubernetes-实战-高可用集群部署/</id>
    <published>2019-06-14T17:44:28.000Z</published>
    <updated>2019-06-14T17:45:45.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>本文所有节点 OS 均为 CentOS 7.4 。</p><h3 id="1-关闭-selinux"><a href="#1-关闭-selinux" class="headerlink" title="1.关闭 selinux"></a>1.关闭 selinux</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/selinux/config </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of three two values:</span></span><br><span class="line"><span class="comment">#     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="comment">#     minimum - Modification of targeted policy. Only selected processes are protected. </span></span><br><span class="line"><span class="comment">#     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node211 ~]<span class="comment"># getenforce </span></span><br><span class="line">Disabled</span><br></pre></td></tr></table></figure><h3 id="2-关于-firewalld"><a href="#2-关于-firewalld" class="headerlink" title="2. 关于 firewalld"></a>2. 关于 firewalld</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br></pre></td></tr></table></figure><h3 id="3-安装必要-yum-源：epel-release"><a href="#3-安装必要-yum-源：epel-release" class="headerlink" title="3. 安装必要 yum 源：epel-release"></a>3. 安装必要 yum 源：epel-release</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install epel-release</span></span><br><span class="line">[root@node211 ~]<span class="comment"># ls /etc/yum.repos.d/epel.repo </span></span><br><span class="line">/etc/yum.repos.d/epel.repo</span><br></pre></td></tr></table></figure><h3 id="4-关闭节点-swap-空间"><a href="#4-关闭节点-swap-空间" class="headerlink" title="4. 关闭节点 swap 空间"></a>4. 关闭节点 swap 空间</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/fstab </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># /etc/fstab</span></span><br><span class="line"><span class="comment"># Created by anaconda on Thu Jun 13 09:45:52 2019</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Accessible filesystems, by reference, are maintained under '/dev/disk'</span></span><br><span class="line"><span class="comment"># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">/dev/mapper/centos-root /                       xfs     defaults        0 0</span><br><span class="line">UUID=c0f0a31a-0c36-42cf-b52a-8f3b027ef948 /boot                   xfs     defaults        0 0</span><br><span class="line"><span class="comment">#/dev/mapper/centos-swap swap                    swap    defaults        0 0</span></span><br><span class="line">[root@node211 ~]<span class="comment"># free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.7G        102M        3.3G        8.3M        230M        3.3G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure><h3 id="5-安装-docker-ce"><a href="#5-安装-docker-ce" class="headerlink" title="5. 安装 docker-ce"></a>5. 安装 docker-ce</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># </span></span><br><span class="line">[root@node211 ~]<span class="comment"># head -n 6 /etc/yum.repos.d/docker-ce.repo</span></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line">[root@node211 ~]<span class="comment"># rpm -q docker</span></span><br><span class="line">docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64</span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl start docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status docker</span></span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-06-14 19:48:40 CST; 3s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line"> Main PID: 11488 (dockerd-current)</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           ├─11488 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --<span class="built_in">exec</span>-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr...</span><br><span class="line">           └─11495 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-r...</span><br><span class="line"></span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.282909889+08:00"</span> level=info msg=<span class="string">"Docker daemon"</span> commit=<span class="string">"b2f74b2/1.13.1"</span> graphdriver=overlay2 version=1.13.1</span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.293315055+08:00"</span> level=info msg=<span class="string">"API listen on /var/run/docker.sock"</span></span><br><span class="line">Jun 14 19:48:40 node211 systemd[1]: Started Docker Application Container Engine.</span><br></pre></td></tr></table></figure><h3 id="6-开启必要系统参数-sysctl"><a href="#6-开启必要系统参数-sysctl" class="headerlink" title="6. 开启必要系统参数 sysctl"></a>6. 开启必要系统参数 sysctl</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># sysctl -p</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br></pre></td></tr></table></figure><h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>因为 kubeadm 官方文档中没有详细步骤，因此相关描述尽量具体到命令行。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.211</td><td>master</td></tr><tr><td>192.168.77.212</td><td>master</td></tr><tr><td>192.168.77.213</td><td>master </td></tr><tr><td>192.168.77.214</td><td>node</td></tr></tbody></table><h3 id="1-安装-kubeadm"><a href="#1-安装-kubeadm" class="headerlink" title="1. 安装 kubeadm"></a>1. 安装 kubeadm</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/yum.repos.d/kubernetes.repo </span></span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">proxy=socks5://127.0.0.1:1080</span><br><span class="line">[root@node211 ~]<span class="comment"># yum install kubeadm kubelet</span></span><br><span class="line">[root@node211 ~]<span class="comment"># which kubeadm </span></span><br><span class="line">/usr/bin/kubeadm</span><br></pre></td></tr></table></figure><h3 id="2-安装-keepalived"><a href="#2-安装-keepalived" class="headerlink" title="2. 安装 keepalived"></a>2. 安装 keepalived</h3><p>所有 master 节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install keepalived </span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install keepalived</span></span><br></pre></td></tr></table></figure><p>编辑 node211 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 100         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编辑 node212 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 90         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编辑 node213 配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 70         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>node211, node212, node213 重启 keepalived：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl restart keepalived</span></span><br></pre></td></tr></table></figure><p>因为 node211 优先级最高，此时 VIP 192.168.77.219 应该在 node211 节点，查看 node211 节点 IP：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># ip ad </span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 52:54:00:42:fd:a6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.77.211/20 brd 192.168.79.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.77.219/20 scope global secondary eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::5554:b212:7895:c8ad/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::3e97:25b9:cc1a:809c/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::7a4f:3726:af17:18bf/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </span><br><span class="line">    link/ether 02:42:6f:0e:81:59 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>配置无异常，node211,node212,node213 设置开机自启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br></pre></td></tr></table></figure><h3 id="3-安装-haproxy"><a href="#3-安装-haproxy" class="headerlink" title="3. 安装 haproxy"></a>3. 安装 haproxy</h3><p>所有 master 节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install haproxy</span></span><br></pre></td></tr></table></figure><p>编辑所有 master 节点配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/haproxy/haproxy.cfg </span></span><br><span class="line">global</span><br><span class="line">        chroot  /var/lib/haproxy</span><br><span class="line">        daemon</span><br><span class="line">        group haproxy</span><br><span class="line">        user haproxy</span><br><span class="line">        <span class="built_in">log</span> 127.0.0.1:514 local0 warning</span><br><span class="line">        pidfile /var/lib/haproxy.pid</span><br><span class="line">        maxconn 20000</span><br><span class="line">        spread-checks 3</span><br><span class="line">        nbproc 8</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">        <span class="built_in">log</span>     global</span><br><span class="line">        mode    tcp</span><br><span class="line">        retries 3</span><br><span class="line">        option redispatch</span><br><span class="line"></span><br><span class="line">listen https-apiserver</span><br><span class="line">        <span class="built_in">bind</span> *:8443</span><br><span class="line">        mode tcp</span><br><span class="line">        balance roundrobin</span><br><span class="line">        timeout server 900s</span><br><span class="line">        timeout connect 15s</span><br><span class="line"></span><br><span class="line">        server m1 192.168.77.211:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m2 192.168.77.212:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m3 192.168.77.213:6443 check port 6443 inter 5000 fall 5</span><br></pre></td></tr></table></figure><p>所有 master 节点启动 haproxy，并设置 开机自启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br></pre></td></tr></table></figure><h3 id="4-编写-kubeadm-配置文件"><a href="#4-编写-kubeadm-配置文件" class="headerlink" title="4. 编写 kubeadm 配置文件"></a>4. 编写 kubeadm 配置文件</h3><p>在 node211 节点编写 kubeadm 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat kubeadm-init.yaml</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">"192.168.77.219:8443"</span></span><br><span class="line">dns:</span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">kubernetesVersion: v1.14.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: <span class="string">"10.123.0.0/16"</span></span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: <span class="string">"ipvs"</span></span><br></pre></td></tr></table></figure><h3 id="5-初始化"><a href="#5-初始化" class="headerlink" title="5. 初始化"></a>5. 初始化</h3><p>在 node211 节点执行初始化操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubeadm init --config=kubeadm-init.yaml --experimental-upload-certs</span></span><br><span class="line">[init] Using Kubernetes version: v1.14.3</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node211"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node211"</span>: lookup node211 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node211 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.211 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"sa"</span> key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"kubelet.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[etcd] Creating static Pod manifest <span class="keyword">for</span> <span class="built_in">local</span> etcd <span class="keyword">in</span> <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[<span class="built_in">wait</span>-control-plane] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="string">"/etc/kubernetes/manifests"</span>. This can take up to 4m0s</span><br><span class="line">[kubelet-check] Initial timeout of 40s passed.</span><br><span class="line">[apiclient] All control plane components are healthy after 107.014141 seconds</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap <span class="string">"kubelet-config-1.14"</span> <span class="keyword">in</span> namespace kube-system with the configuration <span class="keyword">for</span> the kubelets <span class="keyword">in</span> the cluster</span><br><span class="line">[upload-certs] Storing the certificates <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: ptuvy5.hl4rzxugpxpgkgkh</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstrap-token] creating the <span class="string">"cluster-info"</span> ConfigMap <span class="keyword">in</span> the <span class="string">"kube-public"</span> namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following <span class="built_in">command</span> on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">    --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted <span class="keyword">in</span> two hours; If necessary, you can use </span><br><span class="line"><span class="string">"kubeadm init phase upload-certs --experimental-upload-certs"</span> to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS     ROLES    AGE   VERSION</span><br><span class="line">node211   NotReady   master   82s   v1.14.3</span><br></pre></td></tr></table></figure><h3 id="6-部署-flannel-网络插件"><a href="#6-部署-flannel-网络插件" class="headerlink" title="6. 部署 flannel 网络插件"></a>6. 部署 flannel 网络插件</h3><p>在 node211 节点部署 flannel 插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure><p>查看部署状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            0/1     Pending   0          3m17s</span><br><span class="line">coredns-d5947d4b-zdptx            0/1     Pending   0          3m17s</span><br><span class="line">etcd-node211                      1/1     Running   0          2m48s</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          2m28s</span><br><span class="line">kube-controller-manager-node211   1/1     Running   0          2m59s</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          36s</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          3m16s</span><br><span class="line">kube-scheduler-node211            1/1     Running   0          2m41s</span><br></pre></td></tr></table></figure><h3 id="7-添加其他-master-节点"><a href="#7-添加其他-master-节点" class="headerlink" title="7. 添加其他 master 节点"></a>7. 添加其他 master 节点</h3><p>按照 node211 初始化提示，在 node212 节点及 node213 节点添加到集群，角色为 master：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node212"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node212"</span>: lookup node212 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_sh ip_vs_rr ip_vs_wrr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node212 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.212 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   7m48s   v1.14.3</span><br><span class="line">node212   Ready    master   66s     v1.14.3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node213"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node213"</span>: lookup node213 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node213 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.213 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   11m     v1.14.3</span><br><span class="line">node212   Ready    master   4m39s   v1.14.3</span><br><span class="line">node213   Ready    master   72s     v1.14.3</span><br></pre></td></tr></table></figure><h3 id="8-添加其他-node-节点"><a href="#8-添加其他-node-节点" class="headerlink" title="8. 添加其他 node 节点"></a>8. 添加其他 node 节点</h3><p>按照 node211 初始化提示，添加 node214 节点到集群，角色为 node：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@node214 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 </span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node214"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node214"</span>: lookup node214 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> on the control-plane to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>至此 kubeadm 配合 keepalived &amp; haproxy 搭建高可用集群就完成了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   4h10m   v1.14.3</span><br><span class="line">node212   Ready    master   4h3m    v1.14.3</span><br><span class="line">node213   Ready    master   4h      v1.14.3</span><br><span class="line">node214   Ready    &lt;none&gt;   3h57m   v1.14.3</span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            1/1     Running   0          4h10m</span><br><span class="line">coredns-d5947d4b-zdptx            1/1     Running   0          4h10m</span><br><span class="line">etcd-node211                      1/1     Running   0          4h9m</span><br><span class="line">etcd-node212                      1/1     Running   0          4h3m</span><br><span class="line">etcd-node213                      1/1     Running   0          4h</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          4h9m</span><br><span class="line">kube-apiserver-node212            1/1     Running   1          4h3m</span><br><span class="line">kube-apiserver-node213            1/1     Running   0          3h59m</span><br><span class="line">kube-controller-manager-node211   1/1     Running   1          4h9m</span><br><span class="line">kube-controller-manager-node212   1/1     Running   0          4h2m</span><br><span class="line">kube-controller-manager-node213   1/1     Running   0          3h59m</span><br><span class="line">kube-flannel-ds-amd64-gchpj       1/1     Running   0          4h</span><br><span class="line">kube-flannel-ds-amd64-mx44p       1/1     Running   0          3h57m</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          4h7m</span><br><span class="line">kube-flannel-ds-amd64-x9rm7       1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-fj448                  1/1     Running   0          4h</span><br><span class="line">kube-proxy-jmhm7                  1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-s7jdf                  1/1     Running   0          3h57m</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          4h10m</span><br><span class="line">kube-scheduler-node211            1/1     Running   1          4h9m</span><br><span class="line">kube-scheduler-node212            1/1     Running   0          4h2m</span><br><span class="line">kube-scheduler-node213            1/1     Running   0          3h59m</span><br></pre></td></tr></table></figure><h3 id="HA-机制"><a href="#HA-机制" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>由集群节点上运行的 keepalived &amp; haproxy 提供 VIP &amp; LB，集群中所有节点的 kubelet 连接至 VIP:<haproxy port> EndPoints。</haproxy></p><p>当 VIP 所在节点发生故障，VIP 切换到集群中其他 master 节点，即可正常提供服务。</p><h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><ol><li>kubeadm 需要正常网络支持，需要确保自己处于正常网络环境下；</li><li>kubeadm 在添加节点时，有可能会 hang 住，未查明原因；</li><li>kubeadm 默认生成证书有效期为 1年，若想要修改，则需要手动生成证书替换；</li><li>…</li></ol><h2 id="kubespray"><a href="#kubespray" class="headerlink" title="kubespray"></a>kubespray</h2><p>因为 kubespray 项目主要使用 ansible 配合 kubeadm 部署，具体内容可以直接查看 github 文档，因此不详细记录具体步骤。</p><h3 id="环境信息-1"><a href="#环境信息-1" class="headerlink" title="环境信息"></a>环境信息</h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.201</td><td>master</td></tr><tr><td>192.168.77.202</td><td>master</td></tr><tr><td>192.168.77.203</td><td>master </td></tr><tr><td>192.168.77.204</td><td>node</td></tr></tbody></table><h3 id="1-安装-kubespray"><a href="#1-安装-kubespray" class="headerlink" title="1. 安装 kubespray"></a>1. 安装 kubespray</h3><p>在 GitHub <a href="https://github.com/kubernetes-sigs/kubespray/releases" target="_blank" rel="noopener">项目链接</a>上下载最新 Release 版本代码。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 ~]<span class="comment"># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.10.3.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="2-安装必要依赖"><a href="#2-安装必要依赖" class="headerlink" title="2. 安装必要依赖"></a>2. 安装必要依赖</h3><p>项目依赖于 Python3，所以这里采用 Python3.6 版本进行安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36-pip</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># pip3 install -r requirements.txt</span></span><br></pre></td></tr></table></figure><ol start="3"><li>生成 ansible inventory</li></ol><p>项目默认提供了一个 Python 脚本用于自动生成 inventory，该脚本生成 inventory 通常需要根据实际情况自己调整。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cp -rfp inventory/sample inventory/mycluster</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># declare -a IPS=(192.168.77.201 192.168.77.202 192.168.77.203 192.168.77.203)</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py $&#123;IPS[@]&#125;</span></span><br><span class="line">DEBUG: Adding group all</span><br><span class="line">DEBUG: Adding group kube-master</span><br><span class="line">DEBUG: Adding group kube-node</span><br><span class="line">DEBUG: Adding group etcd</span><br><span class="line">DEBUG: Adding group k8s-cluster</span><br><span class="line">DEBUG: Adding group calico-rr</span><br><span class="line">DEBUG: Skipping existing host 192.168.77.203.</span><br><span class="line">DEBUG: adding host node1 to group all</span><br><span class="line">DEBUG: adding host node2 to group all</span><br><span class="line">DEBUG: adding host node3 to group all</span><br><span class="line">DEBUG: adding host node1 to group etcd</span><br><span class="line">DEBUG: adding host node2 to group etcd</span><br><span class="line">DEBUG: adding host node3 to group etcd</span><br><span class="line">DEBUG: adding host node1 to group kube-master</span><br><span class="line">DEBUG: adding host node2 to group kube-master</span><br><span class="line">DEBUG: adding host node1 to group kube-node</span><br><span class="line">DEBUG: adding host node2 to group kube-node</span><br><span class="line">DEBUG: adding host node3 to group kube-node</span><br></pre></td></tr></table></figure><p>查看生成 inventory 结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cat inventory/mycluster/hosts.yml </span></span><br><span class="line">all:</span><br><span class="line">  hosts:</span><br><span class="line">    node1:</span><br><span class="line">      ansible_host: 192.168.77.201</span><br><span class="line">      ip: 192.168.77.201</span><br><span class="line">      access_ip: 192.168.77.201</span><br><span class="line">    node2:</span><br><span class="line">      ansible_host: 192.168.77.202</span><br><span class="line">      ip: 192.168.77.202</span><br><span class="line">      access_ip: 192.168.77.202</span><br><span class="line">    node3:</span><br><span class="line">      ansible_host: 192.168.77.203</span><br><span class="line">      ip: 192.168.77.203</span><br><span class="line">      access_ip: 192.168.77.203</span><br><span class="line">  children:</span><br><span class="line">    kube-master:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">    kube-node:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    etcd:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    k8s-cluster:</span><br><span class="line">      children:</span><br><span class="line">        kube-master:</span><br><span class="line">        kube-node:</span><br><span class="line">    calico-rr:</span><br><span class="line">      hosts: &#123;&#125;</span><br></pre></td></tr></table></figure><p>可以看到跟我们计划中的有所差别，根据实际情况调整 kube-master 数量即可。</p><h3 id="4-编写部署配置参数"><a href="#4-编写部署配置参数" class="headerlink" title="4. 编写部署配置参数"></a>4. 编写部署配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/all/all.yml</code> 路径下包含了一些全局配置，比如 proxy 之类的，可以手动调整。</p><h3 id="5-编写-k8s-配置参数"><a href="#5-编写-k8s-配置参数" class="headerlink" title="5. 编写 k8s 配置参数"></a>5. 编写 k8s 配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml</code> 路径下包含了 k8s 所有配置项，根据实际情况编辑修改。</p><h3 id="6-部署"><a href="#6-部署" class="headerlink" title="6. 部署"></a>6. 部署</h3><p>在所有准备工作完成后，执行部署操作。</p><p>注意， Kubespray 部署的前提条件是你的网络是一个正常的网络，可以正常访问所有网站，若无法访问，则根据自身实际情况，调整配置，配置路径为： <code>roles/download/defaults/main.yml</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml</span><br></pre></td></tr></table></figure><p>等待部署完成即可。</p><h3 id="HA-机制-1"><a href="#HA-机制-1" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>集群中所有的 Node 节点自己启动一个 Nginx Static Pod，用于代理转发，将所有指定 <code>127.0.0.1:6443</code> 的请求转发至所有 master 节点真实 apiserver ，这样所有的 kubelet 只需要自己节点即可，无需其他节点参与。</p><h3 id="坑-1"><a href="#坑-1" class="headerlink" title="坑"></a>坑</h3><ol><li>CentOS 默认 Python2.7，需要单独安装 Python3.6</li><li>通过 pip 安装依赖，部分软件包需要 gcc,python36-devel,openssl-devel 等依赖包，需要根据错误提示自行安装，文档中没有提到</li><li>默认会安装 docker &amp; containerd 服务，但是 containerd 服务未设置开机自启动，会导致 docker 无法自动运行</li><li>在安装过程中，会安装 selinux 相应 Python 库，但是该依赖未在 <code>requirements.txt</code> 声明</li><li>…</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论是直接只用 kubeadm + vip 方式部署 HA 集群，还是通过 Kubespray 部署，在网络正常情况下，是很快可以完成的。</p><p>在使用 kubeadm 过程中，因为无需引入第三方依赖库，导致整体流程顺畅，体验极佳。</p><p>在 Kubespray 过程中，因为采用 Python3 方式，但相关依赖又未显示声明，导致部署过程繁琐。但是也比较好理解，Kubespray 作为一个致力于部署企业级 k8s 集群的项目，需要处理大量的边界条件了，这个项目中 YAML 就写了 15k 行，可见一斑。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;p&gt;本文所有节点 OS 均为 CentOS 7.4 。&lt;/p&gt;
&lt;h3 id=&quot;1-关闭-selinux&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何查看磁盘扇区大小</title>
    <link href="https://zdyxry.github.io/2019/06/05/%E6%9F%A5%E7%9C%8B%E7%A3%81%E7%9B%98%E6%89%87%E5%8C%BA%E5%A4%A7%E5%B0%8F/"/>
    <id>https://zdyxry.github.io/2019/06/05/查看磁盘扇区大小/</id>
    <published>2019-06-05T12:45:06.000Z</published>
    <updated>2019-06-05T12:54:40.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="磁盘扇区"><a href="#磁盘扇区" class="headerlink" title="磁盘扇区"></a>磁盘扇区</h2><p>引用维基百科：</p><blockquote><p>In computer disk storage, a sector is a subdivision of a track on a magnetic disk or optical disc. Each sector stores a fixed amount of user-accessible data, traditionally 512 bytes for hard disk drives (HDDs) and 2048 bytes for CD-ROMs and DVD-ROMs. Newer HDDs use 4096-byte (4 KiB) sectors, which are known as the Advanced Format (AF).</p></blockquote><p>扇区大小常见的可以分为 512 bytes, 2048 bytes 和 4096 bytes。</p><h2 id="查看扇区大小"><a href="#查看扇区大小" class="headerlink" title="查看扇区大小"></a>查看扇区大小</h2><p>通过 lsblk 命令可以查看。</p><p>物理扇区 512 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:51:59 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdg</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdg           512     512</span><br><span class="line">├─sdg1        512     512</span><br><span class="line">├─sdg2        512     512</span><br><span class="line">├─sdg3        512     512</span><br><span class="line">└─sdg4        512     512</span><br></pre></td></tr></table></figure><p>物理扇区 4096 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:52:03 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdb</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdb          4096     512</span><br><span class="line">├─sdb1       4096     512</span><br><span class="line">├─sdb2       4096     512</span><br><span class="line">├─sdb3       4096     512</span><br><span class="line">└─sdb4       4096     512</span><br></pre></td></tr></table></figure><p>CDROM 比较特殊，是 2048 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 21:07:35 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sr0</span><br><span class="line">NAME PHY-SEC LOG-SEC</span><br><span class="line">sr0     2048    2048</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;磁盘扇区&quot;&gt;&lt;a href=&quot;#磁盘扇区&quot; class=&quot;headerlink&quot; title=&quot;磁盘扇区&quot;&gt;&lt;/a&gt;磁盘扇区&lt;/h2&gt;&lt;p&gt;引用维基百科：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In computer disk storage, a sector
      
    
    </summary>
    
    
      <category term="Hardware" scheme="https://zdyxry.github.io/tags/Hardware/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-集群部署</title>
    <link href="https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://zdyxry.github.io/2019/05/31/Kubernetes-实战-集群部署/</id>
    <published>2019-05-31T13:22:51.000Z</published>
    <updated>2019-05-31T15:06:10.382Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。</p><p>说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。</p><p>本文并不会介绍具体的部署步骤，望周知。</p><h2 id="Kubernetes-主要组件"><a href="#Kubernetes-主要组件" class="headerlink" title="Kubernetes 主要组件"></a>Kubernetes 主要组件</h2><p>因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：</p><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><ul><li><p>apiserver： 集群中所有其他组件通过 apiserver 进行交互</p></li><li><p>scheduler： 按照 Pod 配置来对 Pod 进行节点调度</p></li><li><p>controller-manager：负责节点管理，资源的具体创建动作， <code>desired state management</code> 具体实行者</p></li><li><p>etcd：用于存储集群中数据的键值存储</p></li></ul><h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><ul><li><p>kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器</p></li><li><p>kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信</p></li><li><p>容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O</p></li></ul><h2 id="Kubernetes-集群准备"><a href="#Kubernetes-集群准备" class="headerlink" title="Kubernetes 集群准备"></a>Kubernetes 集群准备</h2><h3 id="所需资源"><a href="#所需资源" class="headerlink" title="所需资源"></a>所需资源</h3><p>大部分的安装文档中，都会先写明 os 要求，计算 &amp; 存储资源需求，k8s 自身对资源消耗很低，通常的 2c4g + 30GiB 足够运行起来。</p><p>上面说完了硬件资源，那么我们来说下软件资源， k8s 作为一个容器编排系统，它需要的软件资源也是很重要的一部分，这里我们来说一下网络部分。</p><p>假设我们集群中存在 5 个节点，使用 <code>kubeadm init</code> 方式部署集群，那么最基本的，需要 5 个节点的 IP。那如果是高可用的集群呢？我们需要加一个 VIP，也就是 6 个 IP 地址。</p><p>在考虑了 IP 地址之后，我们来说下网段划分，以 flannel 为例，在创建网络时，每个 k8s 节点都会分配一段子网用于 Pod 分配 IP，这里的网段是不可以跟宿主机的网络重叠的，所以这里的网络划分也是一个很重要的资源。</p><p>具体其他网络类型，我会在之后的网络部分详细的写一下。</p><h3 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h3><p>好了，现在我们已经有了资源了（无论是硬件资源还是软件资源），那么我们可以部署了，那此时采用什么方式部署，或者说怎么部署成了问题。</p><hr><p>首先说下最特殊的服务，kubelet，它作为节点的实际管控者，它如果运行在容器中，那么谁来控制kubelet 容器的启停呢？当节点故障恢复后，又如何自启动呢？最开始我使用 Rancher 部署的时候发现他们是将 kubelet 直接部署在容器中的，那是因为他们在节点上还有其他 Agent 用于管理节点，Rancher 相当于 k8s 集群之外的上帝，管控着一切。</p><p>当我们没有 Rancher 这类管理工具时，还是老老实实将 kubelet 以服务的形式部署在宿主机上吧。</p><hr><p>官方推荐使用 <code>kubeadm</code> 进行集群部署，简单快捷，只是还在快速迭代中，存在较多不确定性，那么现在那些大厂是如何部署的呢？</p><p>我花了点时间阅读了下 Github 上面一些关于 k8s 部署项目，简单的罗列一下：</p><table><thead><tr><th>项目名称</th><th>项目地址</th><th>星</th><th>服务运行方式</th><th>ha </th></tr></thead><tbody><tr><td>ansible-kubeadm</td><td><a href="https://github.com/4admin2root/ansible-kubeadm" target="_blank" rel="noopener">https://github.com/4admin2root/ansible-kubeadm</a></td><td>3</td><td>Static Pod</td><td>- </td></tr><tr><td>ansible-kubeadm-ha-cluster</td><td><a href="https://github.com/sv01a/ansible-kubeadm-ha-cluster" target="_blank" rel="noopener">https://github.com/sv01a/ansible-kubeadm-ha-cluster</a></td><td>5</td><td>Docker</td><td>keepvalied </td></tr><tr><td>kubeadm-playbook</td><td><a href="https://github.com/ReSearchITEng/kubeadm-playbook" target="_blank" rel="noopener">https://github.com/ReSearchITEng/kubeadm-playbook</a></td><td>117</td><td>Static Pod</td><td>keepalived </td></tr><tr><td>Kubernetes-ansible</td><td><a href="https://github.com/zhangguanzhang/Kubernetes-ansible" target="_blank" rel="noopener">https://github.com/zhangguanzhang/Kubernetes-ansible</a></td><td>208</td><td>Service</td><td>keepalived &amp; Haproxy</td></tr><tr><td>kubeadm-ansible</td><td><a href="https://github.com/kairen/kubeadm-ansible" target="_blank" rel="noopener">https://github.com/kairen/kubeadm-ansible</a></td><td>281</td><td>Static Pod</td><td>- </td></tr><tr><td>kubeadm-ha</td><td><a href="https://github.com/cookeem/kubeadm-ha" target="_blank" rel="noopener">https://github.com/cookeem/kubeadm-ha</a></td><td>502</td><td>Service</td><td>keepalived &amp; nginx </td></tr><tr><td>kubeasz</td><td><a href="https://github.com/easzlab/kubeasz" target="_blank" rel="noopener">https://github.com/easzlab/kubeasz</a></td><td>2987</td><td>Service</td><td>keepalived &amp; Haproxy </td></tr></tbody></table><p>可以看到虽然有些细微的差别，但是大家做的都围绕着一个目的，就是把上一节提到的 k8s 所有必要组件部署到集群中，我们根据服务运行方式和 HA 方式来说一下。</p><h2 id="服务运行方式"><a href="#服务运行方式" class="headerlink" title="服务运行方式"></a>服务运行方式</h2><p>根据我上面总结的各个项目，大体分为 3 类，分别是：Host Service、Docker、Static Pod。我们一个一个的过一下。</p><h3 id="Host-Service"><a href="#Host-Service" class="headerlink" title="Host Service"></a>Host Service</h3><p>在没有容器的时代，我们要部署一个服务，都是采用 Host Service 方式，我们在宿主机的 OS 上配置一个服务，管理方式可能是 init.d ，也可能是 systemd 。k8s 所有的服务都可以通过 Host Service 方式运行。</p><p>优点：</p><p>在 systemd 大法加持下，所有服务均可通过 systemd 统一管理，可以配置随着系统进行启停。</p><p>缺点：</p><p>如果通过 systemd 方式部署，那么服务配置修改、服务升级是一个大麻烦。</p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>既然我们觉得 Host Service 的方式服务配置修改、升级都比较麻烦，那我们直接通过 Docker 启动就好了，升级直接更新 image 重新启动，一切问题放佛都解决了。</p><p>但是考虑一个问题，当集群全部掉电，系统开机后，k8s 如何自启动？以 Docker 作为容器运行时的基础上，Docker 比较让人诟病的一点就是它是一个 Daemon，在这里反而是优点，我们可以设置 Docker 随开机自启动，并将 k8s 所有服务对应镜像设置为 <code>--restart=always</code>，就可以解决这个问题。</p><p>优点：</p><p>服务配置、升级方便。</p><p>缺点：</p><p>依赖于 Docker，若更换其他 CRI，无法处理极端情况。</p><h3 id="Static-Pod"><a href="#Static-Pod" class="headerlink" title="Static Pod"></a>Static Pod</h3><p>最后我们来说说 Static Pod，这种方式是 kubeadm 目前所采用的方式，也是 k8s 所推荐的方式。</p><p>什么是 Static Pod？其实就是字面意义， <code>静态 Pod</code>，k8s 不去调度的 Pod，而是由 kubelet 直接管理。前面我们在讨论 kubelet 提到，kubelet 是以服务的形式运行在宿主机上的，那么也就是 kubelet 的启停是通过 systemd 控制的，不受 k8s 控制。</p><p>Static Pod 由 kubelet 控制，当 kubelet 启动后，会自动拉起 Static Pod，且 Static Pod 不受 k8s 控制，无论是通过 kubectl 进行删除，还是通过 docker 对该 Pod 进行 stop 动作，kubelet 都会保证 Static Pod 正常运行。</p><p>这个方式很适合我们来处理 k8s 自身的服务，比如 apiserver/scheduler ，每个 master 节点上通过 Static Pod 配置，当 kubelet 启动后，自动拉起 apiserver/scheduler 等服务，那么 k8s 集群也就自启动了，也就解决了我们上面提到的集群全部掉电的情况。</p><p>优点：</p><p>跟随 kubelet 启停，完美适配 k8s 升级场景。</p><p>缺点：</p><p>因为随着 kubelet 启停，所以导致更新 kubelet 配置时需要指定 manifest 文件路径。</p><h2 id="HA-配置"><a href="#HA-配置" class="headerlink" title="HA 配置"></a>HA 配置</h2><p>k8s 作为一个基础架构服务，它的可用性关乎着我们整个业务的稳定，所以一定要保证不会出现单点故障。</p><p>在公有云场景下，由公有云提供 LB 的支持，只要我们部署了多个 master 节点，就不用担心了。</p><p>那在裸机场景下怎么办呢？</p><p>目前通用解决方案是 VIP 配合 LB（也就是 keepalived &amp; haproxy/nginx）。</p><p> keepalived 提供了 VIP。在 k8s 集群部署过程中，所有节点都指定 <code>controlPlaneEndpoint</code> 为 VIP，而 VIP 在集群中的一个 master 节点上。当 VIP 所在节点故障时， VIP 自动漂浮到集群中其他 master 节点上，保证高可用。</p><p> haproxy/nginx 作为 LB，当我们 k8s 集群中所有节点都连接一个 apiserver 时，通过 LB 按照既定策略将请求分发到集群中多个 master 节点上，保证性能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于 k8s 集群的部署大概就是上述这些内容，具体的操作步骤都可以通过官网或者 github 找到相关资料。目前如果个人学习的话，直接通过 kubeadm 部署就好；如果想要在生产环境中部署，那么需要根据自身业务类型，仔细考虑自己是否需要 HA，如何配置 HA。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>BIOS vs UEFI</title>
    <link href="https://zdyxry.github.io/2019/05/26/BIOS-vs-UEFI/"/>
    <id>https://zdyxry.github.io/2019/05/26/BIOS-vs-UEFI/</id>
    <published>2019-05-26T00:46:23.000Z</published>
    <updated>2019-05-26T00:47:06.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>大家应该都安装过操作系统，PC 或者服务器上。那么我们在安装操作系统时通常需要进入到 BIOS 或 UEFI 界面去安装。之前维护的一个 ISO 版本只支持 BIOS，最近有了支持 UEFI 安装的需求，今天来了解一下其中的差异，之后尝试编写一个支持 UEFI 的 KickStart 配置。</p><h2 id="Legacy-BIOS"><a href="#Legacy-BIOS" class="headerlink" title="Legacy BIOS"></a>Legacy BIOS</h2><p>按照惯例，引用维基百科中（不同语言对于名词解释信息可能是完全不同的，最好直接看英文）的解释：</p><blockquote><p>BIOS (/ˈbaɪɒs/ BY-oss; an acronym for Basic Input/Output System and also known as the System BIOS, ROM BIOS or PC BIOS) is non-volatile firmware used to perform hardware initialization during the booting process (power-on startup), and to provide runtime services for operating systems and programs.[1] The BIOS firmware comes pre-installed on a personal computer’s system board, and it is the first software to run when powered on. The name originates from the Basic Input/Output System used in the CP/M operating system in 1975.[2][3] The BIOS originally proprietary to the IBM PC has been reverse engineered by companies looking to create compatible systems. The interface of that original system serves as a de facto standard.</p></blockquote><p>主要功能有：</p><ul><li>POST - 在加载操作系统之前， 检测硬件确保没有错误</li><li>Bootstrap Loader - 寻找引导加载程序，并将控制权转</li><li>BIOS 驱动程序 - 低级驱动程序，使计算机可以对计算机硬件进行基本操作控制</li><li>BIOS/CMOS 设置 - 允许配置硬件设置，包括系统设置，如计算机密码，硬件时钟等</li></ul><h3 id="BIOS-vs-CMOS"><a href="#BIOS-vs-CMOS" class="headerlink" title="BIOS vs CMOS"></a>BIOS vs CMOS</h3><p>更改BIOS配置时，设置不会存储在BIOS芯片本身。相反，它们存储在一个特殊的存储芯片上，称为<code>CMOS</code>。</p><p>与大多数RAM芯片一样，存储BIOS设置的芯片使用CMOS工艺制造。它包含少量数据，通常为256 个字节。CMOS 芯片上的信息包括计算机上安装的磁盘驱动器类型，系统时钟的当前日期和时间以及计算机的引导顺序。</p><p>BIOS是非易失性的：即使计算机没电也会保留其信息，因为即使计算机已关闭，计算机也需要记住其BIOS设置。这就是为什么CMOS有自己的专用电源，即CMOS电池。 通常我们在使用电脑的时候，如果忘记了 BIOS 密码，无法更改 BIOS 设置时， 那么可以通过拔掉 CMOS 电池，再安装即可恢复。</p><h2 id="U-EFI"><a href="#U-EFI" class="headerlink" title="(U)EFI"></a>(U)EFI</h2><blockquote><p>The Unified Extensible Firmware Interface (UEFI) is a specification that defines a software interface between an operating system and platform firmware. UEFI replaces the Basic Input/Output System (BIOS) firmware interface originally present in all IBM PC-compatible personal computers,[1][2] with most UEFI firmware implementations providing legacy support for BIOS services. UEFI can support remote diagnostics and repair of computers, even with no operating system installed.[3]</p></blockquote><p>Intel 为了解决 BIOS 的一些缺点，提出了 EFI ，后来由于各种历史原因，EFI 转变为了 UEFI，其中的 U 是 <code>Unified</code> 。</p><p>那么 BIOS 有啥缺点呢？ 对于服务器级别来说，最大的缺点可能就是不支持 2TiB 以上空间的磁盘引导，关于为什么不支持大家可以自己查阅下 MBR vs GPT 相关资料，这里不详细解释。</p><p>那么老大哥提出了 UEFI，除了解决了引导磁盘的容量限制，还有如下优点（这几点看上去就跟普通用户没啥关系）：</p><ul><li>独立于CPU的架构</li><li>独立于CPU的驱动程序</li><li>灵活的pre-OS环境，包括网络功能</li><li>模块化设计</li><li>向后和向前兼容性</li></ul><h2 id="Linux-安装识别"><a href="#Linux-安装识别" class="headerlink" title="Linux 安装识别"></a>Linux 安装识别</h2><p>了解了大概的概念，那么我们来实际看看 Linux 分别从 BIOS 启动和 UEFI 启动安装有什么不同吧，接下来示例以 CentOS 为例。</p><p>首先我们看下 CentOS ISO 中的目录结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@dell-r720xd-1 CentOS-7.4]<span class="comment"># tree . -d 1</span></span><br><span class="line">.</span><br><span class="line">├── EFI <span class="comment"># EFI 模式下引导程序路径</span></span><br><span class="line">│   └── BOOT</span><br><span class="line">│       └── fonts</span><br><span class="line">├── images <span class="comment"># 系统启动镜像</span></span><br><span class="line">│   └── pxeboot</span><br><span class="line">├── isolinux   <span class="comment"># 默认引导程序路径，包括引导选项配置，引导背景图片，引导内核镜像等</span></span><br><span class="line">├── LiveOS    <span class="comment"># 临时加载镜像</span></span><br><span class="line">├── Packages   <span class="comment"># ISO 附带所有软件包，以 RPM 形式存放</span></span><br><span class="line">└── repodata    <span class="comment"># ISO 中 YUM Repo 配置文件，保存了在只做 YUM Repo 时指定的软件组，支持语言等信息</span></span><br><span class="line">1 [error opening dir]</span><br><span class="line"></span><br><span class="line">9 directories</span><br></pre></td></tr></table></figure><h3 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h3><p>我们来看下在 BIOS 下如何指定安装 KickStart 配置：</p><p>在 <code>isolinux/isolinux.cfg</code> 中指定即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label linux</span><br><span class="line">  menu label ^Install yiran&apos;OS</span><br><span class="line">  menu default</span><br><span class="line">  kernel vmlinuz</span><br><span class="line">  append initrd=initrd.img inst.stage2=hd:LABEL=YIRANOS-2 ks=hd:LABEL=YIRANOS-2:/ks_yiranos.cfg quiet</span><br></pre></td></tr></table></figure></p><h3 id="UEFI"><a href="#UEFI" class="headerlink" title="UEFI"></a>UEFI</h3><p>跟 BIOS 一样，只是换了一个配置位置，只需要在 <code>EFI/BOOT/grub.cfg</code> 中指定 KS 配置即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">### BEGIN /etc/grub.d/10_linux ###</span><br><span class="line">menuentry &apos;Install CentOS 7&apos; --class fedora --class gnu-linux --class gnu --class os &#123;</span><br><span class="line">        linuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 inst.ks=hd:LABEL=YIRANOS-2:/ks_yiranos.cfg quiet</span><br><span class="line">        initrdefi /images/pxeboot/initrd.img</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随着越来越多服务器厂商将 UEFI 设置为默认模式，哪怕我们没有用到 UEFI 的高级功能，也最好将自己的 ISO 支持到 UEFI，避免因为 BIOS 的一些历史遗留问题导致后续技术支持出现困难。后续找时间写一下关于 UEFI 的 KickStart 配置文件。主要变化应该是在 <code>/boot</code> 分区部分有些变化，之后再说啦。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;大家应该都安装过操作系统，PC 或者服务器上。那么我们在安装操作系统时通常需要进入到 BIOS 或 UEFI 界面去安装。之前维护的一个 I
      
    
    </summary>
    
    
      <category term="Hardware" scheme="https://zdyxry.github.io/tags/Hardware/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-镜像管理</title>
    <link href="https://zdyxry.github.io/2019/05/24/Kubernetes-%E5%AE%9E%E6%88%98-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/"/>
    <id>https://zdyxry.github.io/2019/05/24/Kubernetes-实战-镜像管理/</id>
    <published>2019-05-23T23:35:08.000Z</published>
    <updated>2019-05-31T13:11:58.483Z</updated>
    
    <content type="html"><![CDATA[<h2 id="镜像组织形式"><a href="#镜像组织形式" class="headerlink" title="镜像组织形式"></a>镜像组织形式</h2><p>镜像默认采用 OverlayFS 方式挂载，最终效果是将多个目录结构合并为一个。</p><p>其中 lowerdir 为只读路径，最右层级最深。最终容器运行时会将 lowerdir 和 upperdir 合并挂在为 merged，对应容器中的路径为 <code>/</code> 。<br>举例：<br>镜像 testadd:0.5 版本的层级挂载如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:02:24 overlay2]$docker inspect testadd:0.5 |grep Dir</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/693c140b9c70744a7a6ce93de56d3ac7549dae84195cbfac3486062d1ceaccf1/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/work&quot;</span><br></pre></td></tr></table></figure><p>运行该容器后，可以看到多了一个 overlay 方式挂载的路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:05:53 overlay2]$mount |grep overlay</span><br><span class="line">/dev/md127 on /var/lib/docker/overlay2 type ext4 (rw,relatime,data=ordered)</span><br><span class="line">overlay on /var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/3NA23BH5OMSSXWTHGPRS6YENB7:/var/lib/docker/overlay2/l/QQVS7UVPGRBVHRZOBDPMMO4EQM:/var/lib/docker/overlay2/l/E7HTYBVD5SXSZRLVTETODOIANT,upperdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/diff,workdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/work)</span><br></pre></td></tr></table></figure><p>查看对应关系：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:05:53 overlay2]$mount |grep overlay</span><br><span class="line">/dev/md127 on /var/lib/docker/overlay2 type ext4 (rw,relatime,data=ordered)</span><br><span class="line">overlay on /var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/3NA23BH5OMSSXWTHGPRS6YENB7:/var/lib/docker/overlay2/l/QQVS7UVPGRBVHRZOBDPMMO4EQM:/var/lib/docker/overlay2/l/E7HTYBVD5SXSZRLVTETODOIANT,upperdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/diff,workdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/work)</span><br><span class="line">[root@node111 16:06:07 overlay2]$docker inspect testadd:0.5 |grep Dir                                                             </span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/693c140b9c70744a7a6ce93de56d3ac7549dae84195cbfac3486062d1ceaccf1/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/work&quot;</span><br><span class="line">[root@node111 16:06:54 overlay2]$docker ps </span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES</span><br><span class="line">e53180ddcd03        testadd:0.5         &quot;bash&quot;              About a minute ago   Up About a minute                       compassionate_roentgen</span><br><span class="line">[root@node111 16:07:06 overlay2]$docker inspect e53180ddcd03 |grep Dir</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00-init/diff:/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/diff:/var/lib/docker/overlay2/693c140b9c70744a7a6ce93de56d3ac7549dae84195cbfac3486062d1ceaccf1/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/work&quot;</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br></pre></td></tr></table></figure><p>查看容器内根分区，并在容器内创建文件，查看容器挂载 merged 路径下文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@e53180ddcd03 /]# ls </span><br><span class="line">anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  test  tmp  usr  var</span><br><span class="line">[root@e53180ddcd03 /]# touch yiran</span><br><span class="line">[root@e53180ddcd03 /]# ls anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  test  tmp  usr  var  yiran</span><br><span class="line">[root@e53180ddcd03 /]#</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:07:13 overlay2]$cd e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged/</span><br><span class="line">You have mail in /var/spool/mail/root</span><br><span class="line">[root@node111 16:09:31 merged]$ls </span><br><span class="line">anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  test  tmp  usr  var  yiran</span><br><span class="line">[root@node111 16:09:32 merged]$pwd</span><br><span class="line">/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged</span><br></pre></td></tr></table></figure><p>在容器中创建的文件 <code>yiran</code> 在宿主机相应的挂载路径下是可以看到的。</p><h2 id="镜像管理"><a href="#镜像管理" class="headerlink" title="镜像管理"></a>镜像管理</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>默认从 docker.io 获取最新镜像，可以在 /etc/docker/daemon.json 中指定 <code>registry-mirrors</code> 或 <code>insecure-registries</code> 获取私有镜像。</p><p>下载完成后可以看到镜像下载完成后会在 <code>/var/lib/docker/overlay2/</code> 下保存一份镜像真实内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:17:15 docker]$docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">[root@node111 16:17:19 docker]$du -sh . </span><br><span class="line">624K    .</span><br><span class="line">[root@node111 16:17:21 docker]$docker pull centos</span><br><span class="line">Using default tag: latest</span><br><span class="line">Trying to pull repository docker.io/library/centos ... </span><br><span class="line">latest: Pulling from docker.io/library/centos</span><br><span class="line">8ba884070f61: Pull complete </span><br><span class="line">Digest: sha256:b5e66c4651870a1ad435cd75922fe2cb943c9e973a9673822d1414824a1d0475</span><br><span class="line">Status: Downloaded newer image for docker.io/centos:latest</span><br><span class="line">[root@node111 16:18:06 docker]$docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/centos    latest              9f38484d220f        2 months ago        202 MB</span><br><span class="line">[root@node111 16:18:17 docker]$du -sh . </span><br><span class="line">214M    .</span><br><span class="line">[root@node111 16:18:19 docker]$ll overlay2/*</span><br><span class="line">overlay2/6061bd16e5de86e56298bee1496e02998e6a029c34374b21bc0fa3c30202db55:</span><br><span class="line">total 8</span><br><span class="line">drwxr-xr-x 16 root root 4096 May 22 16:18 diff</span><br><span class="line">-rw-r--r--  1 root root   26 May 22 16:17 link</span><br><span class="line"></span><br><span class="line">overlay2/l:</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx 1 root root 72 May 22 16:17 A3LUDFNM6LHHPQ6DVXCEI2KFYQ -&gt; ../6061bd16e5de86e56298bee1496e02998e6a029c34374b21bc0fa3c30202db55/diff</span><br></pre></td></tr></table></figure><h3 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h3><p>在不同服务构建镜像时，应保证最小化且合理分层，这样可以在最底层使用相同的 overlay 缓存，比较空间浪费。<br>参考 OpenStack Kolla 项目层级结构，openstack 所有服务镜像均基于 CentOS，60+服务镜像打包占用总空间为 5.48G：</p><p>注意：<br>尽量使用最精简基础镜像，只安装必要软件包<br>合理拆分服务<br>尽量保证 Dockerfile 中上层指令相同，若顺序不同则会构建出不同的层级，无法利用缓存特性<br>保证层级处于最精简状态<br>…</p><h3 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h3><p>当我们本地存在一份镜像，想要将其上传至指定仓库，我们需要先对镜像打 tag，举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:24:55 image]$docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">testadd             0.2                 4e47f016385b        35 seconds ago      202 MB</span><br><span class="line">docker.io/centos    latest              9f38484d220f        2 months ago        202 MB</span><br><span class="line">[root@node111 16:25:07 image]$cat /etc/docker/daemon.json </span><br><span class="line">&#123;</span><br><span class="line">  &quot;insecure-registries&quot;: [</span><br><span class="line">    &quot;192.168.27.146&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">[root@node111 16:25:12 image]$docker tag testadd:0.2 192.168.27.146/testadd:0.2</span><br><span class="line">[root@node111 16:25:28 image]$docker images</span><br><span class="line">REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">192.168.27.146/testadd   0.2                 4e47f016385b        58 seconds ago      202 MB</span><br><span class="line">testadd                  0.2                 4e47f016385b        58 seconds ago      202 MB</span><br><span class="line">docker.io/centos         latest              9f38484d220f        2 months ago        202 MB</span><br><span class="line">[root@node111 16:25:31 image]$docker push 192.168.27.146/testadd:0.2</span><br><span class="line">The push refers to a repository [192.168.27.146/testadd]</span><br><span class="line">f5011c15a820: Pushed </span><br><span class="line">d69483a6face: Layer already exists </span><br><span class="line">0.2: digest: sha256:072611b154402d0760d7860374eb5dc706712319331710b4f046e043ba2cc26a size: 736</span><br></pre></td></tr></table></figure><p>上传到指定仓库之后，其他节点可以修改 docker 配置文件，重启 docker 后即可直接下载指定镜像。</p><h2 id="Kubernetes-镜像管理"><a href="#Kubernetes-镜像管理" class="headerlink" title="Kubernetes 镜像管理"></a>Kubernetes 镜像管理</h2><h3 id="下载-1"><a href="#下载-1" class="headerlink" title="下载"></a>下载</h3><p>在 k8s 中，没有针对镜像仓库的集群级别配置，节点各自维护自己的仓库地址，如果需要增加仓库地址，需要修改集群中所有节点配置文件，重启 docker 生效。</p><hr><p><strong>20190531 更新</strong></p><p>针对 k8s 中私有仓库的使用更新：</p><ol><li>当 k8s 想要配置 http 私有仓库时，只能通过在节点上修改 docker 配置文件 /etc/docker/daemon.json ，添加  “insecure-registries” 字段，并重启 docker 后，k8s 可以自动拉取所需镜像；</li><li>当 k8s 配置 https 私有仓库时，只需将根证书拷贝到 k8s 节点的 /etc/docker/certs.d/&lt;domain.com&gt;/ 下，创建 k8s secret docker-registry ，在 YAML 中指定拉取镜像所需的 secret，就可以自动拉取了。</li></ol><hr><p>可以在 k8s 中配置指定仓库的 secret 类型为 docker-registry 来配置私有仓库的用户名密码，在之后创建 Pod 时指定该 secret 名称即可自动下载，具体操作如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node3 ~]# kubectl create secret docker-registry regsecret --docker-server=192.168.30.111 --docker-username=admin --docker-password=Harbor12345 --docker-email=yiran@smartx.com</span><br><span class="line">[root@node3 ~]# kubectl get secret regsecret -o yaml</span><br><span class="line">[root@node3 ~]# cat private-reg-pod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: private-reg</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector:</span><br><span class="line">    type: &quot;233&quot;</span><br><span class="line">  containers:</span><br><span class="line">  - name: private-reg-container</span><br><span class="line">    image: 192.168.30.111/test/busybox:latest</span><br><span class="line">    args: [/bin/sh, -c,</span><br><span class="line">           &apos;i=0; while true; do echo &quot;$i: $(date)&quot;; i=$((i+1)); sleep 1; done&apos;]</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: regsecret</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>k8s 自身不提供主动删除节点中无用镜像操作，默认通过配置 GC 参数删除无用镜像。</p><p>不推荐使用其它管理工具或手工进行容器和镜像的清理，因为kubelet需要通过容器来判断pod的运行状态，如果使用其它方式清除容器有可能影响kubelet的正常工作。</p><ul><li><p>–image-gc-high-threshold int32<br>当用于存储镜像的磁盘使用率达到百分之–image-gc-high-threshold时将触发镜像回收(default 85)</p></li><li><p>–image-gc-low-threshold int32<br>删除最近最久未使用（LRU，Least Recently Used）的镜像直到磁盘使用率降为百分之–image-gc-low-threshold或无镜像可删为止 (default 80)</p></li></ul><h2 id="CNCF-Harbor"><a href="#CNCF-Harbor" class="headerlink" title="CNCF-Harbor"></a>CNCF-Harbor</h2><p>Harbor项目是一个具有存储、签署和扫描内容功能的开源云原生registry。Harbor 由 VMware 创建，通过添加用户所需功能（如安全性，身份认证和管理）来扩展开源Docker Distribution，并支持在registry之间复制镜像。Harbor还提供高级安全功能，比如漏洞分析，基于角色的访问控制，活动审计等等。<br>主要功能：</p><ul><li>角色控制/身份校验</li><li>镜像复制</li><li>漏洞扫描</li><li>…</li></ul><p>资源消耗情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 11:09:02 harbor]$docker stats --no-stream</span><br><span class="line">CONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS</span><br><span class="line">90dcf1505eb0        nginx               0.02%               4.152MiB / 15.51GiB   0.03%               6.53MB / 6.77MB     0B / 0B             7</span><br><span class="line">75945a1aa004        harbor-jobservice   0.12%               9.109MiB / 15.51GiB   0.06%               1.24MB / 15.1MB     0B / 0B             13</span><br><span class="line">000f6f349708        harbor-portal       0.07%               1.676MiB / 15.51GiB   0.01%               200kB / 5.47MB      0B / 0B             2</span><br><span class="line">84635df4fe51        harbor-core         0.53%               12.96MiB / 15.51GiB   0.08%               3.62MB / 2.84MB     0B / 0B             14</span><br><span class="line">c6e53b654127        redis               0.18%               6.816MiB / 15.51GiB   0.04%               15.4MB / 1.41MB     0B / 180kB          5</span><br><span class="line">9048a587cf29        registryctl         0.06%               3.031MiB / 15.51GiB   0.02%               114kB / 87.8kB      0B / 0B             7</span><br><span class="line">d88f7c4b36c5        harbor-db           0.03%               8.09MiB / 15.51GiB    0.05%               929kB / 1.57MB      0B / 4.65MB         11</span><br><span class="line">83c474625566        registry            0.20%               19.45MiB / 15.51GiB   0.12%               1.04MB / 335kB      0B / 1.65MB         16</span><br><span class="line">5a3d2707e96f        harbor-log          0.00%               2.152MiB / 15.51GiB   0.01%               766kB / 147kB       81.9kB / 16.4kB     12</span><br></pre></td></tr></table></figure><p>默认推荐部署方式：docker-composer<br>k8s 推荐部署方式：Helm</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在日常开发过程中，公司内部部署私有仓库（比如 Harbor）可以控制用户角色，方便测试同学快速获取最新版本镜像。<br>在融合产品生命周期中，只有升级场景涉及到镜像的导入、上传、下载、删除操作，因此没必要在集群中持续运行一个仓库服务，浪费资源。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>openstack kolla 项目，目标是通过容器化方式部署 openstack，便于 openstack 滚动升级。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;镜像组织形式&quot;&gt;&lt;a href=&quot;#镜像组织形式&quot; class=&quot;headerlink&quot; title=&quot;镜像组织形式&quot;&gt;&lt;/a&gt;镜像组织形式&lt;/h2&gt;&lt;p&gt;镜像默认采用 OverlayFS 方式挂载，最终效果是将多个目录结构合并为一个。&lt;/p&gt;
&lt;p&gt;其中 low
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-日志处理</title>
    <link href="https://zdyxry.github.io/2019/05/17/Kubernetes-%E5%AE%9E%E6%88%98-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/"/>
    <id>https://zdyxry.github.io/2019/05/17/Kubernetes-实战-日志处理/</id>
    <published>2019-05-17T00:50:44.000Z</published>
    <updated>2019-05-20T12:48:02.007Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础日志处理"><a href="#基础日志处理" class="headerlink" title="基础日志处理"></a>基础日志处理</h2><p>在 Kubernetes（简称 k8s）中，所有应用在 Pod（k8s 管理容器最小单位）中运行，标准处理方式为将日志打印到标准日志输出和标准错误输出，这样我们可以通过 <code>kuberctl logs</code> 关键字获取容器运行时日志，根据容器运行时的类型不同，日志保存路径也不同，以 Docker 为例，所有真实日志均在 <code>/var/lib/docker/</code> 路径下，下面我们来看一个例子：</p><p>在 k8s 中创建一个 Pod，Pod 中指定打印当前时间到标准输出中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">counter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">count</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    args:</span> <span class="string">[/bin/sh,</span> <span class="bullet">-c,</span></span><br><span class="line">            <span class="string">'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done'</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>运行该 Pod，通过 <code>kubectl logs</code> 获取当前 Pod 日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 blog]<span class="comment"># kubectl get pod |grep counter</span></span><br><span class="line">counter            1/1     Running   0          9s</span><br><span class="line">[root@node1 blog]<span class="comment"># kubectl logs counter</span></span><br><span class="line">0: Fri May 17 00:34:01 UTC 2019</span><br><span class="line">1: Fri May 17 00:34:02 UTC 2019</span><br><span class="line">2: Fri May 17 00:34:03 UTC 2019</span><br><span class="line">3: Fri May 17 00:34:04 UTC 2019</span><br><span class="line">4: Fri May 17 00:34:05 UTC 2019</span><br><span class="line">5: Fri May 17 00:34:06 UTC 2019</span><br><span class="line">6: Fri May 17 00:34:07 UTC 2019</span><br><span class="line">7: Fri May 17 00:34:08 UTC 2019</span><br><span class="line">8: Fri May 17 00:34:09 UTC 2019</span><br><span class="line">9: Fri May 17 00:34:10 UTC 2019</span><br><span class="line">10: Fri May 17 00:34:11 UTC 2019</span><br><span class="line">11: Fri May 17 00:34:12 UTC 2019</span><br><span class="line">12: Fri May 17 00:34:13 UTC 2019</span><br></pre></td></tr></table></figure><p>这里如果使用 <code>-f</code> 选项，可以持续输出该 Pod 日志。</p><p>那么我们如何找到该容器对应的实际日志文件呢？</p><p>我们可以现在 Docker 中找到该容器信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 blog]<span class="comment"># docker ps |grep count</span></span><br><span class="line">012e0352b193        busybox                                             <span class="string">"/bin/sh -c 'i=0; wh…"</span>   2 minutes ago       Up 2 minutes                            k8s_count_counter_default_75792a2a-783b-11e9-a3d9-525400aea01a_0</span><br></pre></td></tr></table></figure><p>可以看到在 Docker 中该容器名称为    <code>k8s_count_counter_default_75792a2a-783b-11e9-a3d9-525400aea01a_0</code>  ，我们先不去管最后的 UUID 是什么含义，先看前几个字段，跟集群信息关联可以看到，分别是：k8s，容器名称，Pod 名称，namespaces 名称。那么我们来看下这个容器在 Docker 中的配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 containers]<span class="comment"># pwd</span></span><br><span class="line">/var/lib/docker/containers</span><br><span class="line">[root@node1 containers]<span class="comment"># ls</span></span><br><span class="line">012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2  </span><br><span class="line">...</span><br><span class="line">5c9e765f68a15d5510870179160c59d45d0287b9f92265687d6c37a3557a1017  c563c36b0d3d28ea611b270bc1609ae9c0e720c1c1f85ba669cdab5f7099b77b</span><br></pre></td></tr></table></figure><p>在 Docker 容器路径下，我们看到了很多以不知道什么 ID 明明的子目录，最开始以为是我们 <code>docker ps</code> 中看到的最后 uuid，但是发现对不上，那么我们可以查看下 Pod 的详细信息来试图获取:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 containers]<span class="comment"># kubectl describe pod counter</span></span><br><span class="line">Name:         counter</span><br><span class="line">Namespace:    default</span><br><span class="line">Node:         192.168.27.231/192.168.27.231</span><br><span class="line">Start Time:   Fri, 17 May 2019 08:33:58 +0800</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           172.20.2.13</span><br><span class="line">Containers:</span><br><span class="line">  count:</span><br><span class="line">    Container ID:  docker://012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2</span><br><span class="line">    Image:         busybox</span><br><span class="line">    Image ID:      docker-pullable://busybox@sha256:4b6ad3a68d34da29bf7c8ccb5d355ba8b4babcad1f99798204e7abb43e54ee3d</span><br></pre></td></tr></table></figure><p>忽略掉无关信息，我们可以看到 <code>Container ID</code> 字段，这里对应的 ID 就是在 <code>/var/lib/docker/containers/</code> 下的 ID，找到了 ID，我们可以来看看具体的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2]# tree .</span><br><span class="line">.</span><br><span class="line">├── 012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2-json.log</span><br><span class="line">├── checkpoints</span><br><span class="line">├── config.v2.json</span><br><span class="line">├── hostconfig.json</span><br><span class="line">└── mounts</span><br><span class="line"></span><br><span class="line">2 directories, 3 files</span><br></pre></td></tr></table></figure><p>可以看到有配置文件和日志文件，我们今天只来讨论日志，那么我们看下这个日志文件的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2]<span class="comment"># tail 012e0352b19387170b903aff7c73c02fcd023c2f53cbe5b908392ff4e1a126e2-json.log</span></span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"706: Fri May 17 00:45:48 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:48.913451073Z"</span>&#125;</span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"707: Fri May 17 00:45:49 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:49.915605471Z"</span>&#125;</span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"708: Fri May 17 00:45:50 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:50.917823388Z"</span>&#125;,</span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"713: Fri May 17 00:45:55 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:55.928645971Z"</span>&#125;</span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"714: Fri May 17 00:45:56 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:56.930660054Z"</span>&#125;</span><br><span class="line">&#123;<span class="string">"log"</span>:<span class="string">"715: Fri May 17 00:45:57 UTC 2019\n"</span>,<span class="string">"stream"</span>:<span class="string">"stdout"</span>,<span class="string">"time"</span>:<span class="string">"2019-05-17T00:45:57.932763561Z"</span>&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里日志输出是 JSON 格式的，这里跟 <code>kubectl logs</code> 输出不一致啊，为啥这里是 JSON 呢？ 其实这跟你的 Docker 配置有关，具体的配置可以在 Docker 配置中看到，比如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 docker]<span class="comment"># pwd</span></span><br><span class="line">/etc/docker</span><br><span class="line">[root@node1 docker]<span class="comment"># cat daemon.json</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"registry-mirrors"</span>: [</span><br><span class="line">    <span class="string">"https://dockerhub.azk8s.cn"</span>,</span><br><span class="line">    <span class="string">"https://docker.mirrors.ustc.edu.cn"</span>,</span><br><span class="line">    <span class="string">"http://hub-mirror.c.163.com"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"max-concurrent-downloads"</span>: 10,</span><br><span class="line">  <span class="string">"log-driver"</span>: <span class="string">"json-file"</span>,</span><br><span class="line">  <span class="string">"log-level"</span>: <span class="string">"warn"</span>,</span><br><span class="line">  <span class="string">"log-opts"</span>: &#123;</span><br><span class="line">    <span class="string">"max-size"</span>: <span class="string">"10m"</span>,</span><br><span class="line">    <span class="string">"max-file"</span>: <span class="string">"3"</span></span><br><span class="line">    &#125;,</span><br><span class="line">  <span class="string">"data-root"</span>: <span class="string">"/var/lib/docker"</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>  Docker 默认的日志格式是 JSON，这里我猜测是 k8s 通过 Docker 接口获取日志类型，然后进行相应的解析输出（希望之后对 k8s 更深入的了解来验证猜想）。</p><p>  在了解了标准日志处理，那么我们来看下节点级别的日志处理是怎样的。</p><h2 id="节点级别日志"><a href="#节点级别日志" class="headerlink" title="节点级别日志"></a>节点级别日志</h2><h3 id="标准处理"><a href="#标准处理" class="headerlink" title="标准处理"></a>标准处理</h3><p>我们现在知道了 Pod 的日志其实是存放在容器真正运行所在节点上的，那么如果 Pod 一直运行，日志会不断增大，占用很多的日志空间，这个在节点上是怎么控制的呢？</p><p>因为这种方式不是 k8s 推荐的方式，这里并没有采用集群级别的控制方式，而是以节点为粒度的，各个节点通过 logrotate 自己处理日志轮询，logrotate 相信大部分同学都使用过，这里不详细说了。</p><h3 id="特殊处理"><a href="#特殊处理" class="headerlink" title="特殊处理"></a>特殊处理</h3><p>如果我们不想将应用日志都输出到标准输出，想将日志打印到 <code>/var/log/</code> 下的自定义路径下怎么办？我们可以在 Pod 启动时挂载一个 Volume，这个 Volume 就是Pod 所在的节点真实路径，这样我们在容器中就可以直接将日志打印到该路径下，哪怕 Pod 被销毁，日志也会一直存在。</p><p>我们创建一个 Deployment 类型的资源，在创建之前，我们需要先创建出指定的挂载路径: <code>/var/log/yiran/</code>，资源配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@node1</span> <span class="string">blog]#</span> <span class="string">cat</span> <span class="string">log.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">counter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        run:</span> <span class="string">helloworldanilhostpath</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">count</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">[/bin/sh,</span> <span class="bullet">-c,</span></span><br><span class="line">              <span class="string">'i=0; while true; do echo "$i: $(date)" &gt;&gt; /var/log/yiran/test.log; i=$((i+1)); sleep 1; done'</span><span class="string">]</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">yiran-test-log</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/var/log/yiran</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">yiran-test-log</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/var/log/yiran</span></span><br><span class="line"><span class="attr">          type:</span> <span class="string">Directory</span></span><br></pre></td></tr></table></figure><p>创建完成后，我们来查看下该 Pod 的日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 blog]<span class="comment"># kubectl get pod</span></span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">counter-76b584fd8f-7fq99   1/1     Running   0          2m30s</span><br><span class="line">testlog-rs-lwxts           1/1     Running   0          2d1h</span><br><span class="line">[root@node1 blog]<span class="comment"># kubectl logs counter-76b584fd8f-7fq99</span></span><br></pre></td></tr></table></figure><p>可以看到没有标准日志输出，我们按照上面描述，去看下容器对应的 Docker 日志是否为空：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 166103275a89df918b3240ede911192357f7256c1f23b4311b6db44c0f800cc2]<span class="comment"># pwd</span></span><br><span class="line">/var/lib/docker/containers/166103275a89df918b3240ede911192357f7256c1f23b4311b6db44c0f800cc2</span><br><span class="line">[root@node2 166103275a89df918b3240ede911192357f7256c1f23b4311b6db44c0f800cc2]<span class="comment"># ll</span></span><br><span class="line">total 12</span><br><span class="line">-rw-r-----. 1 root root    0 May 18 17:39 166103275a89df918b3240ede911192357f7256c1f23b4311b6db44c0f800cc2-json.log</span><br><span class="line">drwx------. 2 root root    6 May 18 17:39 checkpoints</span><br><span class="line">-rw-------. 1 root root 5173 May 18 17:39 config.v2.json</span><br><span class="line">-rw-r--r--. 1 root root 2064 May 18 17:39 hostconfig.json</span><br><span class="line">drwx------. 2 root root    6 May 18 17:39 mounts</span><br></pre></td></tr></table></figure><p>这里是符合预期的，因为我们将所有的日志输出到指定日志： <code>/var/log/yiran/test.log</code> 中了，我们去看看节点日志是否存在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 yiran]<span class="comment"># pwd</span></span><br><span class="line">/var/<span class="built_in">log</span>/yiran</span><br><span class="line">[root@node2 yiran]<span class="comment"># tailf test.log</span></span><br><span class="line">327: Sat May 18 09:45:23 UTC 2019</span><br><span class="line">328: Sat May 18 09:45:24 UTC 2019</span><br><span class="line">329: Sat May 18 09:45:25 UTC 2019</span><br><span class="line">330: Sat May 18 09:45:26 UTC 2019</span><br></pre></td></tr></table></figure><p>可以看到这里已经按照预期打印在 Pod 所在节点上了，满足我们的需求。</p><p>接下来是重头戏，集群级别的日志控制。</p><h2 id="集群级别日志"><a href="#集群级别日志" class="headerlink" title="集群级别日志"></a>集群级别日志</h2><p>我们为什么需要日志？对于开发者，可以通过日志进行快速的错误排查；对于运维同学可以根据日志来了解程序运行状态。一句话就是日志非常重要。</p><p>那么既然日志这么重要，那么在 k8s 上如何处理日志，尤其是 k8s 提供了 ReplicaSet/DeploymentSet 这类可以自动缩扩容，自动 ha 的资源，我们如果仅仅通过节点级别的日志管理，集群规模小还好，当集群规模变大之后，对于使用日志的同学简直是灾难。</p><p>在看过了标准日志处理和节点日志处理后，我们来看看一个标准的 k8s 集群是如何处理服务日志和应用日志的。</p><p>这里的前提是，我们有一个日志中心（如 ES)去处理日志，有几种配置方式可以选择：</p><h3 id="节点级别日志代理"><a href="#节点级别日志代理" class="headerlink" title="节点级别日志代理"></a>节点级别日志代理</h3><p>在 k8s 集群中运行一个 DaemonSet，启动的容器运行日志转发器，用于将节点上的日志转发到日志中心，日志转发器可以根据各自资源情况和需求自由选择，如 Logstash,Fluentd,Fluent-bit 等等。</p><p>k8s 标准配置中推荐该方案，无论是从资源使用还是从配置管理上都是最佳方案。</p><p>对应配置在 <code>kubernetes/cluster/addons/fluentd-elasticsearch</code> 路径下，我们可以直接创建部署，部署后状态如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 fluentd-elasticsearch]<span class="comment"># kubectl -n kube-system get rs |grep kibana</span></span><br><span class="line">kibana-logging-f4d99b69f          1         1         1       2d6h</span><br><span class="line">[root@node1 fluentd-elasticsearch]<span class="comment"># kubectl -n kube-system get ds |grep fluent</span></span><br><span class="line">fluentd-es-v2.4.0       3         3         3       3            3           &lt;none&gt;                          2d6h</span><br><span class="line">[root@node1 fluentd-elasticsearch]<span class="comment"># kubectl -n kube-system get statefulset</span></span><br><span class="line">NAME                    READY   AGE</span><br><span class="line">elasticsearch-logging   2/2     2d6h</span><br></pre></td></tr></table></figure><p>可以看到 Fluentd 是以 DaemonSet 方式运行的；ElasticSearch 是以 StatefulSet 方式运行的；Kibana 是以 ReplicaSet 方式运行的。</p><p>其中 Fluentd 配置文件中监控的是 <code>/var/log/container</code> 路径下的所有日志，所以我们理论上可以在 ES 中看到所有应用的日志，那么看到日志之后，我们如何与实际的应用对应呢？这里可以看下具体示例：</p><p>创建 ReplicaSet 类型资源，指定 replica 为 1：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@node1</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">log.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">testlog-rs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">testlog-label</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">testlog-label</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">testlog-container-name</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">          args:</span> <span class="string">[/bin/sh,</span> <span class="bullet">-c,</span></span><br><span class="line">                 <span class="string">'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done'</span><span class="string">]</span></span><br></pre></td></tr></table></figure><p>我们将涉及到名称的字段均加上对应 key，便于在 ES 中查看对应关系，那么接下来看看在 ES 中该容器对应的日志是什么形式的：</p><img src="/2019/05/17/Kubernetes-实战-日志处理/log1.png" title="log1"><p>根据上述对应关系，哪怕 Pod 重建了，我们仍可以通过 container_name 字段来查看对应应用日志，便于调试。</p><h3 id="节点级别日志代理配合伴生容器"><a href="#节点级别日志代理配合伴生容器" class="headerlink" title="节点级别日志代理配合伴生容器"></a>节点级别日志代理配合伴生容器</h3><p>我们知道，标准容器日志应该输出到 stdout 和 stderr 中，那么如果我们一个 Pod 中输出多份日志怎么办？虽然这种情况是我们应该极度避免的，我们应该始终保证一个 Pod 只做一件事情。但是我们有时候迫于代码结构或者其他因素，导致我们会遇到这种情况，那么此时我们需要伴生容器配合使用。</p><p>示例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">admin/logging/two-files-counter-pod-streaming-sidecar.yaml</span> </span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">counter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">count</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    args:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">-c</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">&gt;</span></span><br><span class="line"><span class="string">      i=0;</span></span><br><span class="line"><span class="string">      while true;</span></span><br><span class="line"><span class="string">      do</span></span><br><span class="line"><span class="string">        echo "$i: $(date)" &gt;&gt; /var/log/1.log;</span></span><br><span class="line"><span class="string">        echo "$(date) INFO $i" &gt;&gt; /var/log/2.log;</span></span><br><span class="line"><span class="string">        i=$((i+1));</span></span><br><span class="line"><span class="string">        sleep 1;</span></span><br><span class="line"><span class="string">      done</span></span><br><span class="line"><span class="string"></span><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">varlog</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/var/log</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">count-log-1</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    args:</span> <span class="string">[/bin/sh,</span> <span class="bullet">-c,</span> <span class="string">'tail -n+1 -f /var/log/1.log'</span><span class="string">]</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">varlog</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/var/log</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">count-log-2</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    args:</span> <span class="string">[/bin/sh,</span> <span class="bullet">-c,</span> <span class="string">'tail -n+1 -f /var/log/2.log'</span><span class="string">]</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">varlog</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/var/log</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">varlog</span></span><br><span class="line"><span class="attr">    emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p>这种方法是极度不推荐的，如果我们配置了 EFK，那么我们 1 份日志相当于写了 3 份，如果我们 ES 的后端存储是一个副本机制的分布式存储，那么我们 1 份日志相当于写了 3 * 2(或 3，存储副本数)份，这是极大的浪费了存储资源的，且会大大影响 SSD 磁盘寿命。</p><h3 id="Pod-级别日志代理"><a href="#Pod-级别日志代理" class="headerlink" title="Pod 级别日志代理"></a>Pod 级别日志代理</h3><p>如果觉得节点级别日志代理粒度太粗，我们也可以选择 Pod 级别，在每个 Pod 中都启动一个伴生容器作为日志代理，将日志直接转发到日志中心。</p><p>若以该形式部署，则我们的应用程序配置不仅要配置应用自身，还要考虑日志处理策略；节点计算能力现在大幅提升，每个节点的 Pod 数量很大，浪费了大量的计算资源。</p><h3 id="应用自处理日志代理"><a href="#应用自处理日志代理" class="headerlink" title="应用自处理日志代理"></a>应用自处理日志代理</h3><p>如标题，我们当然可以在应用中直接将日志转发到指定的日志中心，这种情况也是极度糟糕的， <code>Make each program do one thing well.</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>到这里我们关于日志部分的介绍就到这里，总的来说我们需要集中式的日志中心，且推荐以节点级别日志代理方式配置，前提是我们能够预留足够的计算资源和存储资源。后续有机会我们可以了解下日志与事件审计关联使用。</p><p>下一篇我们来看下 Kubernetes 中镜像相关管理与使用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基础日志处理&quot;&gt;&lt;a href=&quot;#基础日志处理&quot; class=&quot;headerlink&quot; title=&quot;基础日志处理&quot;&gt;&lt;/a&gt;基础日志处理&lt;/h2&gt;&lt;p&gt;在 Kubernetes（简称 k8s）中，所有应用在 Pod（k8s 管理容器最小单位）中运行，标准处理方
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-微服务</title>
    <link href="https://zdyxry.github.io/2019/05/14/Kubernetes-%E5%AE%9E%E6%88%98-%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    <id>https://zdyxry.github.io/2019/05/14/Kubernetes-实战-微服务/</id>
    <published>2019-05-14T14:32:43.000Z</published>
    <updated>2019-05-14T14:41:31.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h2><p>在 《Kubernetes In Action》的开始，先要了解 k8s 的需求来自于哪里，为什么我们需要 k8s。</p><p>引用维基百科解释：</p><blockquote><p>微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent/Language agnostic) 的 API 集相互通信。</p></blockquote><p>说一说我的理解，在项目早期，都是单体应用，随着功能越来越多，项目越来越大，虽然保证了部署运维的方便，但对于组内同学并不友好，新同学往往要在一坨代码中找自己想要的一点，本地修改提交跑 CI 也是以项目为单位的执行（前段时间 B 站不小心泄露的 Golang 代码就是这种）。当后续升级产品时，因为是以项目为最小粒度，哪怕无关代码，也要被迫进行代码升级，服务重启等操作，带来了额外的风险。</p><p>在 2014年，Martin Fowler 与 James Lewis 共同提出了微服务的概念，把单体应用改为通过接口产生的远程方法调用，将项目拆分，一个项目保证只做一件事情，独立部署和维护。</p><p>优点：</p><ul><li>高度可维护和可测试</li><li>松散耦合</li><li>可独立部署</li><li>围绕业务能力进行组织</li></ul><p>缺点：</p><ul><li>服务数量大幅增加，部署维护困难</li><li>服务间依赖管理</li><li>服务故障处理</li></ul><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p>那么我们提到了项目演进，在同一时间，容器技术的标准化统一也间接促成了微服务的推广（我猜的），Docker 在 2013.03.13 发布第一个版本，容器化技术让我们产品发布形态有了新的选择，开发直接将容器镜像发布，运维同学通过镜像进行产品上线，确保了环境的统一，无须纠结环境配置相关问题（不用吵架了）。</p><p>当我们产品发布采用容器化上线后，我们面临一些其他的问题了：</p><ul><li>微服务追求的是将服务解耦，拆分为多个服务，那么最终发布形态对应的也是多个镜像，运维同学管理这些镜像之间的关系难度增加。</li><li>同时当镜像运行在不同的物理节点上，对计算资源和网络资源的要求是一致的，运维同学需要做到让镜像无感知。</li><li>当产品要进行升级时，镜像之间的依赖关系，故障切换等操作紧靠现有容器功能实现困难。</li></ul><p>这么一看，与之前单体应用比也没好哪里去。于是有了各种容器编排系统，比如 Swarm，Mesos等等，但都不是很好用且各家一个标准，这时候老大哥谷歌发话了，我来把我们内部用了很多年要淘汰的东西拿出来给大家解决问题吧，于是有了 Kubernetes。</p><p>Kubernetes 功能上提供了解决微服务引入的问题，并更好的配合微服务去提供稳定高可用的统一容器化环境，具体如何解决的我们后续可以通过了解 Pod，ConfigMap，ReplicaSet 等功能去详细了解。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可能是因为我考虑问题都是从运维角度去看的，网上的一些文章讲的带来的好处反而没看太清，可能作为 2C 产品，追求敏捷开发，产品不断快速迭代的目标比较适合，但是如果本身作为一个追求稳定可靠的 2B 产品来说，引入 k8s 带来的好处和维护 k8s 带来的成本真的要仔细的从产品层面考虑清楚，这里感觉跟具体的技术关系不大，而是说从产品面向的客户对象考虑，客户想要的是一个什么产品，而 k8s 作为一个还在不断（频繁）迭代的产品来说（可以去看看 release notes 的更新速度），后续若出现某些 API 不兼容等情况，如何去应对，感觉还是个灾难。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E6%9C%8D%E5%8B%99" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%BE%AE%E6%9C%8D%E5%8B%99</a></li><li><a href="https://hoxis.github.io/learn-microservice-from-0.html" target="_blank" rel="noopener">https://hoxis.github.io/learn-microservice-from-0.html</a></li><li><a href="https://www.infoq.cn/article/Rdx-PkjmTpPRA5oox5EM" target="_blank" rel="noopener">https://www.infoq.cn/article/Rdx-PkjmTpPRA5oox5EM</a></li></ul><p>在学习过程中，通过阅读 <a href="https://github.com/cch123" target="_blank" rel="noopener">cch123</a> 的博客对微服务有了更深的了解，这里列一下相关的系列博客链接：</p><ol><li><p><a href="http://xargin.com/disaster-of-microservice-ul/" target="_blank" rel="noopener">微服务的灾难-通用语言</a></p></li><li><p><a href="http://xargin.com/disaster-of-microservice-techstack/" target="_blank" rel="noopener">微服务的灾难-技术栈</a></p></li><li><p><a href="http://xargin.com/disaster-of-microservice-divide/" target="_blank" rel="noopener">微服务的灾难-拆分</a></p></li><li><p><a href="http://xargin.com/disaster-of-microservice-dephell/" target="_blank" rel="noopener">微服务的灾难-依赖地狱</a></p></li><li><p><a href="http://xargin.com/disaster-of-microservice-evconst/" target="_blank" rel="noopener">微服务的灾难-最终一致</a></p></li><li><p><a href="http://xargin.com/disaster-of-microservice-conway-law/" target="_blank" rel="noopener">微服务的灾难-康威定律和 KPI 冲突</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;微服务&quot;&gt;&lt;a href=&quot;#微服务&quot; class=&quot;headerlink&quot; title=&quot;微服务&quot;&gt;&lt;/a&gt;微服务&lt;/h2&gt;&lt;p&gt;在 《Kubernetes In Action》的开始，先要了解 k8s 的需求来自于哪里，为什么我们需要 k8s。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-前言</title>
    <link href="https://zdyxry.github.io/2019/05/11/Kubernetes-%E5%AE%9E%E6%88%98-%E5%89%8D%E8%A8%80/"/>
    <id>https://zdyxry.github.io/2019/05/11/Kubernetes-实战-前言/</id>
    <published>2019-05-11T13:00:55.000Z</published>
    <updated>2019-05-11T13:01:18.094Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes-实战-前言"><a href="#Kubernetes-实战-前言" class="headerlink" title="Kubernetes 实战-前言"></a>Kubernetes 实战-前言</h2><p>自从 Kubernetes 大热之后，一直没跟着版本去了解具体的功能及使用，只是大概了解其中概念。之前推特上有人推荐《Kubernetes In Action》这本书，说是对入门同学很友好，利用五一假期和这个周末，终于看完了，打算把学习过程和其中的一些想法记录下来。</p><h2 id="《Kubernetes-In-Action》"><a href="#《Kubernetes-In-Action》" class="headerlink" title="《Kubernetes In Action》"></a>《Kubernetes In Action》</h2><p>就想推荐人说的那样，这本书作为 101 系列来说，是很称职的，你跟着官方示例做，90%以上都是可以成功的，且讲解门槛不高，推荐。</p><p>中文版是由七牛团队翻译的，虽然其中有一些小的翻译错误，但是整体读下来还是很顺畅的，不影响阅读，当然现在网上已经有原版资源，想读的同学可以去 SaltTiger 搜索下载。</p><p>本书章节较多，分为 3 部分：What？How？Why？首先讲解 k8s 及容器的基本概念，然后讲解 k8s 基本使用，最后介绍了一些 k8s 工作原理及最佳实践。</p><p>当你了解了什么是 k8s，及 k8s 能带来什么好处之后，我们去使用 k8s，从而真实的感受到 k8s 带来的便利，这种感觉是很美好的（表面美好的东西肯定会有某些限制），对我来说这种美好截止到第二部分就停止了。在第三部分中，我们在之前感受到的便利隐藏着很多没有考虑到的边界因素，也意味着我们从一个传统的单节点服务切换到微服务架构上，会新增很多需要去考虑的因素，如果 k8s 内部提供了解决方案，那么很简单，我们直接编写 YAML 就可以了，如果 k8s 没有解决方案呢？我不知道，可能当我真正体验过之后才能给出感受吧（即将发生的事）。</p><p>下一篇我们来说下什么是微服务。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kubernetes-实战-前言&quot;&gt;&lt;a href=&quot;#Kubernetes-实战-前言&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 实战-前言&quot;&gt;&lt;/a&gt;Kubernetes 实战-前言&lt;/h2&gt;&lt;p&gt;自从 Kubernetes 
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode Shell 题解</title>
    <link href="https://zdyxry.github.io/2019/05/11/LeetCode-Shell-%E9%A2%98%E8%A7%A3/"/>
    <id>https://zdyxry.github.io/2019/05/11/LeetCode-Shell-题解/</id>
    <published>2019-05-11T12:17:57.000Z</published>
    <updated>2019-05-11T12:30:27.712Z</updated>
    
    <content type="html"><![CDATA[<p>工作上用 Shell 的频率是很高的，哪怕现在有了 Ansible 或者其他配置工具，Shell 仍是一个以 Linux 作为工作环境的同学的必备技能。<br>之前写过 GitHub 上的 <code>Pure Bash Bible</code>  的博客，看到 LeetCode 上的 Shell 题目好久不更新了，只有 4 道，今天记录一下题解。</p><h2 id="192-Word-Frequency"><a href="#192-Word-Frequency" class="headerlink" title="192. Word Frequency"></a>192. Word Frequency</h2><p>统计文本文件中单词出现次数，倒序输出。</p><p>words.txt<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">the day is sunny the the</span><br><span class="line">the sunny is is</span><br></pre></td></tr></table></figure></p><p>利用 <code>tr</code> <code>sort</code> <code>uniq</code> <code>awk</code> 解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Read from the file words.txt and output the word frequency list to stdout.</span></span><br><span class="line">cat words.txt | tr -s ' ' '\n' | sort | uniq -c | sort -rn | awk '&#123; print $2, $1 &#125;'</span><br></pre></td></tr></table></figure><h1 id="193-Valid-Phone-Numbers"><a href="#193-Valid-Phone-Numbers" class="headerlink" title="193. Valid Phone Numbers"></a>193. Valid Phone Numbers</h1><p>校验电话号码正确性。</p><p>file.txt<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">987-123-4567</span><br><span class="line">123 456 7890</span><br><span class="line">(123) 456-7890</span><br></pre></td></tr></table></figure></p><p>主要是利用 <code>grep</code> 匹配正则，注意转义符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Read from the file file.txt and output all valid phone numbers to stdout.</span></span><br><span class="line">grep -P '^(\d&#123;3&#125;-|\(\d&#123;3&#125;\) )\d&#123;3&#125;-\d&#123;4&#125;$' file.txt</span><br></pre></td></tr></table></figure><h1 id="194-Transpose-File"><a href="#194-Transpose-File" class="headerlink" title="194. Transpose File"></a>194. Transpose File</h1><p>行和列转换。</p><p>file.txt<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name age</span><br><span class="line">alice 21</span><br><span class="line">ryan 30</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Read from the file file.txt and <span class="built_in">print</span> its transposed content to stdout.</span></span><br><span class="line">ncol=`head -n1 file.txt | wc -w`</span><br><span class="line"></span><br><span class="line">for i in `seq 1 $ncol`</span><br><span class="line">do</span><br><span class="line">    echo `cut -d' ' -f$i file.txt`</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h1 id="195-Tenth-Line"><a href="#195-Tenth-Line" class="headerlink" title="195. Tenth Line"></a>195. Tenth Line</h1><p>显示文件第 10 行。</p><p>file.txt<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Line 1</span><br><span class="line">Line 2</span><br><span class="line">Line 3</span><br><span class="line">Line 4</span><br><span class="line">Line 5</span><br><span class="line">Line 6</span><br><span class="line">Line 7</span><br><span class="line">Line 8</span><br><span class="line">Line 9</span><br><span class="line">Line 10</span><br></pre></td></tr></table></figure></p><p>如果直接使用 <code>head</code> <code>tail</code> 的话不能方便处理文件为空的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Read from the file file.txt and output the tenth line to stdout.</span></span><br><span class="line">sed -n '10p' file.txt</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实编写 Shell 在熟知 Linux  内置命令就可以处理大部分场景了，如果处理不来，只能求助于 sed 和 awk, 但我脑子可能不太好使，awk 的语法总是记不住，每次写之前都要查一下语法。 - -</p><p>如果不想在代码中充斥着各种转义处理的话，还是老老实实使用 Python 编写脚本吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;工作上用 Shell 的频率是很高的，哪怕现在有了 Ansible 或者其他配置工具，Shell 仍是一个以 Linux 作为工作环境的同学的必备技能。&lt;br&gt;之前写过 GitHub 上的 &lt;code&gt;Pure Bash Bible&lt;/code&gt;  的博客，看到 LeetC
      
    
    </summary>
    
    
      <category term="alogrithms" scheme="https://zdyxry.github.io/tags/alogrithms/"/>
    
  </entry>
  
  <entry>
    <title>PySnooper 源码阅读</title>
    <link href="https://zdyxry.github.io/2019/04/27/PySnooper-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>https://zdyxry.github.io/2019/04/27/PySnooper-源码阅读/</id>
    <published>2019-04-27T12:14:28.000Z</published>
    <updated>2019-04-27T12:21:19.455Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 18 年的时候 jiajun 同学发过一篇<a href="https://jiajunhuang.com/articles/2018_05_08-how_to_debug.md.html" target="_blank" rel="noopener">博客</a>，讲如何调试相关的总结。结合最近自己的经验，紧靠 logging 和 print 就能解决日常的 80%问题，剩下的 20% 也都可以通过review 代码来解决，我只有当确实没什么思路的时候，才会采用 pdb 的方式去调试。之所以先 review 代码再采用 pdb 的方式是想确认自己已经理清了相关代码的上下文和逻辑，不至于在单步调试的时候出现 <code>恍然大悟</code> （贬义） 的状况。</p><p>最近两天 Github 上关于 Python 的项目最火的就是 PySnooper，这个项目的 Slogan 就是 <code>Never use print for debugging again</code> ，这里的 print 替换为 logging 也没啥差。整个代码在初步可用阶段代码量很少，也确实能够给平时写些小脚本带来便利，便抽时间看了看具体的实现。</p><h2 id="PySnooper"><a href="#PySnooper" class="headerlink" title="PySnooper"></a>PySnooper</h2><p>先来看下目录结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/PySnooper</span><br><span class="line">3d0d051 ✗ $ tree .</span><br><span class="line">.</span><br><span class="line">├── LICENSE</span><br><span class="line">├── MANIFEST.in</span><br><span class="line">├── README.md</span><br><span class="line">├── make_release.sh</span><br><span class="line">├── misc</span><br><span class="line">│   └── IDE\ files</span><br><span class="line">│       └── PySnooper.wpr</span><br><span class="line">├── pysnooper</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── pycompat.py</span><br><span class="line">│   ├── pysnooper.py</span><br><span class="line">│   ├── tracer.py</span><br><span class="line">│   └── utils.py</span><br><span class="line">├── requirements.in</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── setup.py</span><br><span class="line">├── test_requirements.txt</span><br><span class="line">└── tests</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── test_pysnooper.py</span><br><span class="line">    └── utils.py</span><br></pre></td></tr></table></figure><p>可以看到最核心部分都在 pysnooper 部分，我们以官方示例来了解具体是如何工作的:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@yiran30250:~/backup/PySnooper</span><br><span class="line">master ✗ $ cat a.py</span><br><span class="line"><span class="keyword">import</span> pysnooper</span><br><span class="line"></span><br><span class="line"><span class="meta">@pysnooper.snoop('/var/log/test.log')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">number_to_bits</span><span class="params">(number)</span>:</span></span><br><span class="line">    print(<span class="string">'func starting...'</span>)</span><br><span class="line">    <span class="keyword">if</span> number:</span><br><span class="line">        bits = []</span><br><span class="line">        <span class="keyword">while</span> number:</span><br><span class="line">            number, remainder = divmod(number, <span class="number">2</span>)</span><br><span class="line">            bits.insert(<span class="number">0</span>, remainder)</span><br><span class="line">        <span class="keyword">return</span> bits</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">print(number_to_bits(<span class="number">6</span>))</span><br></pre></td></tr></table></figure><p>从示例中可以看到， <code>pysnooper.snoop</code> 作为一个装饰器，装饰所需要调试的函数，并可以再装饰器参数中添加对应的输出目的，比如标准输出，或者指定日志等。</p><p>我们看下 <code>snoop</code> 的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">snoop</span><span class="params">(output=None, variables=<span class="params">()</span>, depth=<span class="number">1</span>, prefix=<span class="string">''</span>, overwrite=False)</span>:</span></span><br><span class="line">    write, truncate = get_write_and_truncate_functions(output) <span class="comment"># 通过输出目标获取 write 函数</span></span><br><span class="line">    <span class="keyword">if</span> truncate <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> overwrite:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"`overwrite=True` can only be used when writing "</span></span><br><span class="line">                        <span class="string">"content to file."</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorate</span><span class="params">(function)</span>:</span></span><br><span class="line">        target_code_object = function.__code__ <span class="comment"># 函数在 python 解释器编译后的字节码对象</span></span><br><span class="line">        tracer = Tracer(target_code_object=target_code_object, write=write,</span><br><span class="line">                        truncate=truncate, variables=variables, depth=depth,</span><br><span class="line">                        prefix=prefix, overwrite=overwrite)</span><br><span class="line">        <span class="comment"># 实例化 Tracer，将现有参数全部传递</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">inner</span><span class="params">(function_, *args, **kwargs)</span>:</span></span><br><span class="line">            <span class="keyword">with</span> tracer: <span class="comment"># 通过 with 关键字调用 tracer，那么 Tracer 内应该实现了上下文管理器的 `__enter__` 和 `__exit__` 方法</span></span><br><span class="line">                <span class="keyword">return</span> function(*args, **kwargs) <span class="comment">#</span></span><br><span class="line">        <span class="keyword">return</span> decorator.decorate(function, inner)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decorate</span><br></pre></td></tr></table></figure><p>主要功能应该在 Trancer 中实现的，我们看下 Trancer 中做了什么？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.original_trace_function = sys.gettrace()</span><br><span class="line">    sys.settrace(self.trace)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, exc_type, exc_value, exc_traceback)</span>:</span></span><br><span class="line">    sys.settrace(self.original_trace_function)</span><br></pre></td></tr></table></figure><p>先忽略其他的，我们先看实现上下文管理器的方法:</p><ul><li>进入上下文环境<ul><li>获取当前跟踪器并记录</li><li>设置追踪器为 <code>self.trace</code></li></ul></li><li>退出上下文环境<ul><li>将追踪器设置为原有值</li></ul></li></ul><p>注意：</p><p><code>sys.settrace</code> 官方文档中描述它只用来做调试类工具，不建议在内部实现复杂逻辑。<br>The gettrace() function is intended only for implementing debuggers, profilers, coverage tools and the like.</p><p>其中追踪器要接受 3 个参数，分别是：frame，event 和 arg。我们先记住 frame就是当前的栈帧就好。</p><p>看一下 <code>self.trace</code> 具体是如何工作的，先看第一部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trace</span><span class="params">(self, frame, event, arg)</span>:</span></span><br><span class="line">    <span class="comment"># 这里的注释写的很清楚了，根据当前 frame 是否为指定的函数字节码对象，如果不是且深度为 1，则直接返回 trace，如果指定了追踪深度，则不断循环，直到追踪到指定函数字节码对象</span></span><br><span class="line">    <span class="keyword">if</span> frame.f_code <span class="keyword">is</span> <span class="keyword">not</span> self.target_code_object:</span><br><span class="line">        <span class="keyword">if</span> self.depth == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> self.trace</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            _frame_candidate = frame</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, self.depth):</span><br><span class="line">                _frame_candidate = _frame_candidate.f_back <span class="comment"># f_back 为当前栈帧的上一个栈帧，便于在当前代码执行完成后可以调回之前代码继续执行</span></span><br><span class="line">                <span class="keyword">if</span> _frame_candidate <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">return</span> self.trace</span><br><span class="line">                <span class="keyword">elif</span> _frame_candidate.f_code <span class="keyword">is</span> self.target_code_object:</span><br><span class="line">                    indent = <span class="string">' '</span> * <span class="number">4</span> * i</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> self.trace</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        indent = <span class="string">''</span></span><br></pre></td></tr></table></figure><p>找到了具体的执行对象，我们看下如何获取环境变量的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trace</span><span class="params">(self, frame, event, arg)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    self.frame_to_old_local_reprs[frame] = old_local_reprs = \</span><br><span class="line">                                           self.frame_to_local_reprs[frame] <span class="comment"># 标记当前变量为现有变量</span></span><br><span class="line">    self.frame_to_local_reprs[frame] = local_reprs = \</span><br><span class="line">                           get_local_reprs(frame, variables=self.variables) <span class="comment"># 获取当前变量</span></span><br><span class="line"></span><br><span class="line">    modified_local_reprs = &#123;&#125;</span><br><span class="line">    newish_local_reprs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> local_reprs.items(): <span class="comment"># 遍历当前本地变量，将其分别放置为新变量和修改变量列表中</span></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> old_local_reprs:</span><br><span class="line">            newish_local_reprs[key] = value</span><br><span class="line">        <span class="keyword">elif</span> old_local_reprs[key] != value:</span><br><span class="line">            modified_local_reprs[key] = value</span><br><span class="line">    <span class="comment"># 将变量通过 write 函数输出到对应的目标中</span></span><br><span class="line">    newish_string = (<span class="string">'Starting var:.. '</span> <span class="keyword">if</span> event == <span class="string">'call'</span> <span class="keyword">else</span></span><br><span class="line">                                                        <span class="string">'New var:....... '</span>)</span><br><span class="line">    <span class="keyword">for</span> name, value_repr <span class="keyword">in</span> sorted(newish_local_reprs.items()):</span><br><span class="line">        self.write(<span class="string">'&#123;indent&#125;&#123;newish_string&#125;&#123;name&#125; = &#123;value_repr&#125;'</span>.format(</span><br><span class="line">                                                               **locals()))</span><br><span class="line">    <span class="keyword">for</span> name, value_repr <span class="keyword">in</span> sorted(modified_local_reprs.items()):</span><br><span class="line">        self.write(<span class="string">'&#123;indent&#125;Modified var:.. &#123;name&#125; = &#123;value_repr&#125;'</span>.format(</span><br><span class="line">                                                               **locals()))</span><br></pre></td></tr></table></figure><p>在上面可以看到大部分都是判断逻辑，看一下 <code>get_local_reprs</code> 中是如何获取当前变量的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_local_reprs</span><span class="params">(frame, variables=<span class="params">()</span>)</span>:</span></span><br><span class="line">    result = &#123;key: get_shortish_repr(value) <span class="keyword">for</span> key, value</span><br><span class="line">                                                     <span class="keyword">in</span> frame.f_locals.items()&#125;</span><br><span class="line">    <span class="keyword">for</span> variable <span class="keyword">in</span> variables:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result[variable] = get_shortish_repr(</span><br><span class="line">                eval(variable, frame.f_globals, frame.f_locals)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>是通过调用 <code>frame.f_locals.items()</code> 获取当前栈帧所具有的本地变量。<br>剩下的部分是对于调试函数为装饰器时，需要特殊处理：跳过装饰器函数，直到找到函数定义部分。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主题功能就通过上述代码来实现，简单高效，我们再来回顾一下：</p><ol><li>通过 settrace 来设置追踪器<br> a. settrace 的行为是先执行追踪器部分，执行完成后执行函数字节码对应行</li><li>在追踪器中，通过 frame 相关属性来获取所需值，如 f_locals, f_code, f_globals</li><li>打印相关信息到目标中，退出上下文管理器，重新设置外层追踪器</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在 18 年的时候 jiajun 同学发过一篇&lt;a href=&quot;https://jiajunhuang.com/articles/2018_
      
    
    </summary>
    
    
      <category term="Python" scheme="https://zdyxry.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>PingCAP tidb-ansible 源码阅读</title>
    <link href="https://zdyxry.github.io/2019/04/20/PingCAP-tidb-ansible-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>https://zdyxry.github.io/2019/04/20/PingCAP-tidb-ansible-源码阅读/</id>
    <published>2019-04-20T03:50:41.000Z</published>
    <updated>2019-04-20T03:51:44.513Z</updated>
    
    <content type="html"><![CDATA[<p>了解我的同学应该知道，我目前负责公司产品的运维工具开发相关的工作，作为一款 2B 的产品，在产品运维过程中，总是有一些不愉快（又不能让客户知道）的繁琐操作：这些操作可能是为了防止过程中出现错误，而不断添加的检测条件；也有可能是历史问题，随着产品发布迭代而一直遗留至今。</p><p>所以我平时也在关注一些开源的 2B 产品的配套运维工具，比如 ZStack、PingCAP 之类的公司。</p><p>但是 ZStack 的开源生态不是很好，感觉只是在保持代码更新（不知道哪个分支）的状态。相比之下 PingCAP 就好很多了，可以很直接的从文档中看到差别，而且社区很活跃。</p><p>最近看到 PingCAP 的一个关于部署维护的 <a href="https://university.pingcap.com/views/common/audition.html?courseId=240255&amp;courseWareId=330983&amp;designId=270990" target="_blank" rel="noopener">视频讲解</a> ，可能时间有限，并没有很深入的讲解细节，有兴趣的同学可以看下。</p><p>在讲解过程中，一个比较核心的工具就是 Ansible，通过 Ansible Playbook 来定义各个步骤，我最近也在使用 Ansible 来进行二次开发，特此学习下 PingCAP 的 <a href="https://github.com/pingcap/tidb-ansible" target="_blank" rel="noopener">tidb-ansible</a> 。</p><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p>TiDB 目前支持 4 种安装方式：</p><ol><li>Ansible Online</li><li>Ansible Offline</li><li>Docker</li><li>Docker compose</li></ol><p>其中最佳实践应该是 Ansible Online 方式，通过控制机联网下载所需依赖及 TiDB binary 文件。当然如果所在环境无法访问互联网，那么只能采用 Offline 方式了。后两种部署方式，感觉只是用于开发测试或者给用户“看看”的情况。</p><p>如果要学习的话肯定要学习最佳实践了，那么我们来看看 Ansible Online 方式。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>TiDB 作为一个 <strong>开源分布式关系型数据库</strong> ，所需要的物理环境是很比较苛刻的，官方最佳实践的需求如下：</p><hr><table><thead><tr><th>组件</th><th>CPU</th><th>内存</th><th>硬盘类型</th><th>网络</th><th>数量(最低要求)</th></tr></thead><tbody><tr><td>TiDB</td><td>16</td><td>32GB</td><td>SAS</td><td>10GbE * 2</td><td>2</td></tr><tr><td>PD</td><td>4</td><td>8GB</td><td>SSD</td><td>10 GbE * 2</td><td>3</td></tr><tr><td>TiKV</td><td>16</td><td>32GB</td><td>SSD</td><td>10 GbE * 2</td><td>3</td></tr><tr><td>监控</td><td>8</td><td>16GB</td><td>SAS</td><td>1GbE * 1</td><td>1</td></tr></tbody></table><p>对于 CPU、内存和磁盘的要求我们暂时忽略，这里注意网卡数量都是推荐的 2 块网卡，应该是会做 bonding，到时候看下代码中是否处理。</p><h2 id="安装部署流程"><a href="#安装部署流程" class="headerlink" title="安装部署流程"></a>安装部署流程</h2><ul><li>控制节点安装依赖</li><li>配置普通用户（tidb）<ul><li>密码</li><li>sudu 权限</li><li>ssh key</li><li>…</li></ul></li><li>配置控制节点到被安装节点 SSH 免密登录<ul><li>tidb 免密</li></ul></li><li>NTP<ul><li>各节点时钟同步</li></ul></li><li>CPU 调节器模式<ul><li>performance</li></ul></li><li>格式化磁盘<ul><li>推荐 ext4，支持 xfs</li><li>挂载参数：nodelalloc 和 noatime </li></ul></li><li>ansible inventory &amp; TiDB 配置修改<ul><li>inventory</li><li>tidb-ansible/conf/*</li></ul></li><li>部署<ul><li>local_prepare.yml</li><li>bootstrap.yml</li><li>deploy.yml</li><li>start.yml</li></ul></li></ul><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="软件依赖"><a href="#软件依赖" class="headerlink" title="软件依赖"></a>软件依赖</h3><p>这里 TiDB 依赖的第三方组件不多，安装方式猜测之所以直接用 pip 是因为要支持不同的发型版本，python 通用些。<br>其中如果部署 pump 并开启 binlog 的话，是需要安装 Kafka 集群的，这里在安装部署文档中没有见到更多的说明。</p><h3 id="普通用户"><a href="#普通用户" class="headerlink" title="普通用户"></a>普通用户</h3><p>这点很重要，一款软件的运行环境很重要，无须 root 权限的坚决不能给与 root 权限，否则之后的权限控制很难做。</p><h3 id="时钟同步"><a href="#时钟同步" class="headerlink" title="时钟同步"></a>时钟同步</h3><p>这里采用的是 ntpd 同步，不知道为什么没有采用 chronyd，在 RHEL7 之后推荐采用 chronyd 来进行时钟同步，根据这篇<a href="https://www.thegeekdiary.com/centos-rhel-7-chrony-vs-ntp-differences-between-ntpd-and-chronyd/" target="_blank" rel="noopener">博客</a>中提到的，chronyd 应该占有绝对<a href="https://chrony.tuxfamily.org/comparison.html" target="_blank" rel="noopener">优势</a>的，不知道这里是出于什么考虑。</p><h3 id="CPU-调节器模式"><a href="#CPU-调节器模式" class="headerlink" title="CPU 调节器模式"></a>CPU 调节器模式</h3><p>这里 TiDB 推荐采用 performance 模式，但是如果你的 2B 产品卖点中有提到 <strong>节省能耗</strong> 相关字眼的，觉得还是要综合考虑才好。</p><h3 id="格式化磁盘"><a href="#格式化磁盘" class="headerlink" title="格式化磁盘"></a>格式化磁盘</h3><p>推荐采用 ext4，在 RHEL7 之后默认的系统分区均为 xfs，这里出于什么考虑也没有透露。</p><p>在我个人使用过程中，xfs 在处理服务器掉电之后的处理很麻烦，经常是需要手动 xfs_repair 去修复磁盘分区。</p><p>TiDB 挂载参数强制要求 <strong>nodelalloc</strong> ，我们看下这个参数的意义：<br><figure class="highlight plain"><figcaption><span>delayed allocation.  Blocks are allocated</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">when the data is copied from userspace to the page cache, either via the write(2) system call or when an mmap&apos;ed page which was previously unallocated is written for the first time.</span><br></pre></td></tr></table></figure></p><h3 id="TiDB-参数"><a href="#TiDB-参数" class="headerlink" title="TiDB 参数"></a>TiDB 参数</h3><p>因为对 TiDB 无更多了解，此节忽略。</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>终于到了我最想了解的地方了：ansible playbook。我们先看下代码结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/tidb-ansible</span><br><span class="line">master ✔ $ tree -L 2 .</span><br><span class="line">.</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── bootstrap.yml</span><br><span class="line">├── clean_log_cron.yml</span><br><span class="line">├── cloud</span><br><span class="line">│   └── aws-ansible</span><br><span class="line">├── collect_diagnosis.yml</span><br><span class="line">├── common_tasks</span><br><span class="line">│   ├── add_evict_leader_scheduler.yml</span><br><span class="line">|   ...</span><br><span class="line">│   └── transfer_pd_leader.yml</span><br><span class="line">├── conf</span><br><span class="line">│   ├── alertmanager.yml</span><br><span class="line">|   ...</span><br><span class="line">│   └── tikv.yml</span><br><span class="line">├── create_users.yml</span><br><span class="line">├── deploy.yml</span><br><span class="line">├── deploy_drainer.yml</span><br><span class="line">├── deploy_ntp.yml</span><br><span class="line">├── excessive_rolling_update.yml</span><br><span class="line">├── filter_plugins</span><br><span class="line">│   └── tags.py</span><br><span class="line">├── graceful_stop.yml</span><br><span class="line">├── group_vars</span><br><span class="line">│   ├── alertmanager_servers.yml</span><br><span class="line">│   ├── all.yml</span><br><span class="line">|   ...</span><br><span class="line">│   ├── tidb_servers.yml</span><br><span class="line">│   └── tikv_servers.yml</span><br><span class="line">├── hosts.ini</span><br><span class="line">├── inventory.ini</span><br><span class="line">├── library</span><br><span class="line">│   ├── coreos_facts</span><br><span class="line">│   ├── docker_facts</span><br><span class="line">│   └── wait_for_pid.py</span><br><span class="line">├── local_prepare.yml</span><br><span class="line">├── <span class="built_in">log</span></span><br><span class="line">├── migrate_monitor.yml</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── roles</span><br><span class="line">│   ├── alertmanager</span><br><span class="line">│   ├── blackbox_exporter</span><br><span class="line">│   ├── bootstrap</span><br><span class="line">|   ...</span><br><span class="line">│   ├── tikv</span><br><span class="line">│   ├── tikv_importer</span><br><span class="line">│   └── tispark</span><br><span class="line">├── rolling_update.yml</span><br><span class="line">├── rolling_update_monitor.yml</span><br><span class="line">├── scripts</span><br><span class="line">│   ...</span><br><span class="line">│   ├── check</span><br><span class="line">│   ├── clsrun.sh</span><br><span class="line">│   ├── disk_performance.json</span><br><span class="line">│   ├── grafana-config-copy.py</span><br><span class="line">|   ...</span><br><span class="line">│   └── tikv_trouble_shooting.json</span><br><span class="line">├── start.yml</span><br><span class="line">├── start_drainer.yml</span><br><span class="line">├── start_spark.yml</span><br><span class="line">├── stop.yml</span><br><span class="line">├── stop_drainer.yml</span><br><span class="line">├── stop_spark.yml</span><br><span class="line">├── templates</span><br><span class="line">│   └── grafana.dest.json.j2</span><br><span class="line">├── unsafe_cleanup.yml</span><br><span class="line">├── unsafe_cleanup_container.yml</span><br><span class="line">└── unsafe_cleanup_data.yml</span><br></pre></td></tr></table></figure><p>一个标准的 Ansible Playbook 结构：在最外层暴露我们需要执行的 YAML 配置，所有具体的操作和配置文件都放到 roles 和 conf 中，下面我们来一点点看具体做了什么，哪些地方是我们需要注意的。</p><h4 id="local-prepare-yml"><a href="#local-prepare-yml" class="headerlink" title="local_prepare.yml"></a>local_prepare.yml</h4><p>在这里一共做了以下这么几件事情：</p><ul><li>准备 binary 下载地址配置</li><li>检查网络(GFW)</li><li>下载 binary</li><li>准备 fio 软件</li><li>清理下载路径</li></ul><p>主要都是做环境准备的工作，其中网络检测这没有用我们日常使用最多的 <code>ping</code> ，而是使用了 <code>curl</code> ，这里猜测是因为有时候机器是可以与互联网通信，但是没有配置 DNS 解析，导致后续的下载也会失败，所以直接 <code>curl baidu.com</code> 是一个不错的选择。</p><p>(这里还检查了 GFW - - )</p><p>用到的 Ansible 模块有：</p><ul><li>file</li><li>shell （果然没人会去用 command 的</li><li>template</li><li>get_url</li><li>…</li></ul><h4 id="bootstrap-yml"><a href="#bootstrap-yml" class="headerlink" title="bootstrap.yml"></a>bootstrap.yml</h4><ul><li>基本环境配置参数检查（机器数、配置、OS发行版）</li><li>python 环境准备，NTP 软件包安装</li><li>设置内核参数</li><li>检查系统配置（cpu 调节器配置如果不是performance， 非开发模式下会强制退出）</li><li>fio 检测<ul><li>psync ,bs=32k,iodepth=4,numjobs=4  randread iops不小于 40k</li><li>psync, bs=32k,iodepth=4,numjobs=4  randrw(100%r,0%w) iops均不低于 10k</li><li>psync, bs=32k, iodepth=1,numjobs=1  randrw(100%r,0%w) lat randread 不高于 0.25ms，write 不高于 30us</li></ul></li></ul><p>内核参数部分修改了如下配置：</p><ul><li>net.core.somaxconn=32768 # 最大 socket 连接数，默认为 128</li><li>vm.swappiness=0 # 禁用 swap 空间</li><li>net.ipv4.tcp_syncookies=0 # 关闭 synccookies，默认为关闭</li><li>net.ipv4.tcp_tw_recycle=0 # TIME_WAIT 快速回收，默认关闭，关于 TIME_WAIT 状态可以在《TCP/IP 详解 卷一 协议》 中了解更多</li><li>fs.file-max=1000000 # 最大文件数，默认为 1024</li><li>irqbalance ONESHOT=yes # irqbalance 不以守护进程方式运行</li></ul><p>关于 fio 检测，嗯，我还是太天真了，这个级别的磁盘延迟要求，估计只有 PCIe SSD 可以满足了吧。</p><h4 id="deploy-yml"><a href="#deploy-yml" class="headerlink" title="deploy.yml"></a>deploy.yml</h4><p>这部分通用的东西比较少，都是内部的一些配置文件和服务。</p><h4 id="start-yml"><a href="#start-yml" class="headerlink" title="start.yml"></a>start.yml</h4><p>貌似 TiDB 服务的启动是有严格的依赖关系的，不知道这里如果启动顺序调整，是否会自动重试直到相应依赖服务启动完成。</p><h2 id="滚动升级"><a href="#滚动升级" class="headerlink" title="滚动升级"></a>滚动升级</h2><p>在视频讲解中，提到 TiDB 支持滚动升级，这一点在一个 2B 产品功能中很重要，随着产品的开发迭代，能够无痛的给客户进行升级是很重要的。</p><p>在官方文档中，看到了两个升级文档，分别为 1.0 升级到 2.0 和 2.0 升级到 2.1。使用的滚动升级 YAML 文件为：rolling_update.yml<br>升级过程为滚动升级，步骤为：</p><ol><li>pre check</li><li>upgrade</li><li>post upgrade</li></ol><p>这里没搞懂为什么 pre check tidb-server 也要顺序执行，看上去只是检测了 TiDB 版本。</p><p>在 YAML 中指定的升级顺序为：</p><ol><li>pd</li><li>tikv</li><li>pump</li><li>tidb</li></ol><p>这里比较遗憾没有找到升级失败的处理方式，升级 PD 过程中检查健康状态连续 1分钟都未恢复正常，此时 Ansible 退出，PD 处于高版本状态，其他组件 tikv，tidb 还处于低版本状态，不知道这时如何处理，也许是产品上保证了兼容性？</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>了解了 TiDB 的安装和升级，着重学习下 Ansible Playbook 的组织形式，在 tidb-ansible 中，所有的功能都拆分的很细，能采用 ansible 的都用 ansible 实现了，但是难免需要一些脚本配合（fio），这里的选择需要根据实际情况来进行相应修改。</p><h2 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h2><p>不知道为什么在代码中 scripts 下放置了许多json 配置文件。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;了解我的同学应该知道，我目前负责公司产品的运维工具开发相关的工作，作为一款 2B 的产品，在产品运维过程中，总是有一些不愉快（又不能让客户知道）的繁琐操作：这些操作可能是为了防止过程中出现错误，而不断添加的检测条件；也有可能是历史问题，随着产品发布迭代而一直遗留至今。&lt;/p
      
    
    </summary>
    
    
      <category term="Ansible" scheme="https://zdyxry.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>记一次 libcgroup 配置失败</title>
    <link href="https://zdyxry.github.io/2019/04/11/%E8%AE%B0%E4%B8%80%E6%AC%A1-libcgroup-%E9%85%8D%E7%BD%AE%E5%A4%B1%E8%B4%A5/"/>
    <id>https://zdyxry.github.io/2019/04/11/记一次-libcgroup-配置失败/</id>
    <published>2019-04-11T14:04:47.000Z</published>
    <updated>2019-04-11T14:06:40.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cgroup-配置失败解决方案"><a href="#cgroup-配置失败解决方案" class="headerlink" title="cgroup 配置失败解决方案"></a>cgroup 配置失败解决方案</h2><p>看过之前博客的同学应该知道，我一直使用的 libcgroup 来进行 cgroup 配置，简单方便。<br>最近遇到了一个报错，很坑，记录一下。</p><h2 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h2><p>接到反馈说有个环境在产品升级之后， cgconfig.service 无法启动，当时的配置如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran-test 21:31:59 ~]<span class="variable">$cat</span> /etc/cgconfig.conf</span><br><span class="line"><span class="comment"># yiran cgroups configuration</span></span><br><span class="line"></span><br><span class="line">group . &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.memory_pressure_enabled = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group yiran &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"0,1,2,3,4,5"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group yiran/bb-main &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"0"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group yiran/bb-io &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group yiran/aa-main &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"2"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group yiran/others &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"3"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"1"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">group yiran/app &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"4,5"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"0"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">group qemu &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"0"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>咋一看，配置文件看上去是正确的，除了最后一个组的 <code>cpuset.cpus</code> 配置略长，但是也没错，按道理应该服务正常启动才对，尝试重启服务查看服务报错信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran-test 21:33:55 ~]<span class="variable">$systemctl</span> restart cgred</span><br><span class="line">[root@yiran-test 21:33:59 ~]<span class="variable">$systemctl</span> restart cgconfig</span><br><span class="line">Job <span class="keyword">for</span> cgconfig.service failed because the control process exited with error code. See <span class="string">"systemctl status cgconfig.service"</span> and <span class="string">"journalctl -xe"</span> <span class="keyword">for</span> details.</span><br><span class="line">[root@yiran-test 21:34:05 ~]<span class="variable">$systemctl</span> status cgconfig</span><br><span class="line">● cgconfig.service - Control Group configuration service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/cgconfig.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: <span class="built_in">exit</span>-code) since 四 2019-04-11 21:34:05 CST; 4s ago</span><br><span class="line">  Process: 6744 ExecStop=/usr/sbin/cgclear -l /etc/cgconfig.conf -L /etc/cgconfig.d -e (code=exited, status=3)</span><br><span class="line">  Process: 11465 ExecStart=/usr/sbin/cgconfigparser -l /etc/cgconfig.conf -L /etc/cgconfig.d -s 1664 (code=exited, status=109)</span><br><span class="line"> Main PID: 11465 (code=exited, status=109)</span><br><span class="line"></span><br><span class="line">4月 11 21:34:05 yiran-test systemd[1]: Starting Control Group configuration service...</span><br><span class="line">4月 11 21:34:05 yiran-test cgconfigparser[11465]: /usr/sbin/cgconfigparser; error loading /etc/cgconfig.conf: Failed to remove a non-empty group</span><br><span class="line">4月 11 21:34:05 yiran-test cgconfigparser[11465]: Error: failed to <span class="built_in">set</span> /sys/fs/cgroup/cpuset/qemu/cpuset.cpus: Invalid argument</span><br><span class="line">4月 11 21:34:05 yiran-test systemd[1]: cgconfig.service: main process exited, code=exited, status=109/n/a</span><br><span class="line">4月 11 21:34:05 yiran-test systemd[1]: Failed to start Control Group configuration service.</span><br><span class="line">4月 11 21:34:05 yiran-test systemd[1]: Unit cgconfig.service entered failed state.</span><br><span class="line">4月 11 21:34:05 yiran-test systemd[1]: cgconfig.service failed.</span><br></pre></td></tr></table></figure><p>发现在重启 cgconfig 时报错，报错信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4月 11 21:34:05 yiran-test cgconfigparser[11465]: /usr/sbin/cgconfigparser; error loading /etc/cgconfig.conf: Failed to remove a non-empty group</span><br><span class="line">4月 11 21:34:05 yiran-test cgconfigparser[11465]: Error: failed to <span class="built_in">set</span> /sys/fs/cgroup/cpuset/qemu/cpuset.cpus: Invalid argument</span><br></pre></td></tr></table></figure><p>第一行说移除一个非空的 cgroup 失败，下一条提示 <code>/sys/fs/cgroup/cpuset/qemu/cpuset.cpus</code> 也就是我们觉得略微异常的 cgroup 参数无效，可是参数明明是正确配置的，为啥就无效了呢？</p><h2 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h2><p>尝试修改 <code>cpuset.cpus</code> ，将其调整为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran-test 21:36:32 ~]<span class="variable">$tail</span> /etc/cgconfig.conf</span><br><span class="line"></span><br><span class="line">group qemu &#123;</span><br><span class="line">    cpuset &#123;</span><br><span class="line">        cpuset.cpus = <span class="string">"6,7,8,9,10"</span>;</span><br><span class="line">        cpuset.mems = <span class="string">"0-1"</span>;</span><br><span class="line">        cpuset.cpu_exclusive = <span class="string">"0"</span>;</span><br><span class="line">        cpuset.mem_hardwall = <span class="string">"1"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 cgconfig 服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran-test 21:36:36 ~]<span class="variable">$systemctl</span> restart cgred</span><br><span class="line">[root@yiran-test 21:36:55 ~]<span class="variable">$systemctl</span> restart cgconfig</span><br><span class="line">[root@yiran-test 21:37:00 ~]<span class="variable">$systemctl</span> status cgconfig</span><br><span class="line">● cgconfig.service - Control Group configuration service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/cgconfig.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (exited) since 四 2019-04-11 21:37:00 CST; 3s ago</span><br><span class="line">  Process: 6744 ExecStop=/usr/sbin/cgclear -l /etc/cgconfig.conf -L /etc/cgconfig.d -e (code=exited, status=3)</span><br><span class="line">  Process: 17491 ExecStart=/usr/sbin/cgconfigparser -l /etc/cgconfig.conf -L /etc/cgconfig.d -s 1664 (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 17491 (code=exited, status=0/SUCCESS)</span><br><span class="line"></span><br><span class="line">4月 11 21:37:00 yiran-test systemd[1]: Starting Control Group configuration service...</span><br><span class="line">4月 11 21:37:00 yiran-test systemd[1]: Started Control Group configuration service.</span><br></pre></td></tr></table></figure><p>服务正常运行了，那么我推测可能是跟 <code>cpuset.cpus</code> 长度有关，这时候只能求助于 Google 啦。</p><p>很容易，我们找到了这个答案，RedHat <a href="https://access.redhat.com/solutions/3364311" target="_blank" rel="noopener">官方 KB</a> 中的介绍：</p><blockquote><p>Root Cause<br>Previously, the internal representation of a value of any cgroup subsystem parameter was limited to have the length of 100 characters at maximum. Consequently, the libcgroup library truncated the values longer than 100 characters before writing them to a file representing matching cgroup subsystem parameter in the kernel.</p></blockquote><blockquote><p>Resolution<br>The maximal length of values of cgroup subsystem parameters in libcgroup has been extended to 4096 characters. As a result, libcgroup now handles values of cgroup subsystem parameters with any length correctly. (BZ#1549175)<br>RHEL 6 <a href="https://access.redhat.com/downloads/content/rhel---6/x86_64/169/libcgroup/0.40.rc1-26.el6/src/fd431d51/package" target="_blank" rel="noopener">https://access.redhat.com/downloads/content/rhel---6/x86_64/169/libcgroup/0.40.rc1-26.el6/src/fd431d51/package</a> (or newer) via RHBA-2018:1861<br>RHEL 7 libcgroup-0.41-20.el7 (or newer) via RHBA-2018:3058</p></blockquote><p>官方给出的解决方式是通过升级 libcgroup 来解决，但是我不想这么做。</p><p>为什么？要知道在生产环境中，我们想要进行第三方软件包的升级是要经过层层测试的，等到测试完成不知道什么时候了，所以我们需要一个快速折中方案。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们知道，libcgroup 只是作为一个配置 cgroup 软件的一种，最终操作的都是 cgroup 实际挂载点下的配置文件，比如 CentOS 默认的 <code>/sys/fs/cgroup/</code> 。</p><p>那么我们来看下正常配置下 qemu 组中的 <code>cpuset.cpus</code> 配置长什么样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran-test 21:37:04 ~]<span class="variable">$cat</span> /sys/fs/cgroup/cpuset/qemu/cpuset.cpus</span><br><span class="line">6-10</span><br></pre></td></tr></table></figure><p>Ok，我们明明在 libcgroup 配置文件中写的是 <code>6,7,8,9,10</code> ,在 cgroup 配置文件中就转换成了 <code>6-10</code>， 那么一切都简单了。</p><p>当我们配置 cgroup 如果 cpu processor id 是连续的，那么我们就可以通过 <code>-</code> 来连接起始和终止 id 就可以了，问题解决。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>libcgroup 作为 RedHat 官方指定的 cgroup 配置工具，没想到会出现这种问题，如果 <code>cpuset.cpus</code> 我们要设置非连续的 cpu processor id 的话，只能通过升级方式解决了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cgroup-配置失败解决方案&quot;&gt;&lt;a href=&quot;#cgroup-配置失败解决方案&quot; class=&quot;headerlink&quot; title=&quot;cgroup 配置失败解决方案&quot;&gt;&lt;/a&gt;cgroup 配置失败解决方案&lt;/h2&gt;&lt;p&gt;看过之前博客的同学应该知道，我一直使
      
    
    </summary>
    
    
      <category term="cgroups" scheme="https://zdyxry.github.io/tags/cgroups/"/>
    
  </entry>
  
  <entry>
    <title>apscheduler 源码阅读</title>
    <link href="https://zdyxry.github.io/2019/04/06/apscheduler-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>https://zdyxry.github.io/2019/04/06/apscheduler-源码阅读/</id>
    <published>2019-04-06T02:58:49.000Z</published>
    <updated>2019-04-06T02:59:12.905Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>apscheduler 全称 <code>Advanced Python Scheduler</code>，调度器，主要功能如下：</p><ul><li>动态添加、删除任务</li><li>暂停、恢复任务</li><li>周期性调度：cron,date,interval</li><li>…</li></ul><p>那么接下来我们根据官方示例，看看 apscheduler 是如何进行处理任务的。</p><p>示例版本为 2.1，因为在 2.1 版本包含目前 master 分支上的主要功能，简单易懂。</p><p>代码结构如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/apscheduler</span><br><span class="line">2.1 ✔ $ tree apscheduler</span><br><span class="line">apscheduler</span><br><span class="line">├── __init__.py</span><br><span class="line">├── events.py</span><br><span class="line">├── job.py</span><br><span class="line">├── jobstores</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── base.py</span><br><span class="line">│   ├── mongodb_store.py</span><br><span class="line">│   ├── ram_store.py</span><br><span class="line">│   ├── redis_store.py</span><br><span class="line">│   ├── shelve_store.py</span><br><span class="line">│   └── sqlalchemy_store.py</span><br><span class="line">├── scheduler.py</span><br><span class="line">├── threadpool.py</span><br><span class="line">├── triggers</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── cron</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   ├── expressions.py</span><br><span class="line">│   │   └── fields.py</span><br><span class="line">│   ├── interval.py</span><br><span class="line">│   └── simple.py</span><br><span class="line">└── util.py</span><br></pre></td></tr></table></figure></p><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> apscheduler.scheduler <span class="keyword">import</span> Scheduler</span><br><span class="line"><span class="keyword">from</span> apscheduler.jobstores.shelve_store <span class="keyword">import</span> ShelveJobStore</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">alarm</span><span class="params">(time)</span>:</span></span><br><span class="line">    print(<span class="string">'Alarm! This alarm was scheduled at %s.'</span> % time)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    scheduler = Scheduler(standalone=<span class="keyword">True</span>)</span><br><span class="line">    scheduler.add_jobstore(ShelveJobStore(<span class="string">'example.db'</span>), <span class="string">'shelve'</span>)</span><br><span class="line">    alarm_time = datetime.now() + timedelta(seconds=<span class="number">10</span>)</span><br><span class="line">    scheduler.add_date_job(alarm, alarm_time, name=<span class="string">'alarm'</span>,</span><br><span class="line">                           jobstore=<span class="string">'shelve'</span>, args=[datetime.now()])</span><br><span class="line">    print(<span class="string">'To clear the alarms, delete the example.db file.'</span>)</span><br><span class="line">    print(<span class="string">'Press Ctrl+C to exit'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        scheduler.start()</span><br><span class="line">    <span class="keyword">except</span> (KeyboardInterrupt, SystemExit):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>上述示例很容易理解，首先对 <code>Scheduler</code> 实例化，然后添加 jobstore，定义一个名为 <code>alarm</code> 的 job，并指定其运行时间为当前时间 + 10s，将该 job 添加到 scheduler 中，添加 job 类型为 <code>date_job</code>，然后启动 scheduler。</p><p>可以看到我们所有的操作都是通过 scheduler 方法实现的，那么我们来看下 <code>Scheduler</code> 类具体实现了哪些功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This class is responsible for scheduling jobs and triggering</span></span><br><span class="line"><span class="string">    their execution.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    _stopped = <span class="keyword">True</span></span><br><span class="line">    _thread = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, gconfig=&#123;&#125;, **options)</span>:</span></span><br><span class="line">        self._wakeup = Event()</span><br><span class="line">        self._jobstores = &#123;&#125;</span><br><span class="line">        self._jobstores_lock = Lock()</span><br><span class="line">        self._listeners = []</span><br><span class="line">        self._listeners_lock = Lock()</span><br><span class="line">        self._pending_jobs = []</span><br><span class="line">        self.configure(gconfig, **options)</span><br></pre></td></tr></table></figure><p>在 <code>Scheduler</code> 构造函数中，对一些变量进行初始化，这里要注意 <code>self._wakeup</code> ，后续的一些主要功能都是通过它来实现的。接下来看看 <code>add_jobstore</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_jobstore</span><span class="params">(self, jobstore, alias, quiet=False)</span>:</span></span><br><span class="line">    self._jobstores_lock.acquire() <span class="comment"># 请求锁</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> alias <span class="keyword">in</span> self._jobstores:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'Alias "%s" is already in use'</span> % alias)</span><br><span class="line">        self._jobstores[alias] = jobstore <span class="comment"># 将 jobstore 别名作为 key，添加到 self._jobstores 中</span></span><br><span class="line">        jobstore.load_jobs() <span class="comment"># 加载 jobstore 中所有 job</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        self._jobstores_lock.release() <span class="comment"># 释放锁</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Notify listeners that a new job store has been added</span></span><br><span class="line">    self._notify_listeners(JobStoreEvent(EVENT_JOBSTORE_ADDED, alias)) <span class="comment"># 事件通知</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Notify the scheduler so it can scan the new job store for jobs</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> quiet:</span><br><span class="line">        self._wakeup.set() <span class="comment"># 将 Event 置为 True</span></span><br></pre></td></tr></table></figure><p>在 <code>add_jobstore</code> 中，将 jobstore 添加到 scheduler 中，并加载当前 jobstore 中的所有任务，接下来将具体的 job 添加到 scheduler 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_date_job</span><span class="params">(self, func, date, args=None, kwargs=None, **options)</span>:</span></span><br><span class="line">    trigger = SimpleTrigger(date)</span><br><span class="line">    <span class="keyword">return</span> self.add_job(trigger, func, args, kwargs, **options)</span><br></pre></td></tr></table></figure><p>这里的 <code>SimpleTrigger</code> 只是多种 Trigger 中的一种，根据 Trigger 类型的不同，最主要的差别在于 <code>get_next_fire_time</code> 计算方式不同。</p><p>如果添加的任务是 interval_job，那么对应 Trigger 为 <code>IntervalTrigger</code> ；如果添加的任务是 cron_job，那么对应的 Trigger 为 <code>CronTrigger</code>。</p><p>可以看到不同的任务只是 Trigger 计算方式不同，最终还是通过 <code>add_job</code> 方法，继续看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_job</span><span class="params">(self, trigger, func, args, kwargs, jobstore=<span class="string">'default'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            **options)</span>:</span></span><br><span class="line">    job = Job(trigger, func, args <span class="keyword">or</span> [], kwargs <span class="keyword">or</span> &#123;&#125;,</span><br><span class="line">              options.pop(<span class="string">'misfire_grace_time'</span>, self.misfire_grace_time),</span><br><span class="line">              options.pop(<span class="string">'coalesce'</span>, self.coalesce), **options) <span class="comment"># 将 job 实例化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.running: <span class="comment"># 如果 scheduler 未启动，那么将其添加到等待队列中</span></span><br><span class="line">        self._pending_jobs.append((job, jobstore))</span><br><span class="line">        logger.info(<span class="string">'Adding job tentatively -- it will be properly '</span></span><br><span class="line">                    <span class="string">'scheduled when the scheduler starts'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self._real_add_job(job, jobstore, <span class="keyword">True</span>) <span class="comment"># 否则添加 job 到 jobstore 中</span></span><br><span class="line">    <span class="keyword">return</span> job</span><br></pre></td></tr></table></figure><p>继续看 <code>_real_add_job</code> 中的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_real_add_job</span><span class="params">(self, job, jobstore, wakeup)</span>:</span></span><br><span class="line">    job.compute_next_run_time(datetime.now()) <span class="comment"># 计算job 下次运行时间</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> job.next_run_time:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Not adding job since it would never be run'</span>)</span><br><span class="line"></span><br><span class="line">    self._jobstores_lock.acquire() <span class="comment"># 请求锁</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            store = self._jobstores[jobstore]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'No such job store: %s'</span> % jobstore)</span><br><span class="line">        store.add_job(job) <span class="comment"># 添加 job</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        self._jobstores_lock.release()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Notify listeners that a new job has been added</span></span><br><span class="line">    event = JobStoreEvent(EVENT_JOBSTORE_JOB_ADDED, jobstore, job)</span><br><span class="line">    self._notify_listeners(event) <span class="comment"># 事件通知</span></span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">'Added job "%s" to job store "%s"'</span>, job, jobstore)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Notify the scheduler about the new job</span></span><br><span class="line">    <span class="keyword">if</span> wakeup:</span><br><span class="line">        self._wakeup.set() <span class="comment"># # 将 Event 置为 True</span></span><br></pre></td></tr></table></figure><p>在 <code>_real_add_job</code> 中我们终于看到 <code>store.add_job(job)</code> ，至于 <code>store.add_job</code> 如何实现我们之后看 <code>JobStore</code> 再说。</p><p>现在我们已经给 scheduler 添加了 jobstore 和 job，那么看下 scheduler 是如何运行的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Schedule all pending jobs</span></span><br><span class="line">    <span class="keyword">for</span> job, jobstore <span class="keyword">in</span> self._pending_jobs: <span class="comment"># 将 scheduler 未运行时添加的 job，即在等待队列中的 job 添加到 jobstore 中</span></span><br><span class="line">        self._real_add_job(job, jobstore, <span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">del</span> self._pending_jobs[:]</span><br><span class="line"></span><br><span class="line">    self._stopped = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">if</span> self.standalone:</span><br><span class="line">        self._main_loop()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self._thread = Thread(target=self._main_loop, name=<span class="string">'APScheduler'</span>)</span><br><span class="line">        self._thread.setDaemon(self.daemonic)</span><br><span class="line">        self._thread.start()</span><br></pre></td></tr></table></figure><p>在 scheduler 运行时，会先将所有的 job 加载到 jobstore 中，然后调用 <code>self._main_loop</code> ，如果 Standalone 为 True，则会一直阻塞知道没有 job 需要运行，看看 <code>self._main_loop</code> 做了啥：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_main_loop</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Executes jobs on schedule."""</span></span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">'Scheduler started'</span>)</span><br><span class="line">    self._notify_listeners(SchedulerEvent(EVENT_SCHEDULER_START)) <span class="comment"># 事件通知 </span></span><br><span class="line"></span><br><span class="line">    self._wakeup.clear()</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> self._stopped:</span><br><span class="line">        logger.debug(<span class="string">'Looking for jobs to run'</span>)</span><br><span class="line">        now = datetime.now()</span><br><span class="line">        next_wakeup_time = self._process_jobs(now) <span class="comment"># 计算下次唤醒时间</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> next_wakeup_time <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            wait_seconds = time_difference(next_wakeup_time, now)</span><br><span class="line">            logger.debug(<span class="string">'Next wakeup is due at %s (in %f seconds)'</span>,</span><br><span class="line">                         next_wakeup_time, wait_seconds)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self._wakeup.wait(wait_seconds) <span class="comment"># 等待 Event flag</span></span><br><span class="line">            <span class="keyword">except</span> IOError:  <span class="comment"># Catch errno 514 on some Linux kernels</span></span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            self._wakeup.clear()</span><br><span class="line">        <span class="keyword">elif</span> self.standalone:</span><br><span class="line">            logger.debug(<span class="string">'No jobs left; shutting down scheduler'</span>)</span><br><span class="line">            self.shutdown() <span class="comment"># 若 scheduler standalone 为 True 且 jobs 为空，则停止 scheduler</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logger.debug(<span class="string">'No jobs; waiting until a job is added'</span>)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self._wakeup.wait() <span class="comment"># 等待 Event flag</span></span><br><span class="line">            <span class="keyword">except</span> IOError:  <span class="comment"># Catch errno 514 on some Linux kernels</span></span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            self._wakeup.clear()</span><br></pre></td></tr></table></figure><p>还记得上面提到的 <code>Scheduler</code> 构造函数中的 <code>self._wakeup</code> 么，它实际上是 <code>threading.Event</code> ，它的 wait 方法会一直 block 直到 Event flag 为 True，也就是我们上面看到的 <code>self._wakeup.set()</code> ，那么我们可以知道在 <code>Scheduler</code> 中有几种场景会置为 True：</p><ol><li>Scheduler.shutdown</li><li>Scheduler.add_jobstore</li><li>Scheduler._real_add_job</li></ol><p>如果没有触发上述场景，则 <code>_main_loop</code> 会根据 jobs 的执行时间一直循环等待。</p><h2 id="JobStore"><a href="#JobStore" class="headerlink" title="JobStore"></a>JobStore</h2><p>在 apscheduler 中，JobStore 只是单纯的实现了 Job 相关的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobStore</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_job</span><span class="params">(self, job)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_job</span><span class="params">(self, job)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove_job</span><span class="params">(self, job)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_jobs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br></pre></td></tr></table></figure><p>其中，对 job 的操作会根据 JobStore 类型的不同，而采用不同的序列化方式，比如在 <code>MongoDBJobStore</code> 中采用的是 <code>bson.binary</code>，而在其他 JobStore 比如 <code>RedisJobStore</code> 中采用的都是 <code>pickle</code>。</p><h2 id="Events"><a href="#Events" class="headerlink" title="Events"></a>Events</h2><p>在 <code>Scheduler</code> 中，我们已经看到通过 <code>threading.Event</code> 来实现事件通知的，那么我们通知的 <code>Event</code> 都是在 <code>apscheduler.events</code> 中定义好的，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobEvent</span><span class="params">(SchedulerEvent)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, code, job, scheduled_run_time, retval=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 exception=None, traceback=None)</span>:</span></span><br><span class="line">        SchedulerEvent.__init__(self, code)</span><br><span class="line">        self.job = job</span><br><span class="line">        self.scheduled_run_time = scheduled_run_time</span><br><span class="line">        self.retval = retval</span><br><span class="line">        self.exception = exception</span><br><span class="line">        self.traceback = traceback</span><br></pre></td></tr></table></figure><p>在 <code>JobEvent</code> 中，我们能看到 job 的执行时间，返回值，异常捕获等信息。如果看过之前关于 <a href="https://zdyxry.github.io/2019/03/31/huey-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/">huey 博客</a> 的同学应该知道，在 huey 中是可以直接通过 task id 获取 task 执行结果的，但是在 apscheduler 中，我们并没有直接获取该结果的方法，而是通过在 <code>Scheduler</code> 中的 <code>add_listener</code> 添加监听者，监控指定成功的 Job 获取该 Job 的返回值，感觉这里不太友好。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>到这里我们基本上已经将 apscheduler 的流程走了一遍，具体的 Trigger 计算时间的方法之后有机会单独写一下关于 cron，interval，date 的计算方法。</p><p>与 huey 相比，apscheduler 使用上要简单，但是简单也意味着功能的不足，比如获取 job 执行结果、job retry 机制等等。当然也有比较好的地方，apscheduler 在跟 web 框架比如 Flask，Django 集成的时候有一些第三方插件可以直接使用，不用像 Huey 一样要单独启动一个 consumer 进程，比较方便。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;apscheduler 全称 &lt;code&gt;Advanced Python Scheduler&lt;/code&gt;，调度器，主要功能如下：&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Python" scheme="https://zdyxry.github.io/tags/Python/"/>
    
  </entry>
  
</feed>

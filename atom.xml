<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yiran&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zdyxry.github.io/"/>
  <updated>2019-09-13T14:07:29.869Z</updated>
  <id>https://zdyxry.github.io/</id>
  
  <author>
    <name>yiran</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 实战-Operator Finalizers 实现</title>
    <link href="https://zdyxry.github.io/2019/09/13/Kubernetes-%E5%AE%9E%E6%88%98-Operator-Finalizers/"/>
    <id>https://zdyxry.github.io/2019/09/13/Kubernetes-实战-Operator-Finalizers/</id>
    <published>2019-09-13T12:24:41.000Z</published>
    <updated>2019-09-13T14:07:29.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在写 k8s Operator，在看示例的时候看到 controller 都会设置 Finalizers，今天来聊一聊 Finalizers 和相关实现。</p><h2 id="Finalizers"><a href="#Finalizers" class="headerlink" title="Finalizers"></a>Finalizers</h2><p>Finalizers 允许 Operator 控制器实现异步的 pre-delete hook。比如你给 API 类型中的每个对象都创建了对应的外部资源，你希望在 k8s 删除对应资源时同时删除关联的外部资源，那么可以通过 Finalizers 来实现。</p><p>Finalizers 是由字符串组成的列表，当 Finalizers 字段存在时，相关资源不允许被强制删除。存在 Finalizers 字段的的资源对象接收的第一个删除请求设置 <code>metadata.deletionTimestamp</code> 字段的值， 但不删除具体资源，在该字段设置后， <code>finalizer</code> 列表中的对象只能被删除，不能做其他操作。</p><p>当 <code>metadata.deletionTimestamp</code> 字段非空时，controller watch 对象并执行对应 finalizers 的动作，当所有动作执行完后，需要清空 finalizers ，之后 k8s 会删除真正想要删除的资源。</p><h2 id="Operator-finalizers-使用"><a href="#Operator-finalizers-使用" class="headerlink" title="Operator finalizers 使用"></a>Operator finalizers 使用</h2><p>介绍了 Finalizers 概念，那么我们来看看在 Operator 中如何使用，在 Operator Controller 中，最重要的逻辑就是 Reconcile 方法，finalizers 也是在 Reconcile 中实现的。要注意的是，设置了 Finalizers 会导致 k8s 的 delete 动作转为设置 <code>metadata.deletionTimestamp</code> 字段，如果你通过 <code>kubectl get</code> 命令看到资源存在这个字段，则表示资源正在删除（deleting）。</p><p>有以下几点需要理解：</p><ol><li>如果资源对象未被删除且未设置 finalizers，则添加 finalizer并更新 k8s 资源对象；</li><li>如果正在删除资源对象并且 finalizers 仍然存在于 finalizers 列表中，则执行 pre-delete hook并删除 finalizers ，更新资源对象；</li><li>由于以上两点，需要确保 pre-delete hook是幂等的。</li></ol><h3 id="kuberbuilder-示例"><a href="#kuberbuilder-示例" class="headerlink" title="kuberbuilder 示例"></a>kuberbuilder 示例</h3><p>我们来看一个 kubebuilder 官方示例：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *CronJobReconciler)</span> <span class="title">Reconcile</span><span class="params">(req ctrl.Request)</span> <span class="params">(ctrl.Result, error)</span></span> &#123;</span><br><span class="line">    ctx := context.Background()</span><br><span class="line">    log := r.Log.WithValues(<span class="string">"cronjob"</span>, req.NamespacedName)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> cronJob batch.CronJob</span><br><span class="line">    <span class="keyword">if</span> err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        log.Error(err, <span class="string">"unable to fetch CronJob"</span>)</span><br><span class="line">        <span class="keyword">return</span> ctrl.Result&#123;&#125;, ignoreNotFound(err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明 finalizer 字段，类型为字符串</span></span><br><span class="line">    myFinalizerName := <span class="string">"storage.finalizers.tutorial.kubebuilder.io"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过检查 DeletionTimestamp 字段是否为0 判断资源是否被删除</span></span><br><span class="line">    <span class="keyword">if</span> cronJob.ObjectMeta.DeletionTimestamp.IsZero() &#123;</span><br><span class="line">        <span class="comment">// 如果为0 ，则资源未被删除，我们需要检测是否存在 finalizer，如果不存在，则添加，并更新到资源对象中</span></span><br><span class="line">        <span class="keyword">if</span> !containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) &#123;</span><br><span class="line">            cronJob.ObjectMeta.Finalizers = <span class="built_in">append</span>(cronJob.ObjectMeta.Finalizers, myFinalizerName)</span><br><span class="line">            <span class="keyword">if</span> err := r.Update(context.Background(), cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果不为 0 ，则对象处于删除中</span></span><br><span class="line">        <span class="keyword">if</span> containsString(cronJob.ObjectMeta.Finalizers, myFinalizerName) &#123;</span><br><span class="line">            <span class="comment">// 如果存在 finalizer 且与上述声明的 finalizer 匹配，那么执行对应 hook 逻辑</span></span><br><span class="line">            <span class="keyword">if</span> err := r.deleteExternalResources(cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="comment">// 如果删除失败，则直接返回对应 err，controller 会自动执行重试逻辑</span></span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果对应 hook 执行成功，那么清空 finalizers， k8s 删除对应资源</span></span><br><span class="line">            cronJob.ObjectMeta.Finalizers = removeString(cronJob.ObjectMeta.Finalizers, myFinalizerName)</span><br><span class="line">            <span class="keyword">if</span> err := r.Update(context.Background(), cronJob); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ctrl.Result&#123;&#125;, err</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *Reconciler)</span> <span class="title">deleteExternalResources</span><span class="params">(cronJob *batch.CronJob)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// 删除 crobJob关联的外部资源逻辑</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// 需要确保实现是幂等的</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">containsString</span><span class="params">(slice []<span class="keyword">string</span>, s <span class="keyword">string</span>)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> _, item := <span class="keyword">range</span> slice &#123;</span><br><span class="line">        <span class="keyword">if</span> item == s &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">removeString</span><span class="params">(slice []<span class="keyword">string</span>, s <span class="keyword">string</span>)</span> <span class="params">(result []<span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> _, item := <span class="keyword">range</span> slice &#123;</span><br><span class="line">        <span class="keyword">if</span> item == s &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">        result = <span class="built_in">append</span>(result, item)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="cluster-api-provider-vsphere-实现"><a href="#cluster-api-provider-vsphere-实现" class="headerlink" title="cluster-api-provider-vsphere 实现"></a>cluster-api-provider-vsphere 实现</h3><p>看完了示例，我们来招一个具体项目看看，cluster-api-provider-vsphere 是 cluster-api 相关项目，用于提供 vsphere 相关资源创建的 Operator，采用 kubebuilder 来实现的。</p><p>vspheremachine_controller.go 中实现了 Reconcile 方法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reconcile ensures the back-end state reflects the Kubernetes resource state intent.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *VSphereMachineReconciler)</span> <span class="title">Reconcile</span><span class="params">(req ctrl.Request)</span> <span class="params">(_ ctrl.Result, reterr error)</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Always close the context when exiting this function so we can persist any VSphereMachine changes.</span></span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := machineContext.Patch(); err != <span class="literal">nil</span> &amp;&amp; reterr == <span class="literal">nil</span> &#123;</span><br><span class="line">reterr = err</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Handle deleted machines</span></span><br><span class="line"><span class="keyword">if</span> !vsphereMachine.ObjectMeta.DeletionTimestamp.IsZero() &#123;</span><br><span class="line"><span class="keyword">return</span> r.reconcileDelete(machineContext)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Handle non-deleted machines</span></span><br><span class="line"><span class="keyword">return</span> r.reconcileNormal(machineContext)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>Reconcile</code> 中检测了 <code>DeletionTimestamp</code> 是否为0 ，如果不为0 ，则表示资源处于正在删除中，那么来看下 <code>reconcileDelete</code> 实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *VSphereMachineReconciler)</span> <span class="title">reconcileDelete</span><span class="params">(ctx *context.MachineContext)</span> <span class="params">(reconcile.Result, error)</span></span> &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Handling deleted VSphereMachine"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> vmService services.VirtualMachineService = &amp;govmomi.VMService&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行删除虚拟机逻辑</span></span><br><span class="line">vm, err := vmService.DestroyVM(ctx)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="comment">// 如果删除失败，则直接返回错误，controller 会自动重试</span></span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, errors.Wrapf(err, <span class="string">"failed to destroy VM"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重新调度删除虚拟机逻辑，直到虚拟机状态处于 notfound 状态</span></span><br><span class="line"><span class="keyword">if</span> vm.State != infrav1.VirtualMachineStateNotFound &#123;</span><br><span class="line">ctx.Logger.V(<span class="number">6</span>).Info(<span class="string">"requeuing operation until vm state is reconciled"</span>, <span class="string">"expected-vm-state"</span>, infrav1.VirtualMachineStateNotFound, <span class="string">"actual-vm-state"</span>, vm.State)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: config.DefaultRequeue&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// pre-delete hook执行成功，也就是上面的删除虚拟机逻辑执行成功，则清空 Finalizers</span></span><br><span class="line">ctx.VSphereMachine.Finalizers = clusterutilv1.Filter(ctx.VSphereMachine.Finalizers, infrav1.MachineFinalizer)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到整体逻辑与示例的使用是一致的，主要通过这种方式来达到 pre-delete hook 的效果。</p><h3 id="k8s-initializer-finalizer-practice"><a href="#k8s-initializer-finalizer-practice" class="headerlink" title="k8s-initializer-finalizer-practice"></a>k8s-initializer-finalizer-practice</h3><p>在搜索相关资料的时候，看到有人在 SO 上问了如何使用的<a href="https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer" target="_blank" rel="noopener">问题</a>，其中有个回答中附上了一个练习项目，项目很小，很适合了解 Finalizers 概念。</p><p>相关逻辑如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">customdeployment:=obj.(*crdv1alpha1.CustomDeployment).DeepCopy()</span><br><span class="line">fmt.Println(<span class="string">"Event.............................."</span>)</span><br><span class="line"><span class="keyword">if</span> customdeployment.DeletionTimestamp != <span class="literal">nil</span>&#123;</span><br><span class="line"><span class="comment">// check if it has finalizer</span></span><br><span class="line"><span class="keyword">if</span> customdeployment.GetFinalizers()!=<span class="literal">nil</span>&#123;</span><br><span class="line">finalizers:=customdeployment.GetFinalizers()</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if first finalizer match with deletepod.crd.emruz.com</span></span><br><span class="line"><span class="keyword">if</span> finalizers[<span class="number">0</span>]==<span class="string">"deletepods.crd.emruz.com"</span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line">_,err:=myutil.PatchCustomDeployment(c.clientset,customdeployment, <span class="function"><span class="keyword">func</span><span class="params">(deployment *crdv1alpha1.CustomDeployment)</span> *<span class="title">crdv1alpha1</span>.<span class="title">CustomDeployment</span></span> &#123;</span><br><span class="line"><span class="comment">// delete pods under this deployment</span></span><br><span class="line">err:=myutil.DeletePods(c.kubeclient,c.podLabel)</span><br><span class="line"><span class="keyword">if</span> err!=<span class="literal">nil</span>&#123;</span><br><span class="line">fmt.Println(<span class="string">"Failed to remove all pods. Reason: "</span>,err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// pods sucessfully removed. remove the finalizer</span></span><br><span class="line">customdeployment.ObjectMeta=myutil.RemoveFinalizer(customdeployment.ObjectMeta)</span><br><span class="line"><span class="keyword">return</span> customdeployment</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">if</span> err!=<span class="literal">nil</span>&#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在开发 Operator 时，pre-delete hook 是一个很常见的需求，目前只发现了 Finalizers 适合实现这个功能，需要好好掌握。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers</a> </li><li><a href="https://book.kubebuilder.io/reference/using-finalizers.html" target="_blank" rel="noopener">https://book.kubebuilder.io/reference/using-finalizers.html</a></li><li><a href="https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer" target="_blank" rel="noopener">https://stackoverflow.com/questions/53057185/kubernetes-crd-finalizer</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近在写 k8s Operator，在看示例的时候看到 controller 都会设置 Finalizers，今天来聊一聊 Finalize
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Leader 选举</title>
    <link href="https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/"/>
    <id>https://zdyxry.github.io/2019/09/12/Kubernetes-实战-Leader-选举/</id>
    <published>2019-09-12T14:05:22.000Z</published>
    <updated>2019-09-12T14:06:52.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 <a href="https://github.com/kubernetes-sigs/cluster-api/releases/tag/v0.2.1" target="_blank" rel="noopener">v0.2.1 版本</a> ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。</p><h2 id="K8S-Leader"><a href="#K8S-Leader" class="headerlink" title="K8S Leader"></a>K8S Leader</h2><p>看过之前 K8S 实战系列的朋友应该记得我写过一篇 <a href="https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/">K8S 高可用部署</a>的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。</p><p>我们这里有一个 3 个master 节点的集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node70 21:01:01 ~]$kubectl get node</span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION</span><br><span class="line">node70   Ready    master   64d   v1.15.0</span><br><span class="line">node71   Ready    master   64d   v1.15.0</span><br><span class="line">node72   Ready    master   64d   v1.15.0</span><br></pre></td></tr></table></figure><p>我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。</p><p>但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？</p><p>k8s 所有功能都是通过 <code>services</code> 对外暴露接口，而 <code>services</code> 对应的是具体的 <code>endpoints</code> ，那么来看下 scheduler 和 controller-manager 的 <code>endpoints</code> 是什么：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node70 21:04:46 ~]$kubectl -n kube-system describe endpoints kube-scheduler</span><br><span class="line">Name:         kube-scheduler</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  control-plane.alpha.kubernetes.io/leader:</span><br><span class="line">                &#123;"holderIdentity":"node70_ed12bf09-7aa3-47d6-9546-97752bb589b5","leaseDurationSeconds":15,"acquireTime":"2019-09-11T05:31:58Z","renewTime"...</span><br><span class="line">Subsets:</span><br><span class="line">Events:  &lt;none&gt;</span><br><span class="line">[root@node70 21:05:25 ~]$kubectl -n kube-system describe endpoints kube-controller-manager</span><br><span class="line">Name:         kube-controller-manager</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  control-plane.alpha.kubernetes.io/leader:</span><br><span class="line">                &#123;"holderIdentity":"node71_c8deeaea-2d66-4459-90ee-65c28563062f","leaseDurationSeconds":15,"acquireTime":"2019-09-12T12:44:15Z","renewTime"...</span><br><span class="line">Subsets:</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason          Age   From                     Message</span><br><span class="line">  ----    ------          ----  ----                     -------</span><br><span class="line">  Normal  LeaderElection  22m   kube-controller-manager  node71_c8deeaea-2d66-4459-90ee-65c28563062f became leader</span><br></pre></td></tr></table></figure><p>  可以看到关键字 <code>control-plane.alpha.kubernetes.io/leader</code> ，这两个组件是通过 leader 选举来从集群中多个节点选择一个执行具体动作，如果我们去看 <code>/etc/kubernetes/manifests/</code> 下的配置文件，会看到这行配置：</p>  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> <span class="string">kube-controller-manager</span></span><br><span class="line"><span class="attr">    tier:</span> <span class="string">control-plane</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-controller-manager</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kube-controller-manager</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--allocate-node-cidrs=true</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--leader-elect=true</span></span><br></pre></td></tr></table></figure><p>通过在 YAML 中添加 <code>leader-elect=true</code> 来决定是否进行选主逻辑。而这个参数也是在执行 <code>kubeadm</code> 部署集群时就自动配置好了，无需手动配置。</p><h2 id="Deployment-Leader"><a href="#Deployment-Leader" class="headerlink" title="Deployment Leader"></a>Deployment Leader</h2><p>我们先来说说 Deployment，Deployment 是从 ReplicaSet 进化来的，主要增加的功能有滚动更新、回滚、扩容所容等，可以说是我们日常使用 K8S 最常见的资源类型了。</p><p>那么当我们通过创建 Deployment 间接创建 ReplicaSet 时，我们有时候并不想所有的 ReplicaSet 中的 Pod 运行统一的逻辑。这时候我们就需要一种方式来选择（通知）某一个 Pod ，来确定这个 Pod 提供特殊功能，其他的 Pod 提供普通功能，也就是跟上述 k8s 实现方式一样，通过Leader 选举完成需求。</p><p>Leader 选举有很多方式，或是代码中内嵌选举逻辑，或者通过第三方服务，但是有两个的特点：</p><ol><li>Leader 在同一时间内是唯一的</li><li>当 Leader 所在 Pod 发生异常时，其他 Pod 要可以随时变为 Leader 。</li></ol><h2 id="Leader-选举实现方式"><a href="#Leader-选举实现方式" class="headerlink" title="Leader 选举实现方式"></a>Leader 选举实现方式</h2><h3 id="代码内嵌选主逻辑"><a href="#代码内嵌选主逻辑" class="headerlink" title="代码内嵌选主逻辑"></a>代码内嵌选主逻辑</h3><p>在 Golang 中，k8s client-go 这个package 针对 Leader 相关功能进行了封装，支持3种锁资源，endpoint，configmap，lease，方便使用。</p><p>代码仓库：<a href="https://github.com/kubernetes/client-go/tree/master/tools/leaderelection" target="_blank" rel="noopener">https://github.com/kubernetes/client-go/tree/master/tools/leaderelection</a></p><p>因为这次主要不是说具体实现，再加上我也没看过代码，这里先掠过。</p><h3 id="SideCar"><a href="#SideCar" class="headerlink" title="SideCar"></a>SideCar</h3><p>相比于代码中内嵌选主逻辑，使用 sidecar 就不用担心跨语言的问题了，使用起来也简单许多，我们现在用的这个项目实现：<a href="https://github.com/kubernetes-retired/contrib/blob/master/election/README.md" target="_blank" rel="noopener">https://github.com/kubernetes-retired/contrib/blob/master/election/README.md</a></p><p>使用方式：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="string">"2019-07-12T07:35:32Z"</span></span><br><span class="line"><span class="attr">  generation:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">yiran-test</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">yiran-test</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">yiran-test</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line">        <span class="string">kubectl.kubernetes.io/restartedAt:</span> <span class="string">"2019-09-09T10:58:51+08:00"</span></span><br><span class="line"><span class="attr">      creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">yiran-test</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--election=elect-yiran-test</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--http=0.0.0.0:4444</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">leader-elector:0.5</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">leader-elector</span></span><br><span class="line"><span class="attr">      - args:</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure><p>在 Deployment 或 DaemonSet 中，添加 sidecar 指定端口，最终会在所有的 Pod 中选择一个 Leader 。<br>在业务代码中，只需要访问端口 4444 ，即可获取当前 Pod 中 Leader 信息，目前是通过 hostname 作为唯一标示的。</p><p>如果看了这个项目的代码，会发现它也是使用的 client go 中的实现，只是增加了一层 http server：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getCurrentLeader</span><span class="params">(electionId, namespace <span class="keyword">string</span>, c client.Interface)</span> <span class="params">(<span class="keyword">string</span>, *api.Endpoints, error)</span></span> &#123;</span><br><span class="line">endpoints, err := c.Endpoints(namespace).Get(electionId)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span>, <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">val, found := endpoints.Annotations[leaderelection.LeaderElectionRecordAnnotationKey]</span><br><span class="line"><span class="keyword">if</span> !found &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span>, endpoints, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">electionRecord := leaderelection.LeaderElectionRecord&#123;&#125;</span><br><span class="line"><span class="keyword">if</span> err := json.Unmarshal([]<span class="keyword">byte</span>(val), &amp;electionRecord); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span>, <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> electionRecord.HolderIdentity, endpoints, err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">flags.Parse(os.Args)</span><br><span class="line">validateFlags()</span><br><span class="line"></span><br><span class="line">kubeClient, err := makeClient()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Fatalf(<span class="string">"error connecting to the client: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fn := <span class="function"><span class="keyword">func</span><span class="params">(str <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">leader.Name = str</span><br><span class="line">fmt.Printf(<span class="string">"%s is the leader\n"</span>, leader.Name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e, err := election.NewElection(*name, *id, *namespace, *ttl, fn, kubeClient)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Fatalf(<span class="string">"failed to create election: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">go</span> election.RunElection(e)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(*addr) &gt; <span class="number">0</span> &#123;</span><br><span class="line">http.HandleFunc(<span class="string">"/"</span>, webHandler)</span><br><span class="line">http.ListenAndServe(*addr, <span class="literal">nil</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="是否开启选举参数"><a href="#是否开启选举参数" class="headerlink" title="是否开启选举参数"></a>是否开启选举参数</h2><p>那么我们说了这么多，在实际使用中是否应该开启选举参数呢？<br>这里要说一下我在文章开头提到的有意思的事情，我们来看下cluster-api 的 YAML 文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--enable-leader-election</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">/manager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">us.gcr.io/k8s-artifacts-prod/cluster-api/cluster-api-controller:v0.2.1</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">manager</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure><p>创建的资源类型是 Deployment，replicas 设置为1，传递了参数 <code>--enable-leader-election</code>，当时我觉得有些奇怪，replicas是1 啊，也就是说一共只有1个 Pod，为啥还要开启 Leader 选举逻辑呢？如果说是为了之后的扩容那可以理解，但是在这份配置文件里，应该完全没必要。把这段配置发给 <a href="https://liqiang.io/" target="_blank" rel="noopener">liqiang同学</a> 看，他也觉得有些怪怪的。</p><p>秉着不懂就问的精神，我去 Slack #cluster-api 中提出了我的疑问，得到了以下回复，我格式化一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@cha： </span><br><span class="line">it&apos;s mostly set for best practice. If you scale up the system will still work. If you don&apos;t enable leader election and scale up you will have race conditions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@cha：</span><br><span class="line">but you&apos;re right. If you&apos;re running a single manager you don&apos;t need to enable leader election</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@jdetiber:</span><br><span class="line">With a Deployment you do, because on rolling out a change it will spin up the new RS in parallel with the other so there will be 2 pods running at the same time for a short while</span><br></pre></td></tr></table></figure><p>其中 @cha 的回复跟我预想的差不多，主要处于之后的扩容考虑，需要防止竞争情况出现。但是 @jdetiber 提到了一点很关键，当 Deployment 进行滚动升级的时候，哪怕设置了 Replicas 为1，也会存在短时间同时存在 2 个 Pod 的情况，那么肯定会导致我们的代码逻辑错误，很重要（感叹号）。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实今天这篇文章主要是想记录下上面这段话的，自己在平时使用 k8s 过程中，貌似也仅仅是知道这些功能并使用，真正要开发部分功能时，考虑的边界条件太少了。虽然知道 Deployment 滚动升级会有同时存在 2 个 Pod 的情况，却完全没有想到需要配置 Leader 选举参数来保证代码正确运行，涨知识了。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 &lt;a href=&quot;https://github.c
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>httprunner 源码阅读</title>
    <link href="https://zdyxry.github.io/2019/09/06/httprunner-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>https://zdyxry.github.io/2019/09/06/httprunner-源码阅读/</id>
    <published>2019-09-06T11:50:27.000Z</published>
    <updated>2019-09-07T10:45:58.821Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近工作上每天疲于应付各种事情，周末实在不想继续做工作相关的事情，想起一直想了解的自动化测试框架 <a href="https://github.com/httprunner/httprunner" target="_blank" rel="noopener">httprunner</a>，就阅读下。之前一直有关注作者 <a href="https://debugtalk.com/" target="_blank" rel="noopener">debugtalk</a> 的博客，收获很多。</p><p>随着公司的发展，自己也做过很多的工作，其中就包含测试，但是自己当时大部分都是手工测试，虽然会针对其中的部分进行代码编写，但是不成体系。虽然后来就没有继续负责测试工作了，但是对于测试还是很感兴趣，平时开发过程中，最多也就是使用 unittest 或 pytest 来编写单测，这次通过阅读 httprunner 代码来感受下测试框架。</p><p>P.S. 在使用及了解 httprunner 之前，最好先了解下 unittest。</p><h2 id="httprunner"><a href="#httprunner" class="headerlink" title="httprunner"></a>httprunner</h2><blockquote><p>HttpRunner 是一款面向 HTTP(S) 协议的通用测试框架，只需编写维护一份 YAML/JSON 脚本，即可实现自动化测试、性能测试、线上监控、持续集成等多种测试需求。</p></blockquote><p>一句话总结就是 api 自动化测试，其中用到了以下的开源项目：</p><ol><li>requests</li><li>locust</li><li>unittest</li><li>…</li></ol><p>requests 和 unittest 可以说是 python 开发者用的比较多的两个项目。locust 是一个 api 压力测试，这个我们公司也有用到。</p><p>介绍完了项目，我们来跟着官方文档了解运行流程。</p><h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><p>文档中的 <a href="https://cn.httprunner.org/quickstart/" target="_blank" rel="noopener">快速上手</a> 章节与章节名称很配，真心是 <strong>快速上手</strong> ，通过一个又一个的示例来了解具体的功能使用，循序渐进，简直完美。不过又一点不好的地方是 demo 示例的代码不再 httprunner 中，而是在 docs 项目中，使用起来不是很方便，如果有 Dockerfile 来支撑，就更好了。</p><p>在 httprunner 项目中，项目的包管理是通过 <strong>poetry</strong> 进行的，比 setuptools 要清晰很多。</p><p>首先来看命令，httprunner 随着项目的演进，支持的命令行有 4个，其中 3 个是重复的，1个是压力测试：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[tool.poetry.scripts]</span></span><br><span class="line"><span class="string">hrun</span> <span class="string">=</span> <span class="string">"httprunner.cli:main_hrun"</span></span><br><span class="line"><span class="string">ate</span> <span class="string">=</span> <span class="string">"httprunner.cli:main_hrun"</span></span><br><span class="line"><span class="string">httprunner</span> <span class="string">=</span> <span class="string">"httprunner.cli:main_hrun"</span></span><br><span class="line"><span class="string">locusts</span> <span class="string">=</span> <span class="string">"httprunner.cli:main_locust"</span></span><br></pre></td></tr></table></figure><p>我们以 <code>hrun</code> 为例，从 argparse 接收到的参数均作为参数传递给 HttpRunner，然后执行实例化 HttpRunner，通过 <code>HttpRunner.run</code> 进行用例的执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">runner = HttpRunner(</span><br><span class="line">    failfast=args.failfast,</span><br><span class="line">    save_tests=args.save_tests,</span><br><span class="line">    report_template=args.report_template,</span><br><span class="line">    report_dir=args.report_dir,</span><br><span class="line">    log_level=args.log_level,</span><br><span class="line">    log_file=args.log_file</span><br><span class="line">)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> path <span class="keyword">in</span> args.testcase_paths:</span><br><span class="line">        runner.run(path, dot_env_path=args.dot_env_path)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    color_print(<span class="string">"!!!!!!!!!! exception stage: &#123;&#125; !!!!!!!!!!"</span>.format(runner.exception_stage), <span class="string">"YELLOW"</span>)</span><br><span class="line">    <span class="keyword">raise</span></span><br></pre></td></tr></table></figure><p>我们来看下 <code>runner.run</code> 做了啥：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, path_or_tests, dot_env_path=None, mapping=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> validator.is_testcase_path(path_or_tests):</span><br><span class="line">        <span class="keyword">return</span> self.run_path(path_or_tests, dot_env_path, mapping)</span><br><span class="line">    <span class="keyword">elif</span> validator.is_testcases(path_or_tests):</span><br><span class="line">        <span class="keyword">return</span> self.run_tests(path_or_tests)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> exceptions.ParamsError(<span class="string">"Invalid testcase path or testcases: &#123;&#125;"</span>.format(path_or_tests))</span><br></pre></td></tr></table></figure><p>这里只是进行了一层判断，判断传入的参数是一个路径还是具体的用例，因为我们主要是了解整个执行流程，所以我们直接假设传入的是测试用例，来看看 <code>self.run_tests</code> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_tests</span><span class="params">(self, tests_mapping)</span>:</span></span><br><span class="line">    <span class="string">""" run testcase/testsuite data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    project_mapping = tests_mapping.get(<span class="string">"project_mapping"</span>, &#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> self.save_tests:</span><br><span class="line">        utils.dump_logs(tests_mapping, project_mapping, <span class="string">"loaded"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse tests</span></span><br><span class="line">    self.exception_stage = <span class="string">"parse tests"</span></span><br><span class="line">    parsed_testcases = parser.parse_tests(tests_mapping)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.save_tests:</span><br><span class="line">        utils.dump_logs(parsed_testcases, project_mapping, <span class="string">"parsed"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># add tests to test suite</span></span><br><span class="line">    self.exception_stage = <span class="string">"add tests to test suite"</span></span><br><span class="line">    test_suite = self._add_tests(parsed_testcases)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># run test suite</span></span><br><span class="line">    self.exception_stage = <span class="string">"run test suite"</span></span><br><span class="line">    results = self._run_suite(test_suite)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># aggregate results</span></span><br><span class="line">    self.exception_stage = <span class="string">"aggregate results"</span></span><br><span class="line">    self._summary = self._aggregate(results)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate html report</span></span><br><span class="line">    self.exception_stage = <span class="string">"generate html report"</span></span><br><span class="line">    report.stringify_summary(self._summary)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.save_tests:</span><br><span class="line">        utils.dump_logs(self._summary, project_mapping, <span class="string">"summary"</span>)</span><br><span class="line"></span><br><span class="line">    report_path = report.render_html_report(</span><br><span class="line">        self._summary,</span><br><span class="line">        self.report_template,</span><br><span class="line">        self.report_dir</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> report_path</span><br></pre></td></tr></table></figure><p>先来看下具体的执行函数 <code>self._run_suite</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_run_suite</span><span class="params">(self, test_suite)</span>:</span></span><br><span class="line">    tests_results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> testcase <span class="keyword">in</span> test_suite:</span><br><span class="line">        testcase_name = testcase.config.get(<span class="string">"name"</span>)</span><br><span class="line">        logger.log_info(<span class="string">"Start to run testcase: &#123;&#125;"</span>.format(testcase_name))</span><br><span class="line"></span><br><span class="line">        result = self.unittest_runner.run(testcase)</span><br><span class="line">        tests_results.append((testcase, result))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tests_results</span><br></pre></td></tr></table></figure><p>真正执行用例执行的是 <code>self.unittest_runner</code> ，而 <code>self.unittest_runner = unittest.TextTestRunner(**kwargs)</code>，也就是说这里的 <code>testcase</code> 其实是 <code>unittest.suite.TestSuite</code> 或者 <code>unittest.case.TestCase</code>。</p><p>这里跟预想中的出现了偏差，我以为这里是自己实现的测试执行及结果比对的方法，没想到调用的是 <code>unittest</code>，那么怎么从一个 testcase 变为 <code>unittest</code> 可执行对象的，应该就是在 <code>self._add_tests</code> 中实现的了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_tests</span><span class="params">(self, testcases)</span>:</span></span><br><span class="line">    <span class="string">""" initialize testcase with Runner() and add to test suite.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        testcases (list): testcases list.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        unittest.TestSuite()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure><p>这个函数略长，这里分开说，从注释中可以知道，最终 unittest 执行的是 <code>unittest.TestSuite</code> ，看下具体的逻辑：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">test_suite = unittest.TestSuite() <span class="comment">#定义 test_suite 返回值</span></span><br><span class="line"><span class="keyword">for</span> testcase <span class="keyword">in</span> testcases:</span><br><span class="line">    config = testcase.get(<span class="string">"config"</span>, &#123;&#125;)</span><br><span class="line">    test_runner = runner.Runner(config) <span class="comment"># 实例化 Runner，后续主要执行逻辑都在 Runner 中</span></span><br><span class="line">    TestSequense = type(<span class="string">'TestSequense'</span>, (unittest.TestCase,), &#123;&#125;) <span class="comment"># 通过 type 声明一个 `unittest.TestCase` 的子类</span></span><br><span class="line"></span><br><span class="line">    tests = testcase.get(<span class="string">"teststeps"</span>, [])</span><br><span class="line">    <span class="keyword">for</span> index, test_dict <span class="keyword">in</span> enumerate(tests):</span><br><span class="line">        times = test_dict.get(<span class="string">"times"</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            times = int(times)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">raise</span> exceptions.ParamsError(</span><br><span class="line">                <span class="string">"times should be digit, given: &#123;&#125;"</span>.format(times))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> times_index <span class="keyword">in</span> range(times):</span><br><span class="line">            <span class="comment"># suppose one testcase should not have more than 9999 steps,</span></span><br><span class="line">            <span class="comment"># and one step should not run more than 999 times.</span></span><br><span class="line">            test_method_name = <span class="string">'test_&#123;:04&#125;_&#123;:03&#125;'</span>.format(index, times_index)</span><br><span class="line">            test_method = _add_test(test_runner, test_dict) </span><br><span class="line">            setattr(TestSequense, test_method_name, test_method) <span class="comment"># 将上面的 test_method 定义为 TestSequense 属性</span></span><br><span class="line"></span><br><span class="line">    loaded_testcase = self.test_loader.loadTestsFromTestCase(TestSequense) <span class="comment"># 利用 TestLoader 来找到符合条件的方法，默认 TestLoader 寻找前缀为 `test` </span></span><br><span class="line">    setattr(loaded_testcase, <span class="string">"config"</span>, config)</span><br><span class="line">    setattr(loaded_testcase, <span class="string">"teststeps"</span>, tests)</span><br><span class="line">    setattr(loaded_testcase, <span class="string">"runner"</span>, test_runner)</span><br><span class="line">    test_suite.addTest(loaded_testcase) <span class="comment"># 将 `TestLoader.loadTestsFromTestCase` 找到的 testcase 添加到 `test_suite` 中，并最终返回</span></span><br></pre></td></tr></table></figure><p>看完上面这里，有一个比较重要的就是 <code>_add_test</code> 做了啥：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_test</span><span class="params">(test_runner, test_dict)</span>:</span></span><br><span class="line">    <span class="string">""" add test to testcase.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            test_runner.run_test(test_dict)</span><br><span class="line">        <span class="keyword">except</span> exceptions.MyBaseFailure <span class="keyword">as</span> ex:</span><br><span class="line">            self.fail(str(ex))</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            self.meta_datas = test_runner.meta_datas</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> test</span><br></pre></td></tr></table></figure><p>先忽略其他代码，可以看到 <code>_add_test</code> 最终返回的是一个函数，这个函数名称是 <code>test</code> ，结合上面的 <code>self.test_loader.loadTestsFromTestCase</code> ，可以知道这个函数就是最终执行的方法。</p><p>看完这里，回到 <code>run_tests</code> 中，在 <code>self._run_suite</code> 执行完之后，就进行结果汇总、生成报告了，那么具体的请求是怎么发送出去的，结果又是怎么校验的？猜测是在 <code>runner.Runner</code> 中实现的，接着来看。</p><h2 id="用例请求及校验"><a href="#用例请求及校验" class="headerlink" title="用例请求及校验"></a>用例请求及校验</h2><p>在上面提到，最终 <code>_add_test</code> 返回的函数中，执行的内容只有一个，就是 <code>test_runner.run_test</code> ，那么我们来看看 <code>run_test</code> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_test</span><span class="params">(self, test_dict)</span>:</span></span><br><span class="line">        <span class="string">""" run single teststep of testcase.</span></span><br><span class="line"><span class="string">            test_dict may be in 3 types.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            test_dict (dict):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                # teststep</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                    "name": "teststep description",</span></span><br><span class="line"><span class="string">                    "variables": [],        # optional</span></span><br><span class="line"><span class="string">                    "request": &#123;</span></span><br><span class="line"><span class="string">                        "url": "http://127.0.0.1:5000/api/users/1000",</span></span><br><span class="line"><span class="string">                        "method": "GET"</span></span><br><span class="line"><span class="string">                    &#125;</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                # nested testcase</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                    "config": &#123;...&#125;,</span></span><br><span class="line"><span class="string">                    "teststeps": [</span></span><br><span class="line"><span class="string">                        &#123;...&#125;,</span></span><br><span class="line"><span class="string">                        &#123;...&#125;</span></span><br><span class="line"><span class="string">                    ]</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                # TODO: function</span></span><br><span class="line"><span class="string">                &#123;</span></span><br><span class="line"><span class="string">                    "name": "exec function",</span></span><br><span class="line"><span class="string">                    "function": "$&#123;func()&#125;"</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.meta_datas = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"teststeps"</span> <span class="keyword">in</span> test_dict:</span><br><span class="line">            <span class="comment"># nested testcase</span></span><br><span class="line">            test_dict.setdefault(<span class="string">"config"</span>, &#123;&#125;).setdefault(<span class="string">"variables"</span>, &#123;&#125;)</span><br><span class="line">            test_dict[<span class="string">"config"</span>][<span class="string">"variables"</span>].update(</span><br><span class="line">                self.session_context.session_variables_mapping)</span><br><span class="line">            self._run_testcase(test_dict)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># api</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self._run_test(test_dict)</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="comment"># log exception request_type and name for locust stat</span></span><br><span class="line">                self.exception_request_type = test_dict[<span class="string">"request"</span>][<span class="string">"method"</span>]</span><br><span class="line">                self.exception_name = test_dict.get(<span class="string">"name"</span>)</span><br><span class="line">                <span class="keyword">raise</span></span><br><span class="line">            <span class="keyword">finally</span>:</span><br><span class="line">                self.meta_datas = self.__get_test_data()</span><br></pre></td></tr></table></figure><p>这里的注释虽然很长，但是很清晰的告诉我们这个函数做了什么，他允许传递的参数是嵌套的，所以这里先做了层判断，我们以最简单的来假设，直接看 <code>self._run_test</code> ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_run_test</span><span class="params">(self, test_dict)</span>:</span></span><br><span class="line">        <span class="comment"># clear meta data first to ensure independence for each test</span></span><br><span class="line">        self.__clear_test_data() <span class="comment"># 清空测试结果及 session 信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check skip</span></span><br><span class="line">        self._handle_skip_feature(test_dict) <span class="comment"># 根据测试用例关键字执行相应跳过逻辑</span></span><br><span class="line"></span><br><span class="line">        ... <span class="comment"># N 多准备工作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># setup hooks</span></span><br><span class="line">        setup_hooks = test_dict.get(<span class="string">"setup_hooks"</span>, [])</span><br><span class="line">        <span class="keyword">if</span> setup_hooks:</span><br><span class="line">            self.do_hook_actions(setup_hooks, <span class="string">"setup"</span>) <span class="comment"># 与大部分框架一样，支持 pre hook</span></span><br><span class="line"></span><br><span class="line">        ... <span class="comment"># N 多准备工作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># request</span></span><br><span class="line">        resp = self.http_client_session.request( <span class="comment"># 根据用例发出请求</span></span><br><span class="line">            method,</span><br><span class="line">            parsed_url,</span><br><span class="line">            name=(group_name <span class="keyword">or</span> test_name),</span><br><span class="line">            **parsed_test_request</span><br><span class="line">        )</span><br><span class="line">        resp_obj = response.ResponseObject(resp)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># teardown hooks</span></span><br><span class="line">        teardown_hooks = test_dict.get(<span class="string">"teardown_hooks"</span>, []) <span class="comment"># 支持 post hook</span></span><br><span class="line">        <span class="keyword">if</span> teardown_hooks:</span><br><span class="line">            self.session_context.update_test_variables(<span class="string">"response"</span>, resp_obj)</span><br><span class="line">            self.do_hook_actions(teardown_hooks, <span class="string">"teardown"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract</span></span><br><span class="line">        extractors = test_dict.get(<span class="string">"extract"</span>, &#123;&#125;)</span><br><span class="line">        extracted_variables_mapping = resp_obj.extract_response(extractors)</span><br><span class="line">        self.session_context.update_session_variables(extracted_variables_mapping)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate</span></span><br><span class="line">        validators = test_dict.get(<span class="string">"validate"</span>) <span class="keyword">or</span> test_dict.get(<span class="string">"validators"</span>) <span class="keyword">or</span> []</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.session_context.validate(validators, resp_obj) <span class="comment"># 结果校验</span></span><br><span class="line">        <span class="keyword">except</span> (exceptions.ParamsError, exceptions.ValidationFailure, exceptions.ExtractFailure):</span><br><span class="line">            err_msg = <span class="string">"&#123;&#125; DETAILED REQUEST &amp; RESPONSE &#123;&#125;\n"</span>.format(<span class="string">"*"</span> * <span class="number">32</span>, <span class="string">"*"</span> * <span class="number">32</span>)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            self.validation_results = self.session_context.validation_results</span><br></pre></td></tr></table></figure><p>具体的 http 请求时通过 <code>self.http_client_session.request</code> 发送的，通过 <code>self.session_context.validate</code> 进行结果校验，来看下具体的校验方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> validator <span class="keyword">in</span> validators:</span><br><span class="line">    <span class="comment"># validator should be LazyFunction object</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(validator, parser.LazyFunction):</span><br><span class="line">        <span class="keyword">raise</span> exceptions.ValidationFailure(</span><br><span class="line">            <span class="string">"validator should be parsed first: &#123;&#125;"</span>.format(validators))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># evaluate validator args with context variable mapping.</span></span><br><span class="line">    validator_args = validator.get_args()</span><br><span class="line">    check_item, expect_item = validator_args</span><br><span class="line">    check_value = self.__eval_validator_check( <span class="comment"># 准备参数</span></span><br><span class="line">        check_item,</span><br><span class="line">        resp_obj</span><br><span class="line">    )</span><br><span class="line">    expect_value = self.__eval_validator_expect(expect_item) <span class="comment"># 准备参数</span></span><br><span class="line">    validator.update_args([check_value, expect_value])</span><br><span class="line"></span><br><span class="line">    comparator = validator.func_name</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        validator.to_value(self.test_variables_mapping) <span class="comment"># 执行校验</span></span><br><span class="line">        validator_dict[<span class="string">"check_result"</span>] = <span class="string">"pass"</span></span><br><span class="line">        validate_msg += <span class="string">"\t==&gt; pass"</span></span><br><span class="line">        logger.log_debug(validate_msg)</span><br><span class="line">    <span class="keyword">except</span> (AssertionError, TypeError):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    self.validation_results.append(validator_dict)</span><br></pre></td></tr></table></figure><p>前面都是在准备参数，最终执行完 <code>validator.to_value</code> 如果没有异常，就直接标记 <code>check_result</code> 为 <code>pass</code> 了，那么我们需要看看这里的 <code>to_value</code> 做了什么，感觉这个方法名称不太好，跟实际做的事情感觉不匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_value</span><span class="params">(self, variables_mapping=None)</span>:</span></span><br><span class="line">    <span class="string">""" parse lazy data with evaluated variables mapping.</span></span><br><span class="line"><span class="string">        Notice: variables_mapping should not contain any variable or function.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    variables_mapping = variables_mapping <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">    args = parse_lazy_data(self._args, variables_mapping)</span><br><span class="line">    kwargs = parse_lazy_data(self._kwargs, variables_mapping)</span><br><span class="line">    self.cache_key = self.__prepare_cache_key(args, kwargs)</span><br><span class="line">    <span class="keyword">return</span> self._func(*args, **kwargs)</span><br></pre></td></tr></table></figure><p>看到这里可能会感觉迷糊下，咦，为什么这里执行了 <code>self._func</code> ，<code>self._func</code> 是什么时候定义的，它做了啥？<br>我们在编写测试用例的时候 <code>validate</code> 通常是<code>eq</code>,<code>gt</code>,<code>lt</code> 等字段，如果把他们直接当作函数执行肯定不行的，那么一定存在一个映射关系将这些关键字转换为对应的函数。</p><p>看到这里其实我们漏掉了一个比较重要的步骤，就是 <code>run_tests</code> 中的 <code>parse_tests</code> 。其实很多时候默认的配置是不足以支撑我们真正逻辑的运行的，往往我们需要根据已有配置来扩充信息，达到满足执行要求的状态，接下来我们来看下测试用例准备工作。</p><h2 id="用例解析"><a href="#用例解析" class="headerlink" title="用例解析"></a>用例解析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_tests</span><span class="params">(tests_mapping)</span>:</span></span><br><span class="line">    </span><br><span class="line">    project_mapping = tests_mapping.get(<span class="string">"project_mapping"</span>, &#123;&#125;)</span><br><span class="line">    testcases = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> test_type <span class="keyword">in</span> tests_mapping:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_type == <span class="string">"testsuites"</span>:</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> test_type == <span class="string">"testcases"</span>:</span><br><span class="line">            <span class="keyword">for</span> testcase <span class="keyword">in</span> tests_mapping[<span class="string">"testcases"</span>]:</span><br><span class="line">                parsed_testcase = _parse_testcase(testcase, project_mapping)</span><br><span class="line">                testcases.append(parsed_testcase)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> test_type == <span class="string">"apis"</span>:</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> testcases</span><br></pre></td></tr></table></figure><p>这里同样对 <code>test_type</code> 进行了判断，如果不是 <code>testcases</code> ，则会进行处理，我们还是假设最简单的 <code>testcases</code>，可以看到调用了 <code>_parse_testcase</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_testcase</span><span class="params">(testcase, project_mapping, session_variables_set=None)</span>:</span></span><br><span class="line">    testcase.setdefault(<span class="string">"config"</span>, &#123;&#125;)</span><br><span class="line">    prepared_config = __prepare_config(</span><br><span class="line">        testcase[<span class="string">"config"</span>],</span><br><span class="line">        project_mapping,</span><br><span class="line">        session_variables_set</span><br><span class="line">    )</span><br><span class="line">    prepared_testcase_tests = __prepare_testcase_tests(</span><br><span class="line">        testcase[<span class="string">"teststeps"</span>],</span><br><span class="line">        prepared_config,</span><br><span class="line">        project_mapping,</span><br><span class="line">        session_variables_set</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">"config"</span>: prepared_config,</span><br><span class="line">        <span class="string">"teststeps"</span>: prepared_testcase_tests</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>从官方文档可以知道，每个 testcase 中的 config 其实是全局配置，所以这里先解析了 config，然后将其作为参数传递给了 <code>__prepare_testcase_tests</code> 用来准备对应的 testcase，最终返回准备好的测试用例及配置，那么跟着看下 <code>__prepare_testcase_tests</code>，这个函数很长很长，先忽略其他部分，先来看这段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unify validators' format</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">"validate"</span> <span class="keyword">in</span> test_dict:</span><br><span class="line">    ref_raw_validators = test_dict.pop(<span class="string">"validate"</span>, [])</span><br><span class="line">    test_dict[<span class="string">"validate"</span>] = [</span><br><span class="line">        validator.uniform_validator(_validator)</span><br><span class="line">        <span class="keyword">for</span> _validator <span class="keyword">in</span> ref_raw_validators</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure><p>如果存在 <code>validate</code> ，那么将其转换为 <code>validator.uniform_validator</code> 组成的列表，彷佛抓到了什么，我们来看下 <code>validator.uniform_validator</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniform_validator</span><span class="params">(validator)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># uniform comparator, e.g. lt =&gt; less_than, eq =&gt; equals</span></span><br><span class="line">    comparator = get_uniform_comparator(comparator)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">"check"</span>: check_item,</span><br><span class="line">        <span class="string">"expect"</span>: expect_value,</span><br><span class="line">        <span class="string">"comparator"</span>: comparator</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>最终返回的是一个字典，其中的 <code>comparator</code> 是 <code>get_uniform_comparator</code> 返回值，继续看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_uniform_comparator</span><span class="params">(comparator)</span>:</span></span><br><span class="line">    <span class="string">""" convert comparator alias to uniform name</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> comparator <span class="keyword">in</span> [<span class="string">"eq"</span>, <span class="string">"equals"</span>, <span class="string">"=="</span>, <span class="string">"is"</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"equals"</span></span><br><span class="line">    <span class="keyword">elif</span> comparator <span class="keyword">in</span> [<span class="string">"lt"</span>, <span class="string">"less_than"</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"less_than"</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> comparator</span><br></pre></td></tr></table></figure><p>终于找到了校验关键字的映射关键字，那么我们继续回头看 <code>__prepare_testcase_tests</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert validators to lazy function</span></span><br><span class="line">validators = test_dict.pop(<span class="string">"validate"</span>, [])</span><br><span class="line">prepared_validators = []</span><br><span class="line"><span class="keyword">for</span> _validator <span class="keyword">in</span> validators:</span><br><span class="line">    function_meta = &#123;</span><br><span class="line">        <span class="string">"func_name"</span>: _validator[<span class="string">"comparator"</span>],</span><br><span class="line">        <span class="string">"args"</span>: [</span><br><span class="line">            _validator[<span class="string">"check"</span>],</span><br><span class="line">            _validator[<span class="string">"expect"</span>]</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"kwargs"</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    prepared_validators.append(</span><br><span class="line">        LazyFunction(</span><br><span class="line">            function_meta,</span><br><span class="line">            functions,</span><br><span class="line">            teststep_variables_set</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">test_dict[<span class="string">"validate"</span>] = prepared_validators</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert variables and functions to lazy object.</span></span><br><span class="line"><span class="comment"># raises VariableNotFound if undefined variable exists in test_dict</span></span><br><span class="line">prepared_test_dict = prepare_lazy_data(</span><br><span class="line">    test_dict,</span><br><span class="line">    functions,</span><br><span class="line">    teststep_variables_set</span><br><span class="line">)</span><br><span class="line">prepared_testcase_tests.append(prepared_test_dict)</span><br></pre></td></tr></table></figure><p>在这里针对刚刚得到的关键字对应函数名称又进行了一次封装，现在的 <code>test_dict[&quot;validate&quot;]</code> 就是一个 <code>LazyFunction</code> 组成的列表了，回想最开始我们为啥要看用例解析这部分的代码，因为我们发现在函数校验那里没找到真正执行的函数，我们来看下 <code>LazyFunction</code> 的构造函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, function_meta, functions_mapping=None, check_variables_set=None)</span>:</span></span><br><span class="line">    <span class="string">""" init LazyFunction object with function_meta</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        function_meta (dict): function name, args and kwargs.</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                "func_name": "func",</span></span><br><span class="line"><span class="string">                "args": [1, 2]</span></span><br><span class="line"><span class="string">                "kwargs": &#123;"a": 3, "b": 4&#125;</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.functions_mapping = functions_mapping <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">    self.check_variables_set = check_variables_set <span class="keyword">or</span> set()</span><br><span class="line">    self.cache_key = <span class="keyword">None</span></span><br><span class="line">    self.__parse(function_meta)</span><br></pre></td></tr></table></figure><p>重点在最后一个行，<code>self.__parse(function_meta)</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__parse</span><span class="params">(self, function_meta)</span>:</span></span><br><span class="line">    <span class="string">""" init func as lazy functon instance</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        function_meta (dict): function meta including name, args and kwargs</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self._func = get_mapping_function(</span><br><span class="line">        function_meta[<span class="string">"func_name"</span>],</span><br><span class="line">        self.functions_mapping</span><br><span class="line">    )</span><br><span class="line">    self.func_name = self._func.__name__</span><br><span class="line">    self._args = prepare_lazy_data(</span><br><span class="line">        function_meta.get(<span class="string">"args"</span>, []),</span><br><span class="line">        self.functions_mapping,</span><br><span class="line">        self.check_variables_set</span><br><span class="line">    )</span><br><span class="line">    self._kwargs = prepare_lazy_data(</span><br><span class="line">        function_meta.get(<span class="string">"kwargs"</span>, &#123;&#125;),</span><br><span class="line">        self.functions_mapping,</span><br><span class="line">        self.check_variables_set</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>在这里找到了我们一直想找的 <code>self._func</code> 定义，看函数名是通过 <code>func_name</code> 从一个 map 中返回真正的函数，来看看是不是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mapping_function</span><span class="params">(function_name, functions_mapping)</span>:</span></span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># check if HttpRunner builtin functions</span></span><br><span class="line">        <span class="keyword">from</span> httprunner <span class="keyword">import</span> loader</span><br><span class="line">        built_in_functions = loader.load_builtin_functions()</span><br><span class="line">        <span class="keyword">return</span> built_in_functions[function_name]</span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> httprunner <span class="keyword">import</span> built_in, exceptions, logger, parser, utils, validator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_builtin_functions</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" load built_in module functions</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> load_module_functions(built_in)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_module_functions</span><span class="params">(module)</span>:</span></span><br><span class="line">    module_functions = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, item <span class="keyword">in</span> vars(module).items():</span><br><span class="line">        <span class="keyword">if</span> validator.is_function(item):</span><br><span class="line">            module_functions[name] = item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> module_functions</span><br></pre></td></tr></table></figure><p>一路追下来，具体的实现应该在 <code>built_in.py</code> 没跑：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equals</span><span class="params">(check_value, expect_value)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> check_value == expect_value</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">less_than</span><span class="params">(check_value, expect_value)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> check_value &lt; expect_value</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Ok，到这里我们终于了解了用例的解析工作做了什么，同时也解答了我们上面遗留下来的疑问，结果校验执行的是什么。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在公司内部同样有着测试框架，是基于 Robot Framework 进行开发的，但是我被 Robot 的代码量劝退了。。。httprunner 项目整体来看代码量不大，很适合作为初步了解自动化测试的框架来阅读。</p><p>根据作者自述，项目起源于大疆内部的测试需求，之后转为开源项目，不知道作者现在是否是因为离开了大疆还是其他原因，现在 httprunner 的社区感觉很差，明明是有公司使用的，但是并没有积极参与，作者一个人承担了几乎100% 的代码开发任务，感觉是一个不健康的社区。</p><p>希望自己有机会的话也参与进去，不希望这样一个项目 <code>死</code> 掉。</p><h2 id="P-S"><a href="#P-S" class="headerlink" title="P.S."></a>P.S.</h2><p>其实这篇博客应该是在周五晚上完成的，但是写着写着发现自己对于项目中的整体流程无法理顺，今天又用了大半天的时间重新读了代码，一点一点记录下来。</p><p>嗯，写博客的好处之一。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近工作上每天疲于应付各种事情，周末实在不想继续做工作相关的事情，想起一直想了解的自动化测试框架 &lt;a href=&quot;https://gith
      
    
    </summary>
    
    
      <category term="Python" scheme="https://zdyxry.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux 下常用故障模拟方法</title>
    <link href="https://zdyxry.github.io/2019/08/31/Linux-%E4%B8%8B%E5%B8%B8%E7%94%A8%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E6%96%B9%E5%BC%8F/"/>
    <id>https://zdyxry.github.io/2019/08/31/Linux-下常用故障模拟方式/</id>
    <published>2019-08-30T23:47:00.000Z</published>
    <updated>2019-08-31T03:22:31.320Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在日常开发时，有时候需要保证自己代码的健壮性，需要模拟各种故障测试，比如：磁盘、网络、端口等，今天来汇总一下平时使用最多的几种故障模拟方法</p><h2 id="磁盘"><a href="#磁盘" class="headerlink" title="磁盘"></a>磁盘</h2><h3 id="插入拔出"><a href="#插入拔出" class="headerlink" title="插入拔出"></a>插入拔出</h3><p>服务器的存储控制器如果是直通模式，那么在 OS 中能够直接获取到磁盘插入与拔出事件，有时候我们需要检测到相应的事件来自动化的做某些动作，具体的实现方式见之前的文章 <a href="https://zdyxry.github.io/2019/08/02/Linux-%E4%B8%8B%E7%A3%81%E7%9B%98%E8%AE%BE%E5%A4%87%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E6%96%B9%E5%BC%8F/">Linux 下磁盘设备自动发现方式</a> 。</p><p>那么我们写完了代码想要测试，不想去机房物理操作，怎么模拟呢？</p><h4 id="Hypervisor"><a href="#Hypervisor" class="headerlink" title="Hypervisor"></a>Hypervisor</h4><p>如果你的代码部署的机器是一台虚拟机，那么在 Hypervisor 层面一般都会有对应的接口来完成相应的操作。</p><p>比如 Vsphere ESXi 中可以直接编辑虚拟机，在磁盘选项中有一个“移除”按钮，可以直接移除磁盘：</p><img src="/2019/08/31/Linux-下常用故障模拟方式/vsphere.png" title="vsphere"><p>再比如 KVM 下，可以通过 Libvirt 接口来 <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-managing_guest_virtual_machines_with_virsh-attaching_and_updating_a_device_with_virsh" target="_blank" rel="noopener">detach 磁盘</a>。</p><p>当然对于插入动作，Hypervisor 也会提供对应的功能。</p><h4 id="物理服务器"><a href="#物理服务器" class="headerlink" title="物理服务器"></a>物理服务器</h4><p>如果 OS 不是在 Hypervisor 上，而是直接安装在了物理服务器上，我们怎么做呢？</p><p>通常我们服务器上的磁盘都是 SCSI 设备，会实现完整的 SCSI（接口），可以通过修改相应设备的标置文件来达到目的。</p><p>示例：<br>节点存在设备 <code>/dev/sda</code> ，修改标置文件，在系统中会发现磁盘已经被移除了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   10G  0 disk</span><br><span class="line">sr0              11:0    1  4.3G  0 rom  /run/media/root/CentOS 7 x86_64</span><br><span class="line">vda             252:0    0  100G  0 disk</span><br><span class="line">├─vda1          252:1    0    1G  0 part /boot</span><br><span class="line">└─vda2          252:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0  3.9G  0 lvm</span><br><span class="line">  └─centos-home 253:2    0 45.1G  0 lvm  /home</span><br><span class="line">[root@yiran ~]<span class="comment"># ls /sys/block/sda/device/delete</span></span><br><span class="line">/sys/block/sda/device/delete</span><br><span class="line">[root@yiran ~]<span class="comment"># echo 1 &gt; /sys/block/sda/device/delete</span></span><br><span class="line">[root@yiran ~]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sr0              11:0    1  4.3G  0 rom  /run/media/root/CentOS 7 x86_64</span><br><span class="line">vda             252:0    0  100G  0 disk</span><br><span class="line">├─vda1          252:1    0    1G  0 part /boot</span><br><span class="line">└─vda2          252:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0  3.9G  0 lvm</span><br><span class="line">  └─centos-home 253:2    0 45.1G  0 lvm  /home</span><br></pre></td></tr></table></figure><p>如果我们新开一个终端，可以通过 <code>udevadm</code> 命令查看到设备移除过程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]<span class="comment"># udevadm monitor</span></span><br><span class="line">monitor will <span class="built_in">print</span> the received events <span class="keyword">for</span>:</span><br><span class="line">UDEV - the event <span class="built_in">which</span> udev sends out after rule processing</span><br><span class="line">KERNEL - the kernel uevent</span><br><span class="line"></span><br><span class="line">KERNEL[1255.635671] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/bsg/2:0:0:0 (bsg)</span><br><span class="line">KERNEL[1255.636247] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_generic/sg1 (scsi_generic)</span><br><span class="line">KERNEL[1255.636265] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_device/2:0:0:0 (scsi_device)</span><br><span class="line">KERNEL[1255.636357] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_disk/2:0:0:0 (scsi_disk)</span><br><span class="line">UDEV  [1255.637109] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/bsg/2:0:0:0 (bsg)</span><br><span class="line">KERNEL[1255.638351] remove   /devices/virtual/bdi/8:0 (bdi)</span><br><span class="line">UDEV  [1255.638369] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_device/2:0:0:0 (scsi_device)</span><br><span class="line">KERNEL[1255.638385] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/block/sda (block)</span><br><span class="line">UDEV  [1255.638397] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_disk/2:0:0:0 (scsi_disk)</span><br><span class="line">KERNEL[1255.638417] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0 (scsi)</span><br><span class="line">UDEV  [1255.639174] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_generic/sg1 (scsi_generic)</span><br><span class="line">UDEV  [1255.639192] remove   /devices/virtual/bdi/8:0 (bdi)</span><br><span class="line">UDEV  [1255.641850] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/block/sda (block)</span><br><span class="line">UDEV  [1255.642914] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0 (scsi)</span><br><span class="line">KERNEL[1255.643143] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0 (scsi)</span><br><span class="line">UDEV  [1255.643608] remove   /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0 (scsi)</span><br></pre></td></tr></table></figure><p>说完了拔出磁盘，那么该如何插入呢？最简单的办法肯定是重启 OS，毕竟我们不是真正的拔出了设备，重启 OS 之后设备肯定会重新发现。我们也可以通过修改对应的标置文件来手动重新执行设备扫描动作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sr0              11:0    1  4.3G  0 rom  /run/media/root/CentOS 7 x86_64</span><br><span class="line">vda             252:0    0  100G  0 disk</span><br><span class="line">├─vda1          252:1    0    1G  0 part /boot</span><br><span class="line">└─vda2          252:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0  3.9G  0 lvm</span><br><span class="line">  └─centos-home 253:2    0 45.1G  0 lvm  /home</span><br><span class="line">[root@yiran ~]<span class="comment"># for i in `ls /sys/class/scsi_host/`;do echo "- - -" &gt; /sys/class/scsi_host/$i/scan;done</span></span><br><span class="line">[root@yiran ~]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   10G  0 disk</span><br><span class="line">sr0              11:0    1  4.3G  0 rom  /run/media/root/CentOS 7 x86_64</span><br><span class="line">vda             252:0    0  100G  0 disk</span><br><span class="line">├─vda1          252:1    0    1G  0 part /boot</span><br><span class="line">└─vda2          252:2    0   99G  0 part</span><br><span class="line">  ├─centos-root 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─centos-swap 253:1    0  3.9G  0 lvm</span><br><span class="line">  └─centos-home 253:2    0 45.1G  0 lvm  /home</span><br></pre></td></tr></table></figure><p>同样，在另一个终端可以看到 udev 事件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]<span class="comment"># udevadm monitor</span></span><br><span class="line">monitor will <span class="built_in">print</span> the received events <span class="keyword">for</span>:</span><br><span class="line">UDEV - the event <span class="built_in">which</span> udev sends out after rule processing</span><br><span class="line">KERNEL - the kernel uevent</span><br><span class="line"></span><br><span class="line">KERNEL[1506.675351] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0 (scsi)</span><br><span class="line">KERNEL[1506.675416] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0 (scsi)</span><br><span class="line">KERNEL[1506.675523] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_disk/2:0:0:0 (scsi_disk)</span><br><span class="line">KERNEL[1506.675562] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_device/2:0:0:0 (scsi_device)</span><br><span class="line">KERNEL[1506.676417] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_generic/sg1 (scsi_generic)</span><br><span class="line">KERNEL[1506.676555] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/bsg/2:0:0:0 (bsg)</span><br><span class="line">UDEV  [1506.677524] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0 (scsi)</span><br><span class="line">KERNEL[1506.677662] add      /devices/virtual/bdi/8:0 (bdi)</span><br><span class="line">UDEV  [1506.682939] add      /devices/virtual/bdi/8:0 (bdi)</span><br><span class="line">KERNEL[1506.682968] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/block/sda (block)</span><br><span class="line">UDEV  [1506.683015] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0 (scsi)</span><br><span class="line">UDEV  [1506.683036] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_disk/2:0:0:0 (scsi_disk)</span><br><span class="line">UDEV  [1506.683053] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_device/2:0:0:0 (scsi_device)</span><br><span class="line">UDEV  [1506.689442] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/scsi_generic/sg1 (scsi_generic)</span><br><span class="line">UDEV  [1506.690861] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/bsg/2:0:0:0 (bsg)</span><br><span class="line">UDEV  [1506.711146] add      /devices/pci0000:00/0000:00:07.0/virtio3/host2/target2:0:0/2:0:0:0/block/sda (block)</span><br></pre></td></tr></table></figure><h3 id="延迟"><a href="#延迟" class="headerlink" title="延迟"></a>延迟</h3><p>为了测试软件在高 IO 延迟下的表现，现在我们需要给一块磁盘设置指定的 IO 延迟（latency），可以使用 dm-delay 来实现：</p><p>dm-delay具体参数为：<code>&lt;device&gt; &lt;offset&gt; &lt;delay&gt; [&lt;write_device&gt; &lt;write_offset&gt; &lt;write_delay&gt;]</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 10:45:54 ~]<span class="variable">$modprobe</span> brd rd_nr=1 rd_size=10485760 <span class="comment"># 创建10G ram disk</span></span><br><span class="line">[root@node90 10:46:23 ~]<span class="variable">$blockdev</span> --getsize /dev/ram0</span><br><span class="line">20971520 </span><br><span class="line">[root@node90 10:46:23 ~]<span class="variable">$size</span>=$(blockdev --getsize /dev/ram0) <span class="comment"># Size in 512-bytes sectors</span></span><br><span class="line">[root@node90 10:46:23 ~]<span class="variable">$echo</span> <span class="string">"0 <span class="variable">$size</span> delay /dev/ram0 0 500"</span> | dmsetup create delayed <span class="comment"># 设置读延迟 500</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 10:45:54 ~]cat fio.conf</span><br><span class="line">[random]</span><br><span class="line">filename=/dev/dm-0</span><br><span class="line">readwrite=randread</span><br><span class="line">blocksize=4k</span><br><span class="line">ioengine=sync</span><br><span class="line">direct=1</span><br><span class="line">time_based=1</span><br><span class="line">runtime=10</span><br><span class="line">[root@node90 10:45:54 ~]fio fio.conf</span><br><span class="line">...</span><br><span class="line">clat (usec): min=500769, max=502110, avg=501122.35, stdev=357.23 <span class="comment">#写入延迟 500ms </span></span><br><span class="line">     lat (usec): min=500770, max=502111, avg=501123.45, stdev=357.20 </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>通过 dm-delay，我们可以来进行各种排列组合来模拟磁盘状态，如：读延迟高，写延迟正常；读延迟高，写延迟高；读延迟低，写延迟高等。</p><h3 id="IO-中断"><a href="#IO-中断" class="headerlink" title="IO 中断"></a>IO 中断</h3><p>dm-delay 支持设置 IO 中断，具体命令为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dmsetup suspend /dev/dm-0</span><br><span class="line">$ sudo dmsetup resume  /dev/dm-0</span><br></pre></td></tr></table></figure><p>若在执行 fio 过程中设置 IO 中断， 会看到 iops 为0 的现象：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 10:45:54 ~]fio fio.conf</span><br><span class="line">Starting 1 process</span><br><span class="line">Jobs: 1 (f=1): [r(1)] [0.0% <span class="keyword">done</span>] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 34d:09h:15m:48s]</span><br></pre></td></tr></table></figure><h3 id="IO-Error"><a href="#IO-Error" class="headerlink" title="IO Error"></a>IO Error</h3><p>有时我们也要求验证磁盘突然出现 IO Error 情况，可以通过 <a href="https://www.kernel.org/doc/Documentation/device-mapper/dm-flakey.txt" target="_blank" rel="noopener">dm-flakey</a> 实现：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 11:06:44 ~]<span class="variable">$modprobe</span> brd rd_nr=1 rd_size=10485760 <span class="comment">#10G</span></span><br><span class="line">[root@node90 11:06:58 ~]<span class="variable">$size</span>=$(blockdev --getsize /dev/ram0)</span><br><span class="line">[root@node90 11:06:58 ~]<span class="variable">$echo</span> <span class="string">"0 <span class="variable">$size</span> flakey /dev/ram0 0 60 0"</span> | dmsetup create flakey</span><br><span class="line">[root@node90 11:06:59 ~]<span class="variable">$size</span>=$(blockdev --getsize /dev/ram0)</span><br><span class="line">[root@node90 11:07:06 ~]<span class="variable">$echo</span> <span class="string">"0 <span class="variable">$size</span> flakey /dev/ram0 0 30 30"</span>  | dmsetup reload flakey</span><br><span class="line">[root@node90 11:07:06 ~]<span class="variable">$dmsetup</span> resume flakey</span><br><span class="line">[root@node90 11:07:06 ~]<span class="variable">$dmsetup</span> table flakey</span><br><span class="line">0 20971520 flakey 1:0 0 30 30 0</span><br></pre></td></tr></table></figure><p>若在执行 fio 过程中设置 IO Error，则会看到以下错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 10:45:54 ~]fio fio.conf</span><br><span class="line">Starting 1 process</span><br><span class="line">fio: io_u error on file /dev/dm-0: Input/output error: <span class="built_in">read</span> offset=647606272, buflen=4096</span><br><span class="line">fio: pid=24184, err=5/file:io_u.c:1708, func=io_u error, error=Input/output error</span><br><span class="line"></span><br><span class="line">random: (groupid=0, <span class="built_in">jobs</span>=1): err= 5 (file:io_u.c:1708, func=io_u error, error=Input/output error): pid=24184: Sat Aug 31 11:09:45 2019</span><br><span class="line">  cpu          : usr=0.00%, sys=0.00%, ctx=0, majf=0, minf=69</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>在故障场景中，最常见的应该就是网络故障，尤其是现在分布式应用的场景很多，另一点原因是网络相关的工具很多，可以很方便的使用。</p><h3 id="故障"><a href="#故障" class="headerlink" title="故障"></a>故障</h3><p>最简单的网络故障应该就是连接中断，也就是各个节点之间无法联通了，我们可以直接通过 <code>ifdown &lt;nic name&gt;</code> 来将网卡置为 down，或者如果不在意其他网卡状态的话，可以直接 <code>systemctl stop network</code> 停止网络服务，来模拟无法联通情况。</p><p>有时候我们不想模拟网卡故障，想要模拟固定节点之间的网络故障，那么可以通过 iptables 来实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -s 172.11.30.14 -j DROP</span><br><span class="line">iptables -I OUTPUT -d 172.11.30.14 -j DROP</span><br></pre></td></tr></table></figure><p>将指定 IP 的所有连接都丢弃。</p><h3 id="延迟-1"><a href="#延迟-1" class="headerlink" title="延迟"></a>延迟</h3><p>网络延迟模拟可以通过 <code>tc</code> 来实现，命令很简单，找到指定的网卡名称，并执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]<span class="comment"># tc qdisc add dev eth0 root netem delay 8ms</span></span><br></pre></td></tr></table></figure><p>新开终端，在其他节点 ping 设置了网络延迟的节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@yiran-30-250:~</span><br><span class="line"> $ ping 192.168.30.246</span><br><span class="line">PING 192.168.30.246 (192.168.30.246) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.30.246: icmp_seq=1 ttl=64 time=8.76 ms</span><br><span class="line">64 bytes from 192.168.30.246: icmp_seq=2 ttl=64 time=8.40 ms</span><br><span class="line">64 bytes from 192.168.30.246: icmp_seq=3 ttl=64 time=8.38 ms</span><br><span class="line">64 bytes from 192.168.30.246: icmp_seq=4 ttl=64 time=8.38 ms</span><br><span class="line">^C</span><br><span class="line">--- 192.168.30.246 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3005ms</span><br><span class="line">rtt min/avg/max/mdev = 8.381/8.485/8.769/0.198 ms</span><br></pre></td></tr></table></figure><p>可以看到延迟稳定在 8ms 左右。</p><p>除了 <code>tc</code> 工具外，前几天看到了一个 Golang 实现的 L4 网络代理<a href="https://github.com/fagongzi/netproxy" target="_blank" rel="noopener">项目</a>，可以模拟延迟和丢包，具体关于故障模拟的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *TCPServer)</span> <span class="title">doWrite</span><span class="params">(bytes []<span class="keyword">byte</span>, conn goetty.IOSession, ctl *conf.CtlUnit)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> ctl.DelayMs &gt; <span class="number">0</span> &#123;</span><br><span class="line">log.Infof(<span class="string">"Delay &lt;%d&gt;ms write to &lt;%s&gt;"</span>, ctl.DelayMs, t.proxyUnit.Target)</span><br><span class="line">time.Sleep(time.Millisecond * time.Duration(ctl.DelayMs))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">conn.WriteAndFlush(bytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">_, err = session.Read()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Infof(<span class="string">"Read from client&lt;%s&gt; failure.err=%+v"</span>, session.RemoteAddr(), err)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// write to target</span></span><br><span class="line">ctl := t.proxyUnit.Ctl</span><br><span class="line">bytes := in.RawBuf()[in.GetReaderIndex():in.GetWriteIndex()]</span><br><span class="line"><span class="keyword">if</span> <span class="number">0</span> == ctl.Out.LossRate &#123;</span><br><span class="line">log.Debugf(<span class="string">"write %d bytes to &lt;%s&gt;"</span>, <span class="built_in">len</span>(bytes), conn.RemoteAddr())</span><br><span class="line">t.doWrite(bytes, conn, ctl.Out)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> t.rnd.Intn(<span class="number">100</span>) &gt; ctl.Out.LossRate &#123;</span><br><span class="line">log.Debugf(<span class="string">"write %d bytes to &lt;%s&gt;"</span>, <span class="built_in">len</span>(bytes), conn.RemoteAddr())</span><br><span class="line">t.doWrite(bytes, conn, ctl.Out)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">log.Debugf(<span class="string">"Loss write %d bytes to &lt;%s&gt;"</span>, <span class="built_in">len</span>(bytes), conn.RemoteAddr())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">in.SetReaderIndex(in.GetWriteIndex())</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h2><h3 id="占用"><a href="#占用" class="headerlink" title="占用"></a>占用</h3><p>在使用 Zookeeper 这类有状态应用时，除了通过 service 是否处于 running 来判断服务是否运行外，还需要判断节点的 zk 角色是否符合预期，如 leader 或 follower。</p><p>前几天又一个场景需要造出 Zookeeper 处于运行中，但是角色状态处于异常状态，我通过 <code>nc</code> 来实现的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran ~]# nc -kl 2181</span><br></pre></td></tr></table></figure><p>现将端口占用，然后启动 Zookeeper，就可以了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>故障模拟的主要使用场景是产品的自动化测试中，平时主要是辅助开发来自测，不需要记住具体参数，但是最好了解到什么场景下有什么工具可以使用，省时省力。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-managing_guest_virtual_machines_with_virsh-attaching_and_updating_a_device_with_virsh" target="_blank" rel="noopener">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-managing_guest_virtual_machines_with_virsh-attaching_and_updating_a_device_with_virsh</a></li><li><a href="https://www.enodev.fr/posts/emulate-a-slow-block-device-with-dm-delay.html" target="_blank" rel="noopener">https://www.enodev.fr/posts/emulate-a-slow-block-device-with-dm-delay.html</a></li><li><a href="https://www.kernel.org/doc/Documentation/device-mapper/dm-flakey.txt" target="_blank" rel="noopener">https://www.kernel.org/doc/Documentation/device-mapper/dm-flakey.txt</a></li><li><a href="https://github.com/fagongzi/netproxy" target="_blank" rel="noopener">https://github.com/fagongzi/netproxy</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在日常开发时，有时候需要保证自己代码的健壮性，需要模拟各种故障测试，比如：磁盘、网络、端口等，今天来汇总一下平时使用最多的几种故障模拟方法&lt;
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Cluster API v1alpha2</title>
    <link href="https://zdyxry.github.io/2019/08/23/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API-v1alpha2/"/>
    <id>https://zdyxry.github.io/2019/08/23/Kubernetes-实战-Cluster-API-v1alpha2/</id>
    <published>2019-08-23T12:13:50.000Z</published>
    <updated>2019-08-23T13:45:21.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天继续来聊一聊 cluster-api，在上周看 cluster-api-provider-vsphere 代码的时候吐槽过，cluster-api 最近因为 v1alpha2 版本的开发，变化太快，几乎每天都在变，那么我们就来看看 v1alpha2 具体做了什么。</p><h2 id="Cluster-API-v1alpha2"><a href="#Cluster-API-v1alpha2" class="headerlink" title="Cluster-API v1alpha2"></a>Cluster-API v1alpha2</h2><p>虽然目前 v1alpha2 还没有正式的 release，但是已经趋于稳定，且两个主要的 provider：aws 和 vsphere 都在进行 v1alpha2 版本的适配（最近每天都有 pr 更新）。我们先来了解下为啥要进行 v1alpha2 改动，改动的目的是啥。</p><p>在 v1alpha1 版本中，cluster-api 要求 provider 实现从节点置备到 k8s 部署的全套流程，cluster-api 自身只负责具体的 API 定义及相关控制，在 provider 实现上也不是一个标准的 Operator，（至少）我从概念的理解上比较吃力，每个 provider 需要实现对应 cluster  与 machine 的 actuator ，开发起来要求对 cluster-api 项目本身很熟悉。</p><p>其次，每个 provider 都包含了 k8s 集群部署的流程，虽然大部分实现最终都是使用 kubeadm 工具，但是使用方式千差万别，有 cloud-init、有 ssh 配合密钥、有 ssh 配合密码等等。这部分 provider 中的代码完全都是重复的，可以复用的。</p><p>上面提到的一些缺点，在 v1alpha2 版本中进行了改进，对各个组件进行了拆分，现在使用 cluster-api 需要 3 个控制器：</p><ol><li>Core(cluster-api)</li><li>Bootstrap(kubeadm)</li><li>Infrastructure(aws, gcp, azure, vsphere, etc)</li></ol><img src="/2019/08/23/Kubernetes-实战-Cluster-API-v1alpha2/v1alpha2.png" title="v1alpha2"><p>下面来说说各个控制器负责什么。</p><h2 id="Core-cluster-api"><a href="#Core-cluster-api" class="headerlink" title="Core(cluster-api)"></a>Core(cluster-api)</h2><p>核心控制器，也就是 cluster-api 项目自身，相比于 v1alpha1 版本 v1alpha2 各个方面简直可爱，来看看具体的改动：</p><h3 id="clusterctl"><a href="#clusterctl" class="headerlink" title="clusterctl"></a>clusterctl</h3><p>在 v1alpha1 中，clusterctl 因为耦合了最终的 provider 项目，命令行是由对应的 provider 提供，在 v1alpha2 中完全由 cluster-api 维护。具体代码在 <a href="https://github.com/kubernetes-sigs/cluster-api/blob/master/cmd/clusterctl/README.md" target="_blank" rel="noopener">cmd/clusterctl</a> 下，整体结构感觉跟 kubeadm 很像。</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><p>仍然包含两个主要的 CRD：cluster 和 machine，针对两个资源进行了阶段定义，根据注释就很好 cluster 和 machine 会处于哪些状态：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> (</span><br><span class="line"><span class="comment">// MachinePhasePending is the first state a Machine is assigned by</span></span><br><span class="line"><span class="comment">// Cluster API Machine controller after being created.</span></span><br><span class="line">MachinePhasePending = MachinePhase(<span class="string">"pending"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseProvisioning is the state when the</span></span><br><span class="line"><span class="comment">// Machine infrastructure is being created.</span></span><br><span class="line">MachinePhaseProvisioning = MachinePhase(<span class="string">"provisioning"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseProvisioned is the state when its</span></span><br><span class="line"><span class="comment">// infrastructure has been created and configured.</span></span><br><span class="line">MachinePhaseProvisioned = MachinePhase(<span class="string">"provisioned"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseRunning is the Machine state when it has</span></span><br><span class="line"><span class="comment">// become a Kubernetes Node in a Ready state.</span></span><br><span class="line">MachinePhaseRunning = MachinePhase(<span class="string">"running"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseDeleting is the Machine state when a delete</span></span><br><span class="line"><span class="comment">// request has been sent to the API Server,</span></span><br><span class="line"><span class="comment">// but its infrastructure has not yet been fully deleted.</span></span><br><span class="line">MachinePhaseDeleting = MachinePhase(<span class="string">"deleting"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseDeleted is the Machine state when the object</span></span><br><span class="line"><span class="comment">// and the related infrastructure is deleted and</span></span><br><span class="line"><span class="comment">// ready to be garbage collected by the API Server.</span></span><br><span class="line">MachinePhaseDeleted = MachinePhase(<span class="string">"deleted"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseFailed is the Machine state when the system</span></span><br><span class="line"><span class="comment">// might require user intervention.</span></span><br><span class="line">MachinePhaseFailed = MachinePhase(<span class="string">"failed"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MachinePhaseUnknown is returned if the Machine state cannot be determined.</span></span><br><span class="line">MachinePhaseUnknown = MachinePhase(<span class="string">""</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>cluster-api 不再负责具体的 API 所有资源的定义，而是通过设置资源的 owner 来达到最终 cluster 包含 machine 的效果。</p><p>用 AWS 官方示例展示：</p><p>cluster.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">cluster.x-k8s.io/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Cluster</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  clusterNetwork:</span></span><br><span class="line"><span class="attr">    pods:</span></span><br><span class="line"><span class="attr">      cidrBlocks:</span> <span class="string">["192.168.0.0/16"]</span></span><br><span class="line"><span class="attr">  infrastructureRef:</span></span><br><span class="line"><span class="attr">    apiVersion:</span> <span class="string">infrastructure.cluster.x-k8s.io/v1alpha2</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">AWSCluster</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">infrastructure.cluster.x-k8s.io/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">AWSCluster</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  region:</span> <span class="string">$&#123;AWS_REGION&#125;</span></span><br><span class="line"><span class="attr">  sshKeyName:</span> <span class="string">$&#123;SSH_KEY_NAME&#125;</span></span><br></pre></td></tr></table></figure></p><p>在执行完 <code>kubectl apply -f cluster.yaml</code> 后， aws cluster 控制器会等待 cluster-api 控制器将 <code>AWSCluster</code> 与 <code>Cluster</code> 资源进行关联后进行下一步操作，这样最终能够达到 <code>Cluster</code> 资源是所有资源的 owner，也可以获得到所有资源的具体的 Spec 和 Status 信息。</p><p>（等版本 release 之后好好了解下其中的关系。</p><h2 id="Boostrap-kubeadm"><a href="#Boostrap-kubeadm" class="headerlink" title="Boostrap(kubeadm)"></a>Boostrap(kubeadm)</h2><p>相比 cluster-api 的复杂，目前 Bootstrap 的唯一实现 <a href="https://github.com/kubernetes-sigs/cluster-api-bootstrap-provider-kubeadm" target="_blank" rel="noopener">CABPK</a> 是一个分拆出来的独立项目，这个项目的主要目的是通过 <code>Cluster</code> 和 <code>Machine</code> 及对应的 Infrastructure <code>Cluster</code> 和 <code>Machine</code> 信息，来生成 cloud-init 配置（Userdata）。</p><p>目前 v1alpha2 统一部署 k8s 集群的方式为 cloud-init ，通过 CABPK 项目生成对应 Machine 的 cloud-init 配置，在部署虚拟机的时候传递 Userdata，达到自动配置的效果。也就是说如果你的 Hypervisor 不支持 cloud-init，就没办法使用 cluster-api v1alpha2 进行部署，只能通过 v1alpha1 的方式，在 Infrastructure 控制器做所有事情。</p><p>需要注意的是，在 Infrastructure 控制器工作前，需要保证 Bootstrap 控制器正常工作，生成用于最终创建虚拟机的 cloud-init 配置。</p><h2 id="Infrastructure"><a href="#Infrastructure" class="headerlink" title="Infrastructure"></a>Infrastructure</h2><p>前面说的两个控制器都是社区官方维护的，下面来说下各个厂家（provider）维护的 Infrastructure 控制器（最近几天都在搞这个）。</p><p>v1alpha2 版本要求各个 provider 实现一个标准且完整的 Operator。包含对应的 <code>Cluster</code>,<code>Machine</code>。以下还是使用 cluster-api-provider-vsphere 具体，截止到 20190823 ，<a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/pulls" target="_blank" rel="noopener">对应 PR</a> 还处于未完成状态。</p><h3 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// VSphereClusterSpec defines the desired state of VSphereCluster</span></span><br><span class="line"><span class="keyword">type</span> VSphereClusterSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">Server <span class="keyword">string</span> <span class="string">`json:"server,omitempty"`</span></span><br><span class="line">Username <span class="keyword">string</span> <span class="string">`json:"username,omitempty"`</span></span><br><span class="line">Password <span class="keyword">string</span> <span class="string">`json:"password,omitempty"`</span></span><br><span class="line">Insecure *<span class="keyword">bool</span> <span class="string">`json:"insecure,omitempty"`</span></span><br><span class="line">SSHAuthorizedKeys []<span class="keyword">string</span> <span class="string">`json:"sshAuthorizedKeys,omitempty"`</span></span><br><span class="line">CloudProviderConfiguration cloud.Config <span class="string">`json:"cloudProviderConfiguration,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// VSphereClusterStatus defines the observed state of VSphereClusterSpec</span></span><br><span class="line"><span class="keyword">type</span> VSphereClusterStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">Ready <span class="keyword">bool</span> <span class="string">`json:"ready"`</span></span><br><span class="line">APIEndpoints []APIEndpoint <span class="string">`json:"apiEndpoints,omitempty"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// VSphereMachineSpec defines the desired state of VSphereMachine</span></span><br><span class="line"><span class="keyword">type</span> VSphereMachineSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">MachineRef <span class="keyword">string</span> <span class="string">`json:"machineRef,omitempty"`</span></span><br><span class="line">Template <span class="keyword">string</span> <span class="string">`json:"template"`</span></span><br><span class="line">Datacenter <span class="keyword">string</span> <span class="string">`json:"datacenter"`</span></span><br><span class="line">Network NetworkSpec <span class="string">`json:"network"`</span></span><br><span class="line">NumCPUs <span class="keyword">int32</span> <span class="string">`json:"numCPUs,omitempty"`</span></span><br><span class="line">NumCoresPerSocket <span class="keyword">int32</span> <span class="string">`json:"numCoresPerSocket,omitempty"`</span></span><br><span class="line">MemoryMiB <span class="keyword">int64</span> <span class="string">`json:"memoryMiB,omitempty"`</span></span><br><span class="line">DiskGiB <span class="keyword">int32</span> <span class="string">`json:"diskGiB,omitempty"`</span></span><br><span class="line">TrustedCerts [][]<span class="keyword">byte</span> <span class="string">`json:"trustedCerts,omitempty"`</span></span><br><span class="line">NTPServers []<span class="keyword">string</span> <span class="string">`json:"ntpServers,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// VSphereMachineStatus defines the observed state of VSphereMachine</span></span><br><span class="line"><span class="keyword">type</span> VSphereMachineStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">Ready <span class="keyword">bool</span> <span class="string">`json:"ready"`</span></span><br><span class="line">Addresses []v1.NodeAddress <span class="string">`json:"addresses,omitempty"`</span></span><br><span class="line">TaskRef <span class="keyword">string</span> <span class="string">`json:"taskRef,omitempty"`</span></span><br><span class="line">Network []NetworkStatus <span class="string">`json:"networkStatus,omitempty"`</span></span><br><span class="line">ErrorReason *errors.MachineStatusError <span class="string">`json:"errorReason,omitempty"`</span></span><br><span class="line">ErrorMessage *<span class="keyword">string</span> <span class="string">`json:"errorMessage,omitempty"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义 VSphereCluster 与 VSphereMachine CRD，这里根据自己需要定义相关参数就好，比如如果不想支持自定义目标虚拟机配置，那么就不用提供 CPU、Memory 等配置。</p><h3 id="Controller-1"><a href="#Controller-1" class="headerlink" title="Controller"></a>Controller</h3><p>来分别看看 cluster controller 与 machine controller 做了啥。</p><h4 id="Cluster-Controller"><a href="#Cluster-Controller" class="headerlink" title="Cluster Controller"></a>Cluster Controller</h4><p>在 <code>Reconcile</code> 的实现中，先获取 VSphereCluster 信息：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vsphereCluster := &amp;infrav1.VSphereCluster&#123;&#125;</span><br><span class="line">err := r.Get(parentContext, req.NamespacedName, vsphereCluster)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> apierrors.IsNotFound(err) &#123;</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p><p>然后获取 VSphereCluster 关联的 Cluster 信息：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// util 由 cluster-api 提供</span></span><br><span class="line">cluster, err := util.GetOwnerCluster(parentContext, r.Client, vsphereCluster.ObjectMeta)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> cluster == <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Info(<span class="string">"Waiting for Cluster Controller to set OwnerRef on VSphereCluster"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: <span class="number">10</span> * time.Second&#125;, <span class="literal">nil</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>在获取了足够的信息之后，针对这些信息，创建了 context（这里 aws 是叫 scope，是一个东西），判断是否包含 <code>DeletionTimestamp</code> 来决定进行什么调度，是 <code>reconcileDelete</code> 还是 <code>reconcileNormal</code> 。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">reconcileDelete</span><span class="params">(ctx *context.ClusterContext)</span> <span class="params">(reconcile.Result, error)</span></span> &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Reconciling VSphereCluster delete"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Cluster is deleted so remove the finalizer.</span></span><br><span class="line">ctx.VSphereCluster.Finalizers = util.Filter(ctx.VSphereCluster.Finalizers, infrav1.ClusterFinalizer)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在所有的 provider 中，都对相应资源进行了 Finalizer 配置，方便后续在资源删除前进行一些额外的操作，这里 vsphere 还没有具体的实现。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">reconcileNormal</span><span class="params">(ctx *context.ClusterContext)</span> <span class="params">(reconcile.Result, error)</span></span> &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Reconciling VSphereCluster"</span>)</span><br><span class="line"></span><br><span class="line">vsphereCluster := ctx.VSphereCluster</span><br><span class="line"></span><br><span class="line"><span class="comment">// If the VSphereCluster doesn't have our finalizer, add it.</span></span><br><span class="line"><span class="keyword">if</span> !util.Contains(vsphereCluster.Finalizers, infrav1.ClusterFinalizer) &#123;</span><br><span class="line">vsphereCluster.Finalizers = <span class="built_in">append</span>(vsphereCluster.Finalizers, infrav1.ClusterFinalizer)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set APIEndpoints so the Cluster API Cluster Controller can pull them</span></span><br><span class="line">vsphereCluster.Status.APIEndpoints = []infrav1.APIEndpoint&#123;</span><br><span class="line">&#123;</span><br><span class="line">Host: <span class="string">""</span>, <span class="comment">// vsphereCluster.Status.Network.APIServerELB.DNSName,</span></span><br><span class="line">Port: <span class="number">0</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// No errors, so mark us ready so the Cluster API Cluster Controller can pull it</span></span><br><span class="line">vsphereCluster.Status.Ready = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>reconcileNormal</code> 中对集群进行 Finalizer 判断，然后这里直接更新了 <code>vsphereCluster.Status</code> 具体字段，应该是还没实现完成。</p><h4 id="Machine-Controller"><a href="#Machine-Controller" class="headerlink" title="Machine Controller"></a>Machine Controller</h4><p>与 Cluster 相比，Machine 包含了具体的虚拟机创建动作，相应需要的信息也会多一些，除了通过 <code>VSphereMachine</code>获取关联 <code>Machine</code> 以外，还通过 <code>Machine</code> 获取了 <code>Cluster</code> 信息：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Fetch the Machine.</span></span><br><span class="line">machine, err := util.GetOwnerMachine(parentContext, r.Client, vsphereMachine.ObjectMeta)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> machine == <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Info(<span class="string">"Waiting for Machine Controller to set OwnerRef on VSphereMachine"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: <span class="number">10</span> * time.Second&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">logger = logger.WithName(fmt.Sprintf(<span class="string">"machine=%s"</span>, machine.Name))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Fetch the Cluster.</span></span><br><span class="line">cluster, err := util.GetClusterFromMetadata(parentContext, r.Client, machine.ObjectMeta)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Info(<span class="string">"Machine is missing cluster label or cluster does not exist"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>与 Cluster 一样，也会进行 <code>DeletionTimestamp</code> 判断，来看下 <code>reconcileNormal</code> 的实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *VSphereMachineReconciler)</span> <span class="title">reconcileNormal</span><span class="params">(ctx *context.MachineContext)</span> <span class="params">(reconcile.Result, error)</span></span> &#123;</span><br><span class="line"><span class="comment">// 如果 VSphereMachine 的状态一场，则直接返回</span></span><br><span class="line"><span class="keyword">if</span> ctx.VSphereMachine.Status.ErrorReason != <span class="literal">nil</span> || ctx.VSphereMachine.Status.ErrorMessage != <span class="literal">nil</span> &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Error state detected, skipping reconciliation"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果 VSphereMachine 没有 Finalizer，则添加</span></span><br><span class="line"><span class="keyword">if</span> !util.Contains(ctx.VSphereMachine.Finalizers, infrav1.MachineFinalizer) &#123;</span><br><span class="line">ctx.VSphereMachine.Finalizers = <span class="built_in">append</span>(ctx.VSphereMachine.Finalizers, infrav1.MachineFinalizer)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> !ctx.Cluster.Status.InfrastructureReady &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Cluster infrastructure is not ready yet, requeuing machine"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: waitForClusterInfrastructureReadyDuration&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在真正创建虚拟机之前，需要确保 cloud-init 配置已经生成</span></span><br><span class="line"><span class="keyword">if</span> ctx.Machine.Spec.Bootstrap.Data == <span class="literal">nil</span> &#123;</span><br><span class="line">ctx.Logger.Info(<span class="string">"Waiting for bootstrap data to be available"</span>)</span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;RequeueAfter: <span class="number">10</span> * time.Second&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里缺少真正创建虚拟机的实现</span></span><br><span class="line"><span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>vsphere 所有关于虚拟机的操作都在<a href="https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/tree/master/pkg/cloud/vsphere/services/govmomi" target="_blank" rel="noopener">这里</a>，感兴趣的可以看看。</p><h3 id="main-go"><a href="#main-go" class="headerlink" title="main.go"></a>main.go</h3><p>看完了具体实现逻辑，那么去看下控制器入口： main.go</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">_ = clientgoscheme.AddToScheme(scheme)</span><br><span class="line">_ = infrav1.AddToScheme(scheme)</span><br><span class="line"><span class="comment">// +kubebuilder:scaffold:scheme</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> v, err := time.ParseDuration(os.Getenv(syncPeriodEnvVar)); err == <span class="literal">nil</span> &#123;</span><br><span class="line">defaultSyncPeriod = v</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> v, err := time.ParseDuration(os.Getenv(requeuePeriodEnvVar)); err == <span class="literal">nil</span> &#123;</span><br><span class="line">defaultRequeuePeriod = v</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是 init 函数，这里应该加上 <code>_ = clusterv1.AddToScheme(scheme)</code> ，否则在真正创建对应资源的时候，会报 <code>no kind is registered for the type</code> 的错误。（对 Operator 还不太熟，这里具体原因未了解）</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>相对来说 v1alpha2 版本改动范围较大，但是真香。各个逻辑部分比之前容易理解的多，且provider 实现也比之前简单许多（虽然还是比较麻烦）。</p><p>但是恰恰因为是 alpha 版本，再次吐槽，改动真的是太频繁了，按照项目中 Milestone 规划，在 20190831 就要 release v1alpha2 了，但是目前完成度还只有 <a href="https://github.com/kubernetes-sigs/cluster-api/milestone/6" target="_blank" rel="noopener">58%</a> 。而且目前看到除了 aws 和 vmware 对这个项目参与的比较积极，其他项目都处于假死状态，不知道后续是否会有其他厂家参与进来共同完善。</p><h2 id="P-S"><a href="#P-S" class="headerlink" title="P.S."></a>P.S.</h2><ul><li>cluster-api 相关项目使用 kubebuilder 辅助构建的，随着 kubebuilder v2 正式发布，相关代码结构改动较大，最好花费些时间了解下 kubebuilder v2。</li><li>目前 provider 中 aws 完善度是最高的，但是 aws 中的概念是真的复杂，<strong>概念劝退</strong>。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;今天继续来聊一聊 cluster-api，在上周看 cluster-api-provider-vsphere 代码的时候吐槽过，cluste
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>provider vs provisioner</title>
    <link href="https://zdyxry.github.io/2019/08/17/provider-vs-provisioner/"/>
    <id>https://zdyxry.github.io/2019/08/17/provider-vs-provisioner/</id>
    <published>2019-08-17T13:08:46.000Z</published>
    <updated>2019-08-17T13:09:18.569Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前 liqiang 同学写了一篇博客：<a href="https://liqiang.io/post/status-or-state-fa70399e" target="_blank" rel="noopener">Status 还是 State</a> 用于总结日常工作中遇到的相似词的区别。这两天看代码，经常能够看到两个词：provider 和 provisioner，作为一个英语渣渣，很难准确的理解两个词的区别。</p><h2 id="字典解释"><a href="#字典解释" class="headerlink" title="字典解释"></a>字典解释</h2><p>provider 字典中的解释为：</p><ol><li>供应者（商）</li><li>提供者（商）</li><li>供养人</li><li>…</li></ol><p>provisioner 字典中的解释为：</p><ol><li>粮食供应者</li></ol><p>看上去从字面上也是一个意思，那么我们来找个实际场景看看。</p><h2 id="实际场景"><a href="#实际场景" class="headerlink" title="实际场景"></a>实际场景</h2><h3 id="Terraform"><a href="#Terraform" class="headerlink" title="Terraform"></a>Terraform</h3><p>在 Terraform 概念中，同时存在 provider 和 provisioner 两个概念：</p><figure class="highlight plain"><figcaption><span>is used to create, manage, and update infrastructure resources such as physical machines, VMs, network switches, containers, and more. Almost any infrastructure type can be represented as a resource in Terraform.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. Terraform Cloud, DNSimple, CloudFlare).</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Provisioners are used to execute scripts on a local or remote machine as part of resource creation or destruction. Provisioners can be used to bootstrap a resource, cleanup before destroy, run configuration management, etc.</span><br></pre></td></tr></table></figure><p>这里的 provider 对应 <code>供应商</code> 的含义是可以正确理解的，不同的供应商提供不同的插件。</p><p>provisioner 在这里指的是定义的资源可使用的插件，如 <code>remote-exec</code>,<code>file</code>,<code>chef</code> 等等，跟供应商关系不大。</p><h3 id="Vagrant-Docker"><a href="#Vagrant-Docker" class="headerlink" title="Vagrant Docker"></a>Vagrant Docker</h3><p>在搜索过程中，看到有人在 SO 上问了这么个问题 <a href="https://stackoverflow.com/questions/30394707/vagrant-docker-provider-vs-docker-provisioner" target="_blank" rel="noopener">Vagrant - Docker provider vs. docker provisioner</a></p><p>下面的高票回答：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Docker provisioner help to prepare environment: build and pull images, run containers if you need multiple containers running for your vagrant machine. Docker provider is running vagrant machine in docker container (instead of VM/cloud as other providers do).</span><br></pre></td></tr></table></figure><p>虽然感觉说的不是很清楚，但是也可以看出 provider 是有明确分类的，而 provisioner 更像是具体的动作种类，比如创建、删除、更新等。</p><h3 id="CSI"><a href="#CSI" class="headerlink" title="CSI"></a>CSI</h3><p>在 k8s CSI 中，也有 provisioner 的概念，这里主要是指具体的“置备动作执行者”。</p><p>当然 provisioner 也有多个，根据具体的存储供应商的不同，提供不同的 provisioner。</p><h3 id="Cluster-API"><a href="#Cluster-API" class="headerlink" title="Cluster-API"></a>Cluster-API</h3><p>在 Cluster-API 中，不同厂家为了支持 Cluster-API，会实现不同的 provider，比如 cluster-api-provider-aws, cluster-api-provider-vspher 等。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>根据实际的情况，provider 主要是用来区分供应者，是谁来提供某个事物，通常以公司区分。</p><p>provisioner 使用场景不多，主要是涉及到具体的功能执行，我这里理解为“置备动作执行者”，虽然也可以进行分类，但是主要意思应该是前者比较多一些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;之前 liqiang 同学写了一篇博客：&lt;a href=&quot;https://liqiang.io/post/status-or-state-f
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>cluster-api-provider-vsphere 源码阅读</title>
    <link href="https://zdyxry.github.io/2019/08/16/cluster-api-provider-vsphere-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>https://zdyxry.github.io/2019/08/16/cluster-api-provider-vsphere-源码阅读/</id>
    <published>2019-08-16T12:57:25.000Z</published>
    <updated>2019-08-17T13:03:10.415Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>上一篇博客讲了 Cluster-API 的相关概念，现在我们来找一个 provider 实现看看具体里面做了啥，因为对 vmware 产品中的概念比较熟悉，就找了 cluster-api-provider-vsphere 。</p><p>以下内容均对应 clusterapi v1alpha1 版本。</p><h2 id="clusterctl-命令"><a href="#clusterctl-命令" class="headerlink" title="clusterctl 命令"></a>clusterctl 命令</h2><p>cluster-api provider 提供了命令行 <code>clusterctl</code> 用于给我们快速创建 bootstrap 集群用于创建目标 k8s 集群，我们来执行一下看看具体做了哪些工作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@yiran-workstation:~/go/src/github.com/kubernetes-sigs/cluster-api-provider-vsphere </span><br><span class="line">master ✗ $ clusterctl create cluster \                                                       </span><br><span class="line">  --provider vsphere \</span><br><span class="line">  --bootstrap-type kind \</span><br><span class="line">  --cluster ./out/management-cluster/cluster.yaml \</span><br><span class="line">  --machines ./out/management-cluster/machines.yaml \</span><br><span class="line">  --provider-components ./out/management-cluster/provider-components.yaml \</span><br><span class="line">  --addon-components ./out/management-cluster/addons.yaml \</span><br><span class="line">  --kubeconfig-out ./out/management-cluster/kubeconfig</span><br><span class="line">I0816 17:28:05.815156   14562 createbootstrapcluster.go:27] Preparing bootstrap cluster</span><br><span class="line">I0816 17:29:15.292547   14562 clusterdeployer.go:78] Applying Cluster API stack to bootstrap cluster</span><br><span class="line">I0816 17:29:15.292619   14562 applyclusterapicomponents.go:26] Applying Cluster API Provider Components</span><br><span class="line">I0816 17:29:16.492405   14562 clusterdeployer.go:83] Provisioning target cluster via bootstrap cluster</span><br><span class="line">I0816 17:29:16.505317   14562 applycluster.go:36] Creating cluster object management-cluster in namespace &quot;default&quot;</span><br><span class="line">I0816 17:29:16.518456   14562 clusterdeployer.go:92] Creating control plane machine in namespace &quot;default&quot;</span><br><span class="line">I0816 17:29:16.548814   14562 applymachines.go:36] Creating machines in namespace &quot;default&quot; # 因为众所周知的“网络”问题，导致我的 Pod image 无法拉取，所以就卡在这里了。。。</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到这个命令一共做了如下的事情：</p><ol><li>创建 bootstrap 集群</li><li>安装 Provider 组件</li><li>创建目标 k8s 集群，创建 Cluster CR，Machine CR</li><li>…</li></ol><p>这里因为“网络”问题我无法继续下去了，那么我们来看下命令行的具体实现。</p><p>这个命令最终是由 provider 提供的，在 provider 中需要实现 2个接口来辅助信息的获取：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Deployer <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> This requirement can be removed once after: https://github.com/kubernetes-sigs/cluster-api/issues/158</span></span><br><span class="line">GetIP(cluster *clusterv1.Cluster, machine *clusterv1.Machine) (<span class="keyword">string</span>, error)</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> This requirement can be removed after: https://github.com/kubernetes-sigs/cluster-api/issues/160</span></span><br><span class="line">GetKubeConfig(cluster *clusterv1.Cluster, master *clusterv1.Machine) (<span class="keyword">string</span>, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Bootstrap 集群中，会运行 2个 StatefulSet ，分别对应的是 Cluster-api-control-manager 和 provider-control-manager。其中跟 Hypervisor 打交道的逻辑全部在 provider-control-manager 中实现。</p><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><p>创建流程：<br><img src="/2019/08/16/cluster-api-provider-vsphere-源码阅读/cluster_create.png" title="cluster_create"></p><p>删除流程：<br><img src="/2019/08/16/cluster-api-provider-vsphere-源码阅读/cluster_delete.png" title="cluster_delete"></p><p>在 Cluster Controller 中，最终所有的调度任务都会通过 actuator 完成，actuator 需要实现以下接口：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Actuator <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// Reconcile creates or applies updates to the cluster.</span></span><br><span class="line">Reconcile(*clusterv1.Cluster) error</span><br><span class="line"><span class="comment">// Delete the cluster.</span></span><br><span class="line">Delete(*clusterv1.Cluster) error</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>来看下 vsphere 的实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a *Actuator)</span> <span class="title">Reconcile</span><span class="params">(cluster *clusterv1.Cluster)</span> <span class="params">(opErr error)</span></span> &#123;</span><br><span class="line">ctx, err := context.NewClusterContext(&amp;context.ClusterContextParams&#123;</span><br><span class="line">Cluster:          cluster,</span><br><span class="line">Client:           a.client,</span><br><span class="line">CoreClient:       a.coreClient,</span><br><span class="line">ControllerClient: a.controllerClient,</span><br><span class="line">Logger:           klogr.New().WithName(<span class="string">"[cluster-actuator]"</span>),</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">opErr = actuators.PatchAndHandleError(ctx, <span class="string">"Reconcile"</span>, opErr)</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">ctx.Logger.V(<span class="number">6</span>).Info(<span class="string">"reconciling cluster"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := a.reconcilePKI(ctx); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := a.reconcileCloudConfigSecret(ctx); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := a.reconcileReadyState(ctx); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果在 actuator 中遇到需要等待或重新调度的情况，比如目标虚拟机未创建完成，目标集群未 ready 的情况，需要返回 <code>clusterErr.RequeueAfterError</code> ，从而重新调度。比如在配置 secret 时，如果无法获取目标集群的 k8s client，那么表示集群还未配置完成：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(a *Actuator)</span> <span class="title">reconcileCloudConfigSecret</span><span class="params">(ctx *context.ClusterContext)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">client, err := kubeclient.New(ctx)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">ctx.Logger.Error(err, <span class="string">"target cluster is not ready"</span>)</span><br><span class="line"><span class="keyword">return</span> &amp;clusterErr.RequeueAfterError&#123;RequeueAfter: config.DefaultRequeue&#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="machine"><a href="#machine" class="headerlink" title="machine"></a>machine</h2><p>创建流程：</p><img src="/2019/08/16/cluster-api-provider-vsphere-源码阅读/machine_create.png" title="machine_create"><p>删除流程：</p><img src="/2019/08/16/cluster-api-provider-vsphere-源码阅读/machine_delete.png" title="machine_delete"><p>在 v1alpha1 版本中，machine actuator 负责 2件事情：</p><ol><li>虚拟机构建；</li><li>k8s 集群构建（cloud-init 或 ssh）</li></ol><p>在 vsphere 中，创建虚拟机和 k8s 集群构建两件事情时一起完成的，大概流程如下：</p><ol><li>生成配置信息，如证书</li><li>判断目标 k8s 集群是否需要初始化（kubeadm init）</li><li>根据是否需要初始化及是否为 controlplane，生成对应 cloud-init 配置文件</li><li>克隆虚拟机，并将 cloud-init 配置文件作为 <code>guestinfo.userdata</code> 放入虚拟机配置参数中</li><li>虚拟机开机，通过 <code>vmtoolsd</code> 获取 <code>guestinfo.userdata</code> 信息，并作为 cloud-init 参数进行配置</li><li>k8s 集群构建完成（我在实际测试中，发现并没有自动创建 CNI 插件，不知道是不是 bug）</li><li>vsphere 克隆虚拟机为异步任务，提交克隆虚拟机任务后，得到 task id，后续判断虚拟机是否存在会用到；</li></ol><h2 id="Cloud-init-or-SSH"><a href="#Cloud-init-or-SSH" class="headerlink" title="Cloud-init or SSH"></a>Cloud-init or SSH</h2><h3 id="Cloud-init"><a href="#Cloud-init" class="headerlink" title="Cloud-init"></a>Cloud-init</h3><p>在 vsphere provider 中，k8s 部署是通过 cloud-init 实现的，因为之前对 cloud-init 并不了解，这里大概看了下实现方式。</p><p>首先在克隆虚拟机时，给目标虚拟机添加了 <code>guestinfo.userdata</code> 字段作为虚拟机配置参数，这个参数是配置在 Hypervisor 层面的，理论上存在虚拟机隔离性，那么虚拟机内部应该无法感知这个参数，这时候就需要一个工具：vmtools。各个虚拟化平台都会提供一个 vmtools 或类似的工具，用于给虚拟机提供一些高级功能，如：获取 IP、设置主机名、配置 NTP/DNS Server、执行特定命令等等。这些都是根据 vmtools 的实现方式不同支持的功能也不同。</p><p>vmware vmtools 在安装后，vsphere 可以获取到虚拟机的 IP 等信息，同时，在虚拟机内部会安装一些命令，比如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> $ <span class="built_in">which</span> vmtoolsd     </span><br><span class="line">/usr/bin/vmtoolsd</span><br><span class="line">root@yiran-workstation:~/project/cluster-api-provider-vsphere </span><br><span class="line">7d21730f ✔ $ vmware-*</span><br><span class="line">vmware-checkvm             vmware-hgfsclient          vmware-toolbox-cmd         vmware-vgauth-cmd        </span><br><span class="line">vmware-config-tools.pl     vmware-namespace-cmd       vmware-uninstall-tools.pl  vmware-vmblock-fuse      </span><br><span class="line">vmware-guestproxycerttool  vmware-rpctool             vmware-user-suid-wrapper   vmware-xferlogs</span><br></pre></td></tr></table></figure><p>vsphere provider 代码中并没有很明确的给出参数是如何传递的，通过不断的寻找，我找到了这个项目： <a href="https://github.com/vmware/cloud-init-vmware-guestinfo" target="_blank" rel="noopener">https://github.com/vmware/cloud-init-vmware-guestinfo</a> ，在这里我们可以看到获取 Hypervisor 层面虚拟机的额外配置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_guestinfo_value</span><span class="params">(key)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Returns a guestinfo value for the specified key.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    LOG.debug(<span class="string">"Getting guestinfo value for key %s"</span>, key)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        (stdout, stderr) = util.subp(</span><br><span class="line">            [VMTOOLSD, <span class="string">"--cmd"</span>, <span class="string">"info-get guestinfo."</span> + key])</span><br></pre></td></tr></table></figure><p>那么后续的步骤就可以想象的到了，全靠 cloud-init ，这里因为对 cloud-init 了解不多，之后有机会去学习下。</p><h3 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h3><p>SSH 是我们日常远程连接服务器最常用的方式了，通过用户名，密码（或密钥）来进行 SSH 连接，之后都是通过 Shell 脚本的方式实现 k8s 集群部署，当然 Shell 脚本最终调用的命令是 kubeadm。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过阅读 vsphere provider 代码，我们大概了解了实现一个 Cluster-API provider 需要做哪些事情，最主要的资源控制工作都是在 Cluster-API 实现的，而 provider 主要根据各个场景不同，通过 actuator 实现对应工作，代码量和实现方式上也是千差万别。比如 aws 全部代码可能要 2w 行，而 IBM 的加一起可能不到2k，全看各家的重视程度了。</p><p>在写这篇博客的间隙，Cluster-API 开始了 alpha2 的实质性工作： <a href="https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/developer/v1alpha1-compared-to-v1alpha2.md" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/developer/v1alpha1-compared-to-v1alpha2.md</a> ，还没有了解具体的改动，等 beta 之后再看吧。</p><hr><p>吐槽：</p><p>因为 Cluster-API 处于 v1alpha1 阶段，master 分支代码改动非常大，前几天看的代码今天再看就跟记忆中对不上了，之后看代码的时候一定要注意去一个稳定分支上看，免得费时费力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;上一篇博客讲了 Cluster-API 的相关概念，现在我们来找一个 provider 实现看看具体里面做了啥，因为对 vmware 产品中
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Cluster API</title>
    <link href="https://zdyxry.github.io/2019/08/09/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API/"/>
    <id>https://zdyxry.github.io/2019/08/09/Kubernetes-实战-Cluster-API/</id>
    <published>2019-08-09T11:41:49.000Z</published>
    <updated>2019-08-09T11:46:26.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在当前 Kubernetes 生态中，生命周期管理相关工具官方的有 kubeadm、kubespray（部署集群部分通过 kubeadm） ，开源社区还有很多其他的实现，我们可以通过这类工具来实现 k8s 集群的部署，升级，增删节点，但是使用一个工具的前提是：基础设施已经准备完成。只有当基础设施准备完成后，kubeadm 之类工具才可以正常工作。</p><p>当我们在部署 Kubernetes 集群时，节点可能在任何环境上，比如 AWS、OpenStack、Vsphere、Azure 等，那么想要自动化配置基础设施，通常我们根据自己的环境不同，编写不通的代码来支持我们的虚拟化（or 服务器）场景。</p><p>基础设施包括不限于：</p><ul><li>OS 安装</li><li>Load Balance 配置</li><li>网络配置</li><li>IP 分配</li><li>…</li></ul><h2 id="Cluster-API"><a href="#Cluster-API" class="headerlink" title="Cluster-API"></a>Cluster-API</h2><p>Kubernetes 社区针对基础设施问题，发起了一个项目：cluster-api，目前处于 alpha1 版本，项目目标：</p><ol><li>使用声明式API管理 Kubernetes 集群的生命周期</li><li>支持多种环境，私有云或公有云</li><li>使用社区中现有的工具完成相应功能</li><li>…</li></ol><h3 id="功能简述"><a href="#功能简述" class="headerlink" title="功能简述"></a>功能简述</h3><ol><li>无需创建额外基础设施前提下创建 bootstrap cluster</li><li>通过 bootstrap cluster 创建目标 k8s 集群</li></ol><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol><li>cluster-api 使用声明式 API 管理 k8s 集群，需要环境中先存在一个 k8s 集群，通常成为 bootstrap cluster，若不存在，也可通过提供的命令行工具 clusterctl 创建 bootstrap cluster</li><li>在 bootstrap cluster 中，部署 CRD 及相应的 cluster api 控制器及 provider 控制器</li><li>在 bootstrap cluster 中，开始创建我们真正想要创建的资源：k8s 集群<br>创建资源类型为 Cluster、Machine 或 MachineDeployment ，对应的控制器会自动为我们创建好虚拟机</li><li>在虚拟机创建完成后，通过 kubeadm 创建 k8s 集群</li></ol><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><h4 id="虚拟机创建"><a href="#虚拟机创建" class="headerlink" title="虚拟机创建"></a>虚拟机创建</h4><p>目前看到的几个 Cluster-API Provider 项目实现，虚拟机均通过克隆的方式创建出来的。</p><h4 id="虚拟机创建失败处理"><a href="#虚拟机创建失败处理" class="headerlink" title="虚拟机创建失败处理"></a>虚拟机创建失败处理</h4><p>百度：每次等待 30s 查询一次，重试10次，若仍未成功，则创建失败</p><p>腾讯：每 2s 查询一次，若不成功，则一直循环</p><p>vsphere：提交虚拟机创建任务后未检查是否正确创建，未发现重试逻辑</p><p>OpenStack：每 10s 检测一次，若超过设定 timeout ，则创建失败</p><h4 id="虚拟机控制方式"><a href="#虚拟机控制方式" class="headerlink" title="虚拟机控制方式"></a>虚拟机控制方式</h4><p>如果虚拟化平台支持 Cloud-init（或类似功能），后续 k8s 部署通过 Cloud-init 实现；</p><p>若虚拟化平台不支持，则 bootstrap cluster 中的 Pod 通过 ssh 方式进入倒虚拟机内部执行命令。</p><p>通过 Clout-init 实现的好处是Controller 只需要控制虚拟机开机即可，后续无需再主动与虚拟机进行通信，由 Cloud-init 自行触发部署 k8s集群任务；若没有 Cloud-init，则 Controller 在提交虚拟机创建任务后，需要循环等待虚拟机是否正常，等待正常后还需通过 ssh 主动与虚拟机进行连接控制，过于繁琐。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Cluster API 目前还处于 Alpha1 版本，但是已经有很多厂家对其进行适配了，目前看到完成度比较高的公有云是 AWS，私有云是 Vsphere，之后好好读一下 Vsphere 的代码，了解下具体实现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在当前 Kubernetes 生态中，生命周期管理相关工具官方的有 kubeadm、kubespray（部署集群部分通过 kubeadm） 
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>代码统计工具 cloc 基本使用</title>
    <link href="https://zdyxry.github.io/2019/08/09/%E4%BB%A3%E7%A0%81%E7%BB%9F%E8%AE%A1%E5%B7%A5%E5%85%B7-cloc-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>https://zdyxry.github.io/2019/08/09/代码统计工具-cloc-基本使用/</id>
    <published>2019-08-09T11:35:14.000Z</published>
    <updated>2019-08-09T11:35:56.877Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在看一个新项目的时候，通常我都会了解下这个项目的代码量，然后心里给个预期，大概需要多久了解这个项目。</p><h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>在以前，我一般都是使用 <code>find</code> 配合 <code>wc</code> 来完成，比如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@yiran-workstation:/tmp/cloc </span><br><span class="line"> $ find . -name <span class="string">"*.go"</span> | xargs wc -l &#123;&#125; </span><br><span class="line">  81 ./installer.go</span><br><span class="line">  81 total</span><br></pre></td></tr></table></figure><p>显示有一个 <code>installer.go</code> 的文件，一共有 81行。但是这里有个问题，就是 wc 是不会统计代码里面的具体内容的，比如注释、空白行等。</p><p>这时候我们就需要一个更高级的工具了： <code>cloc</code></p><h2 id="cloc"><a href="#cloc" class="headerlink" title="cloc"></a>cloc</h2><p><a href="https://github.com/AlDanial/cloc" target="_blank" rel="noopener">cloc</a> 是一个 Perl 语言实现的项目，用途就像它的名字全称：Count Lines of Code。</p><p>使用方法的话最简单的直接加上项目路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">root@yiran-workstation:~/go/src/github.com/kubermatic/kubeone </span><br><span class="line">master ✗ $ <span class="built_in">pwd</span>        </span><br><span class="line">/root/go/src/github.com/kubermatic/kubeone</span><br><span class="line">root@yiran-workstation:~/go/src/github.com/kubermatic/kubeone </span><br><span class="line">master ✗ $ cloc .                   </span><br><span class="line">     292 text files.</span><br><span class="line">     274 unique files.                                          </span><br><span class="line">      78 files ignored.</span><br><span class="line"></span><br><span class="line">github.com/AlDanial/cloc v 1.70  T=0.80 s (269.1 files/s, 32223.3 lines/s)</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Language                     files          blank        comment           code</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Go                             122           2861           2791          14374</span><br><span class="line">Markdown                        38           1003              0           3172</span><br><span class="line">YAML                            41             14            158            524</span><br><span class="line">Bourne Shell                     5             44             89            206</span><br><span class="line">Python                           4             66             76            186</span><br><span class="line">XML                              4              0              0            100</span><br><span class="line">make                             1             17             13             52</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">SUM:                           215           4005           3127          18614</span><br><span class="line">-------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>可以看到统计结果，其中默认会按照语言分类。</p><p>如果是 Golang 的项目，且使用了 vendor ，那么可以通过 <code>--exclude-dir</code> 来过滤掉某些路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">aster ✗ $ cloc . </span><br><span class="line">    3304 text files.</span><br><span class="line">    3170 unique files.                                          </span><br><span class="line">     429 files ignored.</span><br><span class="line"></span><br><span class="line">github.com/AlDanial/cloc v 1.70  T=12.40 s (232.7 files/s, 101850.0 lines/s)</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">Language                      files          blank        comment           code</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">Go                             2501         101422         113437        1007168</span><br><span class="line">Markdown                        108           2688              0           6786</span><br><span class="line">Protocol Buffers                 53           3006           9076           5133</span><br><span class="line">YAML                            108            305            377           4032</span><br><span class="line">Bourne Shell                     38            381            839           1897</span><br><span class="line">JSON                              7              0              0           1748</span><br><span class="line">Assembly                         35            271            320           1603</span><br><span class="line">Python                            5            277            113            588</span><br><span class="line">make                             23            165            162            556</span><br><span class="line">Bourne Again Shell                3             45             56            378</span><br><span class="line">XML                               4              0              0            106</span><br><span class="line">C                                 1              8              7             24</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">SUM:                           2886         108568         124387        1030019</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">root@yiran-workstation:~/go/src/github.com/kubernetes-sigs/cluster-api-provider-vsphere </span><br><span class="line">master ✗ $ ls    </span><br><span class="line">boilerplate.go.txt  cmd                 config           Dockerfile  go.mod  hack     Makefile  OWNERS_ALIASES  PROJECT    RELEASE.md  SECURITY_CONTACTS</span><br><span class="line">build               code-of-conduct.md  CONTRIBUTING.md  docs        go.sum  LICENSE  OWNERS    pkg             README.md  scripts     vendor</span><br><span class="line">root@yiran-workstation:~/go/src/github.com/kubernetes-sigs/cluster-api-provider-vsphere </span><br><span class="line">master ✗ $ cloc . --exclude-dir=vendor</span><br><span class="line">     234 text files.</span><br><span class="line">     234 unique files.                                          </span><br><span class="line">      73 files ignored.</span><br><span class="line"></span><br><span class="line">github.com/AlDanial/cloc v 1.70  T=0.55 s (294.2 files/s, 26466.9 lines/s)</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Language                     files          blank        comment           code</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Go                              64           1110           1355           5106</span><br><span class="line">YAML                            37            150            302           1907</span><br><span class="line">Bourne Shell                    22            256            475           1125</span><br><span class="line">Markdown                        17            332              0            895</span><br><span class="line">Python                           5            277            113            588</span><br><span class="line">make                             9             57            112            177</span><br><span class="line">JSON                             4              0              0            131</span><br><span class="line">XML                              4              0              0            106</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">SUM:                           162           2182           2357          10035</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">root@yiran-workstation:~/go/src/github.com/kubernetes-sigs/cluster-api-provider-vsphere </span><br><span class="line">master ✗ $</span><br></pre></td></tr></table></figure><p>有时候仅仅按照语言分类还不够，我们想看到具体的那些文件代码量比较大，可以使用 <code>--by-file</code> 参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">root@yiran-workstation:~/go/src/github.com/kubermatic/kubeone </span><br><span class="line">master ✗ $ cloc . --by-file |head -n 30</span><br><span class="line">     292 text files.</span><br><span class="line">     274 unique files.                                          </span><br><span class="line">      78 files ignored.</span><br><span class="line"></span><br><span class="line">github.com/AlDanial/cloc v 1.70  T=0.70 s (307.5 files/s, 36823.9 lines/s)</span><br><span class="line">--------------------------------------------------------------------------------------------------------------------</span><br><span class="line">File                                                                             blank        comment           code</span><br><span class="line">--------------------------------------------------------------------------------------------------------------------</span><br><span class="line">./pkg/templates/machinecontroller/deployment.go                                     52             24            762</span><br><span class="line">./pkg/apis/kubeone/v1alpha1/zz_generated.conversion.go                              86             54            614</span><br><span class="line">./pkg/apis/kubeone/validation/validation_test.go                                    22             12            530</span><br><span class="line">./pkg/cmd/config.go                                                                 81             42            477</span><br><span class="line">./pkg/apis/kubeadm/v1beta2/zz_generated.deepcopy.go                                 53             60            443</span><br><span class="line">./pkg/apis/kubeadm/v1beta1/zz_generated.deepcopy.go                                 53             60            439</span><br><span class="line">./pkg/templates/weave/weave-net.go                                                  32             13            433</span><br><span class="line">./pkg/config/cluster.go                                                             80             69            417</span><br><span class="line">./pkg/terraform/config.go                                                           76             30            368</span><br><span class="line">./pkg/templates/canal/daemonset.go                                                   9             42            344</span><br><span class="line">./pkg/yamled/document_test.go                                                      113             15            344</span><br><span class="line">./pkg/apis/kubeone/v1alpha1/zz_generated.deepcopy.go                                47             53            340</span><br><span class="line">./pkg/apis/kubeone/zz_generated.deepcopy.go                                         47             53            340</span><br><span class="line">./pkg/templates/machinecontroller/webhook.go                                        35             22            332</span><br><span class="line">./pkg/installer/installation/prerequisites.go                                       73             21            281</span><br><span class="line">./pkg/templates/canal/prerequisites.go                                              16             40            257</span><br><span class="line">./pkg/upgrader/upgrade/preflight_checks.go                                          44             30            247</span><br><span class="line">./pkg/yamled/document.go                                                            70             31            240</span><br><span class="line">./pkg/upgrader/upgrade/preflight_checks_test.go                                     14             12            237</span><br><span class="line">./pkg/templates/metricsserver/deployment.go                                         22             13            227</span><br><span class="line">./pkg/templates/externalccm/packet.go                                               20             12            222</span><br><span class="line">./docs/quickstart-vsphere.md                                                        59              0            222</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在看一个新项目的时候，通常我都会了解下这个项目的代码量，然后心里给个预期，大概需要多久了解这个项目。&lt;/p&gt;
&lt;h2 id=&quot;wc&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="tools" scheme="https://zdyxry.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>Linux 下磁盘设备自动发现方式</title>
    <link href="https://zdyxry.github.io/2019/08/02/Linux-%E4%B8%8B%E7%A3%81%E7%9B%98%E8%AE%BE%E5%A4%87%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E6%96%B9%E5%BC%8F/"/>
    <id>https://zdyxry.github.io/2019/08/02/Linux-下磁盘设备自动发现方式/</id>
    <published>2019-08-02T15:10:04.000Z</published>
    <updated>2019-08-02T15:11:01.335Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>如果在 PC 上安装过 Linux，那么通常会遇到过硬件设备无法发现的问题，这类问题最终都可以通过 google 来解决掉。那么当我们在服务器场景下，如何做到设备自动发现且在设备发现后执行某些动作呢？</p><p>最近看了几个关于存储系统的 Operator 部分实现，记录一下。</p><h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><p>最简单的肯定是我们写一个循环，永远检测我们要发现的设备，比如 lsblk 可以列举当前服务器所有 block 设备，那么我们就在循环内部执行 lsblk，diff 每次执行的结果，如果有新的设备，那么执行某些操作。</p><p>lsblk 是通过读取 /sys/block 下的具体目录判断的，那么我么也可以直接读取该路径下的目录来实现。</p><p>如果是网络设备也是一样，我们可以在循环内部执行 <code>ip link list</code> 来获取所有网络设备。</p><h2 id="UDEV"><a href="#UDEV" class="headerlink" title="UDEV"></a>UDEV</h2><p>照常先引用维基百科的解释：</p><blockquote><p>udev 是Linux kernel 2.6系列的设备管理器。它主要的功能是管理/dev目錄底下的设备节点。它同时也是用来接替devfs及hotplug的功能，这意味着它要在添加/删除硬件时处理/dev目录以及所有用户空间的行为，包括加载firmware时。</p></blockquote><p>如果你的 OS 是通过 systemd 来管理所有进程的话，那么可以发现一个服务叫做 <code>systemd-udevd</code> ，这个是 udev 的守护进程:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 19:58:10 ~]<span class="variable">$systemctl</span> status systemd-udevd</span><br><span class="line">● systemd-udevd.service - udev Kernel Device Manager</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/systemd-udevd.service; static; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-06-14 15:25:55 CST; 1 months 18 days ago</span><br><span class="line">     Docs: man:systemd-udevd.service(8)</span><br><span class="line">           man:udev(7)</span><br><span class="line"> Main PID: 698 (systemd-udevd)</span><br><span class="line">   Status: <span class="string">"Processing with 56 children at max"</span></span><br><span class="line">    Tasks: 1</span><br><span class="line">   Memory: 24.0M</span><br><span class="line">   CGroup: /system.slice/systemd-udevd.service</span><br><span class="line">           └─698 /usr/lib/systemd/systemd-udevd</span><br><span class="line"></span><br><span class="line">Aug 02 10:29:17 node90 python[4395]: detected unhandled Python exception <span class="keyword">in</span> <span class="string">'/usr/lib/python2.7/site-packages/cpuinfo/cpuinfo.py'</span></span><br><span class="line">Aug 02 10:29:17 node90 python[4395]: can<span class="string">'t communicate with ABRT daemon, is it running? [Errno 2] No such file or directory</span></span><br><span class="line"><span class="string">Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.</span></span><br></pre></td></tr></table></figure><p>udev 可以让我们对硬件的使用限制大大减少，除了常见的硬件发现，还有一个场景就是网卡改名，比如 82599 网卡，在 CentOS 上大概率识别为 <code>enp4s0f1</code> 之类的网卡名，如果我们想要统一服务器网卡名称，那么我们可以通过设置 udev 规则，匹配 mac 地址来做到，这里不细说。</p><p>我们来说说 udev 自动发现设备。udev 提供了完整的工具集，可以共我们使用，比如 udevadm：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node 20:02:41 ~]<span class="variable">$udevadm</span> --<span class="built_in">help</span></span><br><span class="line">udevadm [--<span class="built_in">help</span>] [--version] [--debug] COMMAND [COMMAND OPTIONS]</span><br><span class="line"></span><br><span class="line">Send control commands or <span class="built_in">test</span> the device manager.</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">  info          Query sysfs or the udev database</span><br><span class="line">  trigger       Request events from the kernel</span><br><span class="line">  settle        Wait <span class="keyword">for</span> pending udev events</span><br><span class="line">  control       Control the udev daemon</span><br><span class="line">  monitor       Listen to kernel and udev events</span><br><span class="line">  <span class="built_in">test</span>          Test an event run</span><br><span class="line">  <span class="built_in">test</span>-builtin  Test a built-in <span class="built_in">command</span></span><br></pre></td></tr></table></figure><p>我们可以通过 udevadm 来查看硬件设备的具体信息，也可以通过 udevadm 来进行显示的设备监控。</p><p>除了通过 udevadm 命令，我们还可以通过编写 udev 配置文件来实现设备发现后的具体动作，在 <code>/etc/udev/rules.d/</code> 路径下可以防止我们自己的配置文件。</p><p>比如我们想实现功能：节点上插入磁盘后，执行某条命令，那么我们可以这么定义：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node90 20:05:21 rules.d]<span class="variable">$pwd</span></span><br><span class="line">/etc/udev/rules.d</span><br><span class="line">[root@node 20:06:29 rules.d]<span class="variable">$cat</span> 98-disk-udev.rules </span><br><span class="line">KERNEL==<span class="string">"sd[a-z]"</span>, SUBSYSTEM==<span class="string">"block"</span>, ACTION==<span class="string">"add"</span>, RUN+=<span class="string">"echo add"</span></span><br><span class="line"></span><br><span class="line">KERNEL==<span class="string">"sd[a-z]"</span>, SUBSYSTEM==<span class="string">"block"</span>, ACTION==<span class="string">"remove"</span>, RUN+=<span class="string">"echo remove"</span></span><br></pre></td></tr></table></figure><p>这个规则具体含义为：</p><ul><li><p>当检测到设备 <code>sd[a-z]</code> , 类型为 <code>block</code>且动作为 <code>add</code> ，那么执行 <code>echo add</code> 操作。</p></li><li><p>当检测到设备 <code>sd[a-z]</code> ，类型为 <code>block</code>且动作为 <code>remove</code> ，那么执行 <code>echo add</code> 操作。</p></li></ul><p>我们来看一个具体的例子，执行 <code>udevadm monito</code> 来监控节点设备，然后插入一块 scsi 磁盘：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@node 20:14:04 rules.d]<span class="variable">$udevadm</span> monitor</span><br><span class="line">monitor will <span class="built_in">print</span> the received events <span class="keyword">for</span>:</span><br><span class="line">UDEV - the event <span class="built_in">which</span> udev sends out after rule processing</span><br><span class="line">KERNEL - the kernel uevent</span><br><span class="line">KERNEL[4250923.838205] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0 (scsi)</span><br><span class="line">KERNEL[4250923.838232] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0 (scsi)</span><br><span class="line">KERNEL[4250923.838241] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_disk/6:0:0:0 (scsi_disk)</span><br><span class="line">KERNEL[4250923.838248] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_device/6:0:0:0 (scsi_device)</span><br><span class="line">KERNEL[4250923.838344] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_generic/sg6 (scsi_generic)</span><br><span class="line">KERNEL[4250923.838355] add      /devices/virtual/bdi/8:96 (bdi)</span><br><span class="line">KERNEL[4250923.838364] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/bsg/6:0:0:0 (bsg)</span><br><span class="line">UDEV  [4250923.844166] add      /devices/virtual/bdi/8:96 (bdi)</span><br><span class="line">UDEV  [4250923.844180] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0 (scsi)</span><br><span class="line">UDEV  [4250923.844189] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0 (scsi)</span><br><span class="line">UDEV  [4250923.844196] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_device/6:0:0:0 (scsi_device)</span><br><span class="line">UDEV  [4250923.844203] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_disk/6:0:0:0 (scsi_disk)</span><br><span class="line">UDEV  [4250923.848018] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/scsi_generic/sg6 (scsi_generic)</span><br><span class="line">UDEV  [4250923.851041] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/bsg/6:0:0:0 (bsg)</span><br><span class="line">KERNEL[4250923.882845] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/block/sdg (block)</span><br><span class="line">KERNEL[4250923.882863] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/block/sdg/sdg1 (block)</span><br><span class="line">UDEV  [4250928.290659] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/block/sdg (block)</span><br><span class="line">UDEV  [4250928.379634] add      /devices/pci0000:00/0000:00:1f.2/ata6/host6/target6:0:0/6:0:0:0/block/sdg/sdg1 (block)</span><br></pre></td></tr></table></figure><p>可以看到 udev 监控到了设备名称，设备 pci id 以及设备触发的动作，这里与我们定义的规则相对应。</p><h2 id="具体使用"><a href="#具体使用" class="headerlink" title="具体使用"></a>具体使用</h2><h3 id="命令行-1"><a href="#命令行-1" class="headerlink" title="命令行"></a>命令行</h3><p>在 OpenShift 的 local storage operator 中，是通过不断的执行 lsblk 比较结果来判断的，相关代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run and create disk config</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(d *DiskMaker)</span> <span class="title">Run</span><span class="params">(stop &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">ticker := time.NewTicker(checkDuration)</span><br><span class="line"><span class="keyword">defer</span> ticker.Stop()</span><br><span class="line"></span><br><span class="line">err := os.MkdirAll(d.symlinkLocation, <span class="number">0755</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Errorf(<span class="string">"error creating local-storage directory %s: %v"</span>, d.symlinkLocation, err)</span><br><span class="line">os.Exit(<span class="number">-1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-ticker.C:</span><br><span class="line">diskConfig, err := d.loadConfig()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Errorf(<span class="string">"error loading configuration: %v"</span>, err)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">d.symLinkDisks(diskConfig)</span><br><span class="line"><span class="keyword">case</span> &lt;-stop:</span><br><span class="line">klog.Infof(<span class="string">"exiting, received message on stop channel"</span>)</span><br><span class="line">os.Exit(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(d *DiskMaker)</span> <span class="title">symLinkDisks</span><span class="params">(diskConfig *DiskConfig)</span></span> &#123;</span><br><span class="line">cmd := exec.Command(<span class="string">"lsblk"</span>, <span class="string">"--list"</span>, <span class="string">"-o"</span>, <span class="string">"NAME,MOUNTPOINT"</span>, <span class="string">"--noheadings"</span>)</span><br><span class="line"><span class="keyword">var</span> out bytes.Buffer</span><br><span class="line"><span class="keyword">var</span> err error</span><br><span class="line">cmd.Stdout = &amp;out</span><br><span class="line">err = cmd.Run()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"error running lsblk: %v"</span>, err)</span><br><span class="line">e := newEvent(ErrorRunningBlockList, msg, <span class="string">""</span>)</span><br><span class="line">d.eventSync.report(e, d.localVolume)</span><br><span class="line">klog.Errorf(msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">deviceSet, err := d.findNewDisks(out.String())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"error reading blocklist: %v"</span>, err)</span><br><span class="line">e := newEvent(ErrorReadingBlockList, msg, <span class="string">""</span>)</span><br><span class="line">d.eventSync.report(e, d.localVolume)</span><br><span class="line">klog.Errorf(msg)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(deviceSet) == <span class="number">0</span> &#123;</span><br><span class="line">klog.V(<span class="number">3</span>).Infof(<span class="string">"unable to find any new disks"</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDEV-1"><a href="#UDEV-1" class="headerlink" title="UDEV"></a>UDEV</h3><p>在 Rook 项目中，除了通过 lsblk 来获取设备，还监控了 udev 规则，具体代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// Scans `udevadm monitor` output for block sub-system events. Each line of</span></span><br><span class="line"><span class="comment">// output matching a set of substrings is sent to the provided channel. An event</span></span><br><span class="line"><span class="comment">// is returned if it passes any matches tests, and passes all exclusion tests.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">rawUdevBlockMonitor</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">string</span>, matches, exclusions []<span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> <span class="built_in">close</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment">// stdbuf -oL performs line bufferred output</span></span><br><span class="line"><span class="comment">// 后台执行 udevadm monitor 命令</span></span><br><span class="line">cmd := exec.Command(<span class="string">"stdbuf"</span>, <span class="string">"-oL"</span>, <span class="string">"udevadm"</span>, <span class="string">"monitor"</span>, <span class="string">"-u"</span>, <span class="string">"-k"</span>, <span class="string">"-s"</span>, <span class="string">"block"</span>)</span><br><span class="line">stdout, err := cmd.StdoutPipe()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Warningf(<span class="string">"Cannot open udevadm stdout: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">err = cmd.Start()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Warningf(<span class="string">"Cannot start udevadm monitoring: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过 bufio.NewScanner 实时获取 udevadm monitor 命令输出结果</span></span><br><span class="line">scanner := bufio.NewScanner(stdout)</span><br><span class="line"><span class="keyword">for</span> scanner.Scan() &#123;</span><br><span class="line">text := scanner.Text()</span><br><span class="line">logger.Debugf(<span class="string">"udevadm monitor: %s"</span>, text)</span><br><span class="line"><span class="comment">// 对输出结果进行正则匹配</span></span><br><span class="line">match, err := matchUdevEvent(text, matches, exclusions)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Warningf(<span class="string">"udevadm filtering failed: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> match &#123;</span><br><span class="line">    <span class="comment">// 若匹配成功，则将结果发送到对应 channel，即 events</span></span><br><span class="line">c &lt;- text</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err := scanner.Err(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">logger.Warningf(<span class="string">"udevadm monitor scanner error: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">logger.Info(<span class="string">"udevadm monitor finished"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Monitors udev for block device changes, and collapses these events such that</span></span><br><span class="line"><span class="comment">// only one event is emitted per period in order to deal with flapping.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">udevBlockMonitor</span><span class="params">(c <span class="keyword">chan</span> <span class="keyword">string</span>, period time.Duration)</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> <span class="built_in">close</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment">// return any add or remove events, but none that match device mapper</span></span><br><span class="line"><span class="comment">// events. string matching is case-insensitve</span></span><br><span class="line"><span class="comment">// 定义 events channel，用于传输匹配结果</span></span><br><span class="line">events := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">go</span> rawUdevBlockMonitor(events,</span><br><span class="line">    <span class="comment">// 正则表达式，用于获取设备名称</span></span><br><span class="line">[]<span class="keyword">string</span>&#123;<span class="string">"(?i)add"</span>, <span class="string">"(?i)remove"</span>&#125;,</span><br><span class="line">[]<span class="keyword">string</span>&#123;<span class="string">"(?i)dm-[0-9]+"</span>, <span class="string">"(?i)rbd[0-9]+"</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">    <span class="comment">// 死循环，不断从 events channel 中获取匹配后的结果</span></span><br><span class="line">event, ok := &lt;-events</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">timeout := time.NewTimer(period)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">    <span class="comment">// 这里如果在 timeout 时间周期内，出现多次 events，貌似会丢弃掉后续 event 信息，不知道出于什么考虑。。</span></span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-timeout.C:</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="keyword">case</span> _, ok := &lt;-events:</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">c &lt;- event</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="http://www.reactivated.net/writing_udev_rules.html" target="_blank" rel="noopener">http://www.reactivated.net/writing_udev_rules.html</a> </li><li><a href="https://www.thegeekdiary.com/beginners-guide-to-udev-in-linux/" target="_blank" rel="noopener">https://www.thegeekdiary.com/beginners-guide-to-udev-in-linux/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;如果在 PC 上安装过 Linux，那么通常会遇到过硬件设备无法发现的问题，这类问题最终都可以通过 google 来解决掉。那么当我们在服务
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
      <category term="Golang" scheme="https://zdyxry.github.io/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-平滑移除节点</title>
    <link href="https://zdyxry.github.io/2019/08/01/Kubernetes-%E5%AE%9E%E6%88%98-%E5%B9%B3%E6%BB%91%E7%A7%BB%E9%99%A4%E8%8A%82%E7%82%B9/"/>
    <id>https://zdyxry.github.io/2019/08/01/Kubernetes-实战-平滑移除节点/</id>
    <published>2019-08-01T15:07:29.000Z</published>
    <updated>2019-08-02T15:08:09.794Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>自己玩 K8S 以来，搭建的环境没有10几套，也有5，6套了，当环境测试完成后，基本上直接删除掉了，也没有想着一直维护，最近在维护一个集群的时候，想要删除一个节点，发现自己一直不知道如何删除节点，特此记录。</p><h2 id="平滑移除"><a href="#平滑移除" class="headerlink" title="平滑移除"></a>平滑移除</h2><h3 id="获取节点列表"><a href="#获取节点列表" class="headerlink" title="获取节点列表"></a>获取节点列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br></pre></td></tr></table></figure><h3 id="设置不可调度"><a href="#设置不可调度" class="headerlink" title="设置不可调度"></a>设置不可调度</h3><p>由于节点目前处于正常工作状态，集群中新建资源还是有可能创建到该节点的，所以先将节点设置为不可调度：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl cordon <span class="variable">$node_name</span></span><br></pre></td></tr></table></figure><h3 id="将节点上资源调度到其他节点"><a href="#将节点上资源调度到其他节点" class="headerlink" title="将节点上资源调度到其他节点"></a>将节点上资源调度到其他节点</h3><p>目前集群已经不会分配新的资源在该节点上了，但是节点还运行着现有的业务，所以我们需要将节点上的业务分配到其他节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain <span class="variable">$node_name</span></span><br></pre></td></tr></table></figure><p>注意：DaemonSet Pod 和 Static Pod 是不会在集群中其他节点重建的。</p><h3 id="移除节点"><a href="#移除节点" class="headerlink" title="移除节点"></a>移除节点</h3><p>当前集群中已经没有任何资源分配在节点上了，那么我们可以直接移除节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete <span class="variable">$node_name</span></span><br></pre></td></tr></table></figure><p>至此，我们平滑移除了一个 k8s 节点。如果移除的是一个 master 节点，那么记得之后还要添加一个新的 master 节点到集群中，避免集群可靠性降低。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes" target="_blank" rel="noopener">https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;自己玩 K8S 以来，搭建的环境没有10几套，也有5，6套了，当环境测试完成后，基本上直接删除掉了，也没有想着一直维护，最近在维护一个集群的
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Heap In Python &amp; Golang</title>
    <link href="https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/"/>
    <id>https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/</id>
    <published>2019-07-28T04:11:45.000Z</published>
    <updated>2019-07-28T04:16:47.995Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近使用到了 heap 这个数据结构，记录一下在 Python 和 Golang 中最基本的使用方法～ </p><blockquote><p>堆（英语：Heap）是计算机科学中的一種特別的樹狀数据结构。若是滿足以下特性，即可稱為堆積：「給定堆積中任意節點P和C，若P是C的母節點，那麼P的值會小於等於（或大於等於）C的值」。若母節點的值恆小於等於子節點的值，此堆積稱為最小堆積（min heap）；反之，若母節點的值恆大於等於子節點的值，此堆積稱為最大堆積（max heap）。在堆積中最頂端的那一個節點，稱作根節點（root node），根節點本身沒有母節點（parent node）。</p></blockquote><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="create"><a href="#create" class="headerlink" title="create"></a>create</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: heapq.heapify(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: a</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: b = []</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: heapq.heappu</span><br><span class="line">heapq.heappush     heapq.heappushpop</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: heapq.heappush(b, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: heapq.heappush(b, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: heapq.heappush(b, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: heapq.heappush(b, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: heapq.heappush(b, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: b</span><br><span class="line">Out[<span class="number">11</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h3 id="read"><a href="#read" class="headerlink" title="read"></a>read</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">28</span>]: b</span><br><span class="line">Out[<span class="number">28</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: heapq.nlargest(<span class="number">3</span>, b)</span><br><span class="line">Out[<span class="number">29</span>]: [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: heapq.nsmallest(<span class="number">2</span>, b)</span><br><span class="line">Out[<span class="number">30</span>]: [<span class="number">1</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><h3 id="update"><a href="#update" class="headerlink" title="update"></a>update</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">32</span>]: heapq.heappush(b, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: b</span><br><span class="line">Out[<span class="number">33</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: heapq.heapreplace(b, <span class="number">7</span>)</span><br><span class="line">Out[<span class="number">39</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: b</span><br><span class="line">Out[<span class="number">40</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: a = [<span class="number">6</span>,<span class="number">8</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">42</span>]: heapq.heapify(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: b</span><br><span class="line">Out[<span class="number">43</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: c = heapq.merge(a,b)</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: list(c)</span><br><span class="line">Out[<span class="number">46</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">50</span>]: b</span><br><span class="line">Out[<span class="number">50</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">51</span>]: heapq.heappop(b)</span><br><span class="line">Out[<span class="number">51</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: b</span><br><span class="line">Out[<span class="number">52</span>]: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><h2 id="Golang"><a href="#Golang" class="headerlink" title="Golang"></a>Golang</h2><p>Golang 中没有 heapq 这种封装好的库可以直接使用，不过有 <code>container/heap</code> ，提供了同样的方法，只是我们需要先对我们的操作对象实现搜有的 <code>heap.Interface</code>  方法。</p><h2 id="Constructor"><a href="#Constructor" class="headerlink" title="Constructor"></a>Constructor</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// An IntHeap is a min-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> IntHeap []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Len</span><span class="params">()</span> <span class="title">int</span></span>           &#123; <span class="keyword">return</span> <span class="built_in">len</span>(h) &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h[i] &lt; h[j] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Swap</span><span class="params">(i, j <span class="keyword">int</span>)</span></span>      &#123; h[i], h[j] = h[j], h[i] &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Push</span><span class="params">(x <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="comment">// Push and Pop use pointer receivers because they modify the slice's length,</span></span><br><span class="line"><span class="comment">// not just its contents.</span></span><br><span class="line">*h = <span class="built_in">append</span>(*h, x.(<span class="keyword">int</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取最后一个元素，在 `container/heap.Pop` 中，将堆顶的元素放置在最后，然后调用 `container/heap.Down` 将当前堆顶元素下沉到适当位置。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Pop</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">old := *h</span><br><span class="line">n := <span class="built_in">len</span>(old)</span><br><span class="line">x := old[n<span class="number">-1</span>]</span><br><span class="line">*h = old[<span class="number">0</span> : n<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// // An IntHeap is a max-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> MaxHeap <span class="keyword">struct</span> &#123;</span><br><span class="line">IntHeap</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h MaxHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h.IntHeap[i] &gt; h.IntHeap[j] &#125;</span><br></pre></td></tr></table></figure><h3 id="create-1"><a href="#create-1" class="headerlink" title="create"></a>create</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// This example inserts several ints into an IntHeap, checks the minimum,</span></span><br><span class="line"><span class="comment">// and removes them in order of priority.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"minimum: %d\n"</span>, (*h)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="read-1"><a href="#read-1" class="headerlink" title="read"></a>read</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"minimum: %d\n"</span>, (*h)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">h1 := &amp;MaxHeap&#123;[]<span class="keyword">int</span>&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;&#125;</span><br><span class="line">heap.Init(h1)</span><br><span class="line">heap.Push(h1, <span class="number">3</span>)</span><br><span class="line">fmt.Printf(<span class="string">"maximum: %d\n"</span>, h1.IntHeap[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> h1.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h1))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="update-1"><a href="#update-1" class="headerlink" title="update"></a>update</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="delete-1"><a href="#delete-1" class="headerlink" title="delete"></a>delete</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This example demonstrates an integer heap built using the heap interface.</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"container/heap"</span></span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// An IntHeap is a min-heap of ints.</span></span><br><span class="line"><span class="keyword">type</span> IntHeap []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Len</span><span class="params">()</span> <span class="title">int</span></span>           &#123; <span class="keyword">return</span> <span class="built_in">len</span>(h) &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Less</span><span class="params">(i, j <span class="keyword">int</span>)</span> <span class="title">bool</span></span> &#123; <span class="keyword">return</span> h[i] &lt; h[j] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h IntHeap)</span> <span class="title">Swap</span><span class="params">(i, j <span class="keyword">int</span>)</span></span>      &#123; h[i], h[j] = h[j], h[i] &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Push</span><span class="params">(x <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="comment">// Push and Pop use pointer receivers because they modify the slice's length,</span></span><br><span class="line"><span class="comment">// not just its contents.</span></span><br><span class="line">*h = <span class="built_in">append</span>(*h, x.(<span class="keyword">int</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(h *IntHeap)</span> <span class="title">Pop</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line">old := *h</span><br><span class="line">n := <span class="built_in">len</span>(old)</span><br><span class="line">x := old[n<span class="number">-1</span>]</span><br><span class="line">*h = old[<span class="number">0</span> : n<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This example inserts several ints into an IntHeap, checks the minimum,</span></span><br><span class="line"><span class="comment">// and removes them in order of priority.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">h := &amp;IntHeap&#123;<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>&#125;</span><br><span class="line">heap.Init(h)</span><br><span class="line">heap.Push(h, <span class="number">3</span>)</span><br><span class="line">fmt.Println(h)</span><br><span class="line">heap.Remove(h, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> h.Len() &gt; <span class="number">0</span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">"%d "</span>, heap.Pop(h))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Heap 的使用场景有：优先级队列、TopK 等等。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://godoc.org/container/heap" target="_blank" rel="noopener">https://godoc.org/container/heap</a></li><li><a href="https://docs.python.org/3.7/library/heapq.html" target="_blank" rel="noopener">https://docs.python.org/3.7/library/heapq.html</a></li><li><a href="https://ieevee.com/tech/2018/01/29/go-heap.html" target="_blank" rel="noopener">https://ieevee.com/tech/2018/01/29/go-heap.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近使用到了 heap 这个数据结构，记录一下在 Python 和 Golang 中最基本的使用方法～ &lt;/p&gt;
&lt;blockquote&gt;

      
    
    </summary>
    
    
      <category term="Python" scheme="https://zdyxry.github.io/tags/Python/"/>
    
      <category term="Golang" scheme="https://zdyxry.github.io/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>CentOS定制-软件源错误</title>
    <link href="https://zdyxry.github.io/2019/07/21/CentOS%E5%AE%9A%E5%88%B6-%E8%BD%AF%E4%BB%B6%E6%BA%90%E9%94%99%E8%AF%AF/"/>
    <id>https://zdyxry.github.io/2019/07/21/CentOS定制-软件源错误/</id>
    <published>2019-07-21T11:58:43.000Z</published>
    <updated>2019-07-21T12:00:33.172Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无法安装 OS 的问题，记录一下解决方式。</p><h2 id="定制-OS"><a href="#定制-OS" class="headerlink" title="定制 OS"></a>定制 OS</h2><p>关于定制 OS，在之前的博客中已经提到过几次了，CentOS 是比较容易改动的一个发行版，因为有着 RHEL 红（爸）帽（爸），有着完善的文档可以参考。</p><p>主要需要注意的是两点：  </p><ol><li>分区方式</li><li>软件包选择</li></ol><p>今天遇到的问题是第二点。</p><p>先说下前提，由于是 2B 产品，所以对于每次的 BaseOS 版本升级都非常谨慎，每次 BaseOS 版本都会进行各种测试。但是如果仅仅是升级部分所需要的软件包，就不用这么麻烦了，我们可以定制自己所需要的软件组（group），来进行安装/升级。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这次接到的需要是升级 Kernel、libiscsi、qemu 三个软件，后两个是虚拟化相关的，相关依赖较少；kernel 是跟 BaseOS 版本关联性很大的。</p><p>比如 CentOS 7.6 中，kernel 版本为：kernel-3.10.0-957.el7.x86_64.rpm，这个版本对 selinux 等相关软件是有依赖要求的，我在这里翻车了。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>像往常一样，将对应的 rpm 放置到了对应的 yum 源中，更新 yum 源，制作 ISO，在安装过程中报错：</p><img src="/2019/07/21/CentOS定制-软件源错误/os1.png" title="OS1"><p>报错显示是软件源出了问题，但是没有更多的信息了，这时候我们可以通过 console 连接到其他的 pty 中，查看对应的日志，比如 CentOS 默认的日志在： <code>/tmp/packaging.log</code> 中：</p><img src="/2019/07/21/CentOS定制-软件源错误/os2.png" title="OS2"><p>我们可以看到日志中提示 kernel 与当前软件源中的 selinux-policy-targeted 冲突，因为安装 OS 所用的软件源就是 ISO ，所以这里肯定是我们打包 ISO 时遗漏了依赖关系导致的，我们将对应的 Kernel 所需依赖更新，重新构建 ISO 就可以了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-踩坑记录（持续更新）</title>
    <link href="https://zdyxry.github.io/2019/07/13/Kubernetes-%E5%AE%9E%E6%88%98-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/"/>
    <id>https://zdyxry.github.io/2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/</id>
    <published>2019-07-13T01:34:45.000Z</published>
    <updated>2019-07-18T22:59:27.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更新。</p><h3 id="更新记录"><a href="#更新记录" class="headerlink" title="更新记录"></a>更新记录</h3><ul><li>2019.07.13 02:00 来自加班中的 yiran</li><li>2019.07.19 06:52 早起不想去公司的 yiran</li></ul><h2 id="coredns-无法解析域名"><a href="#coredns-无法解析域名" class="headerlink" title="coredns 无法解析域名"></a>coredns 无法解析域名</h2><p>在 Kubernetes 环境中，使用 kubeadm 工具部署的集群，会自动部署 coredns 作为集群的域名服务，每当我们创建了自己的 service，都可以通过域名直接访问，不用再考虑自己多个 Pod 的 IP 不同如何连接的问题。</p><p>最近遇到多个环境出现无法解析域名的问题，具体现象如下：</p><ol><li>集群部署完成后，部署 daemonset 资源，每个节点均运行一个 busybox；</li><li>在 busybox 中对 <code>kubernetes</code> 默认域名进行解析，查看解析结果。</li></ol><p>正常情况应该是所有的 busybox 都可以正常解析才对，但是最近几个环境中均出现了 3 个node 中1个node 上的 pod 无法解析的问题，示例代码如下：</p><p>daemonset.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:1.28.4"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 21:28:40 ~]<span class="variable">$for</span> i <span class="keyword">in</span> `kubectl get pod  -o wide  |grep ds | awk <span class="string">'&#123;print $1&#125;'</span>`;<span class="keyword">do</span> kubectl <span class="built_in">exec</span> <span class="variable">$i</span> nslookup kubernetes;<span class="built_in">echo</span> ;<span class="keyword">done</span></span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10</span><br><span class="line"></span><br><span class="line">nslookup: can<span class="string">'t resolve '</span>kubernetes<span class="string">'</span></span><br><span class="line"><span class="string">command terminated with exit code 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br></pre></td></tr></table></figure><p>在第一个节点的 Pod 解析时失效，最后命令执行 1min 超时退出。</p><p>经过查看发现节点的 NetFilter 相关系统配置未生效，导致 iptables 相关功能失效，具体可以参考 <a href="https://github.com/kubernetes/kubernetes/issues/21613" target="_blank" rel="noopener">issue</a>。</p><p>解决方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br></pre></td></tr></table></figure><h2 id="Flannel-OOM"><a href="#Flannel-OOM" class="headerlink" title="Flannel OOM"></a>Flannel OOM</h2><p>在配置好集群业务后，发现业务时不时的出现中断情况，最开始排查业务自身问题，未发现 Pod 出现重启或异常的日志，开始排查 k8s 状态，发现在节点 <code>/var/log/messages</code> 日志中，Flannel 一直处于 OOM 状态，惨不忍睹。</p><p>之前还略微惊奇，Flannel 默认的计算资源中，内存只要 50MiB，且上限也是 50MiB，没有给自己留一丝余地，看到 <a href="https://github.com/coreos/flannel/issues/963" target="_blank" rel="noopener">issue</a> 中的描述，感觉这个不是一个偶发事件，最终我将 Flannel 的内存调整为 250MiB 后，未出现 OOM 情况。</p><p>issue 中提到的 <code>kubectl patch</code> 命令未自动生效，我通过更新 ds 配置，然后依次手动删除节点上的 Flannel Pod 使其生效。</p><h2 id="Nginx-Ingress"><a href="#Nginx-Ingress" class="headerlink" title="Nginx Ingress"></a>Nginx Ingress</h2><p>Nginx Ingress 有多个版本，在编写 Ingress 规则的时候一定要看清自己集群中的 Nginx Ingress 版本，我最开始就是因为这个看错了文档。。</p><p>主要的版本有： <code>kubernetes/ingress-nginx</code> , <code>nginxinc/kubernetes-ingress with NGINX</code> 和 <code>nginxinc/kubernetes-ingress with NGINX PLUS</code> ，具体的对比规则可以在 <a href="https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md" target="_blank" rel="noopener">Github</a> 中了解。</p><p>在 <code>kubernetes/ingress-nginx</code> 中，默认 <code>ssl-redirect</code> 参数是 <code>true</code> ，如果自己的服务不支持 https，那么需要显示的声明该参数为 false 才可以，这里需要注意一下。</p><p>在配置 nginx 参数的时候，要注意语法，正确的书写方式如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      &#123;"apiVersion":"networking.k8s.io/v1beta1","kind":"Ingress","metadata":&#123;"annotations":&#123;"kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/proxy-read-timeout":"3600","nginx.ingress.kubernetes.io/proxy-send-timeout":"3600","nginx.ingress.kubernetes.io/ssl-redirect":"true","nginx.ingress.kubernetes.io/use-regex":"true","nginx.org/websocket-services":"websockify"&#125;,"name":"websockify","namespace":"default"&#125;,"spec":&#123;"rules":[&#123;"http":&#123;"paths":[&#123;"backend":&#123;"serviceName":"websockify","servicePort":8000&#125;,"path":"/websockify"&#125;]&#125;&#125;]&#125;&#125;</span></span><br><span class="line"><span class="string">    kubernetes.io/ingress.class: nginx</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"</span></span><br></pre></td></tr></table></figure><p>相关 issue 链接： <a href="https://github.com/kubernetes/ingress-nginx/issues/2007" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/issues/2007</a> </p><h2 id="Docker-稳定性"><a href="#Docker-稳定性" class="headerlink" title="Docker 稳定性"></a>Docker 稳定性</h2><p>在修改 Docker 配置后，需要重启 Docker.service 使配置生效，在一次重启操作中，直接导致物理节点宕机，自动重启了。。。</p><p>重启后观察物理节点日志，未发现异常日志，目前待复现调查，很坑很诡异。</p><h2 id="Helm-values-为空更新错误"><a href="#Helm-values-为空更新错误" class="headerlink" title="Helm values 为空更新错误"></a>Helm values 为空更新错误</h2><p>今天在给应用编写 Helm Charts 的时候，在 Values 中通过 resources.requests.cpu 方式指定了 cpu 和内存，在测试的时候忘记填写具体数值了，像这面这样：</p><p>values:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br></pre></td></tr></table></figure></p><p>在 helm templates 中定义 daemonset，指定使用 resources 字段。</p><p>直接执行 helm 命令安装成功了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:44:09 <span class="built_in">test</span>]<span class="variable">$helm</span> install . --name-template <span class="built_in">test</span></span><br><span class="line">NAME: <span class="built_in">test</span></span><br><span class="line">LAST DEPLOYED: 2019-07-15 20:44:18.151073881 +0800 CST m=+0.092592446</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods -l <span class="string">"app=test,release=test"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br></pre></td></tr></table></figure><p>我们查看创建出来的 daemonset 资源状态：</p><p>daemonset/test</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">Name:</span>           <span class="string">test</span></span><br><span class="line"><span class="attr">Selector:</span>       <span class="string">app=test</span></span><br><span class="line"><span class="attr">Node-Selector:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="string">Pods</span> <span class="attr">Status:</span>  <span class="number">3</span> <span class="string">Running</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Waiting</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Succeeded</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Failed</span></span><br><span class="line"><span class="string">Pod</span> <span class="attr">Template:</span></span><br><span class="line"><span class="attr">  Labels:</span>  <span class="string">app=test</span></span><br><span class="line"><span class="attr">  Containers:</span></span><br><span class="line"><span class="attr">   test:</span></span><br><span class="line"><span class="attr">    Image:</span>      <span class="string">harbor.zdyxry.com/test/test:0.1.2</span></span><br><span class="line"><span class="attr">    Port:</span>       <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line">    <span class="string">Host</span> <span class="attr">Port:</span>  <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line"><span class="attr">    Command:</span></span><br><span class="line">      <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">      -</span><span class="string">c</span></span><br><span class="line"><span class="attr">    Args:</span></span><br><span class="line">      <span class="string">gunicorn</span> <span class="bullet">-b</span> <span class="string">:10402</span> <span class="bullet">-k</span> <span class="string">gevent</span> <span class="string">test.main:flask_app</span> <span class="bullet">-w</span> <span class="number">2</span> <span class="bullet">--timeout</span> <span class="number">40</span> <span class="bullet">--pid</span> <span class="string">/var/run/test.pid</span></span><br><span class="line"><span class="attr">    Limits:</span></span><br><span class="line"><span class="attr">      cpu:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>  <span class="number">0</span></span><br><span class="line"><span class="attr">    Requests:</span></span><br><span class="line"><span class="attr">      cpu:</span>        <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">    Environment:</span>  <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure><p>这时候我在检查资源的时候发现自己忘记设置资源了，我计划通过更新 values 数值来更新 daemonset：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br></pre></td></tr></table></figure><p>执行 helm upgrade 时候报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:49:36 <span class="built_in">test</span>]<span class="variable">$helm</span> upgrade <span class="built_in">test</span> .</span><br><span class="line">Error: UPGRADE FAILED: error validating <span class="string">""</span>: error validating data: [unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.memory, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.memory]</span><br></pre></td></tr></table></figure><p>根据报错信息可以看到这个字段之前是 <code>nil</code> ，现在我们要更新为有效类型更新失败，只能通过 <code>helm uninstall</code> 卸载后再次安装修复该问题。</p><p>这个问题只在 daemonset 类型下会出现。</p><h2 id="Flannel-网卡丢失"><a href="#Flannel-网卡丢失" class="headerlink" title="Flannel 网卡丢失"></a>Flannel 网卡丢失</h2><p>在通常情况下，我们的 k8s 节点都只有单一的网络环境，也就是有一块网卡，在部署 Flannel 插件的时候，默认会找默认路由所在的网卡，并将其绑定在上面。</p><p>由于内部测试环境较为特殊，我将其绑定在一个 ovs port 上，这个具体配置在 flannel yaml 中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">kube-flannel</span></span><br><span class="line"><span class="attr">  image:</span> <span class="string">quay.io/coreos/flannel:v0.11.0-amd64</span></span><br><span class="line"><span class="attr">  command:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">/opt/bin/flanneld</span></span><br><span class="line"><span class="attr">  args:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--ip-masq</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--kube-subnet-mgr</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--iface=port-storage</span>  <span class="comment"># 在这里我强制指定了 iface</span></span><br></pre></td></tr></table></figure><p>正常运行时时没有问题的，但是对 ovs port 进行了 <code>ifdown</code> 操作后，在 OS 层面就无法找到这个 ovs port 了，flannel 默认的 <code>flannel.1</code> 这个 link 也丢失了，当我尝试 <code>ifup</code> ovs port，这个 port 正常恢复工作了，但是 <code>flannel.1</code> 无法自动恢复，目前找到的办法是手动重建 flannel pod。</p><p>猜测这个动作在 flannel 的init 相关步骤执行的，在之后 container 正常运行时没有考虑 <code>flannel.1</code> 不存在的情况。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用经验通常是踩了一个又一个坑过来的~ </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>关于Ansible的一点经验</title>
    <link href="https://zdyxry.github.io/2019/07/05/%E5%85%B3%E4%BA%8EAnsible%E7%9A%84%E4%B8%80%E7%82%B9%E7%BB%8F%E9%AA%8C/"/>
    <id>https://zdyxry.github.io/2019/07/05/关于Ansible的一点经验/</id>
    <published>2019-07-05T12:55:19.000Z</published>
    <updated>2019-07-05T12:57:01.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>先介绍下 Kubespray，Kubespray 是 K8S SIG 下的项目，目标是帮助用户创建 <code>生产环境级别</code> 的 k8s 集群。</p><p>是通过 Ansible Playbook 实现的，是的，这又是一个 Ansible 项目，其中 YAML 文件就有 15k 行，名副其实的大项目。</p><p>花费了几天时间陆陆续续看完了整个项目，大概了解了其中的工作流程，具体内容不提，感觉 Ansible 90% 的使用例子都可以在这个项目中找到，是一个值得阅读的项目。</p><p>之前写过一篇当时理解的最佳实践，今天趁此机会再总结下最近使用 Ansible 的一些经验。</p><h2 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h2><p>使用 tag 对 ansible task 进行划分，比如在重启某些服务的时候，我们只希望在初次安装的时候重启，在后续升级的时候不进行重启，那么我们就可以对这个重启服务的 task 进行tag 区分。</p><p>tag 使用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:35:56 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">1 directory, 4 files</span><br><span class="line">[root@node111 16:35:58 ansible]<span class="variable">$cat</span> test.yaml </span><br><span class="line">- hosts: cluster</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: root</span><br><span class="line">  become_method: sudo</span><br><span class="line">  tasks:</span><br><span class="line">  - yum:</span><br><span class="line">      name: <span class="string">"&#123;&#123; item &#125;&#125;"</span></span><br><span class="line">      state: present</span><br><span class="line">    loop:</span><br><span class="line">    - httpd</span><br><span class="line">    - memcached</span><br><span class="line">    tags:</span><br><span class="line">    - packages</span><br><span class="line">  </span><br><span class="line">  - template:</span><br><span class="line">      src: templates/src.j2</span><br><span class="line">      dest: /etc/foo.conf</span><br><span class="line">    tags:</span><br><span class="line">    - configuration</span><br><span class="line">[root@node111 16:36:01 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml --tags configuration -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [template] *****************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=1    changed=0    unreachable=0    failed=0</span><br></pre></td></tr></table></figure><h2 id="roles-meta-管理依赖"><a href="#roles-meta-管理依赖" class="headerlink" title="roles/meta 管理依赖"></a>roles/meta 管理依赖</h2><p>在 playbook 中存在多个 roles，且其中有相互依赖关系时，合理使用 meta 配置，填写其所依赖的 roles。注意，被依赖的 roles 会优先执行，示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:49:12 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── roles</span><br><span class="line">│   ├── a</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── b</span><br><span class="line">│       ├── defaults</span><br><span class="line">│       ├── files</span><br><span class="line">│       ├── handlers</span><br><span class="line">│       ├── meta</span><br><span class="line">│       ├── tasks</span><br><span class="line">│       │   └── main.yaml</span><br><span class="line">│       ├── templates</span><br><span class="line">│       └── vars</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">18 directories, 7 files</span><br><span class="line">[root@node111 16:49:18 ansible]<span class="variable">$cat</span> roles/a/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: a</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"a"</span></span><br><span class="line">[root@node111 16:49:25 ansible]<span class="variable">$cat</span> roles/a/meta/main.yaml </span><br><span class="line">---</span><br><span class="line">dependencies:</span><br><span class="line">  - &#123; role: b &#125;</span><br><span class="line">[root@node111 16:49:33 ansible]<span class="variable">$cat</span> roles/b/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: b</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"b"</span></span><br><span class="line">[root@node111 16:49:40 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [b : b] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TASK [a : a] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line"></span><br><span class="line">You have new mail <span class="keyword">in</span> /var/spool/mail/root</span><br><span class="line">[root@node111 16:50:05 ansible]$</span><br></pre></td></tr></table></figure><h2 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h2><p>在一个大型项目中，我们无论是服务的数量还是各个服务对应的参数数量都是极其惊人的，那么我们就要合理的管理相应参数，这里 kubespray 项目给出了一个很好的示例，先来看下 kubespray 的组织结构（简化版）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">inventory/</span><br><span class="line">├── <span class="built_in">local</span></span><br><span class="line">│   └── group_vars -&gt; ../sample/group_vars</span><br><span class="line">└── sample</span><br><span class="line">    └── group_vars</span><br><span class="line">        ├── all</span><br><span class="line">        └── k8s-cluster</span><br><span class="line">roles</span><br><span class="line">├── container-engine</span><br><span class="line">│   ├── containerd</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   ├── docker</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── meta</span><br><span class="line">├── kubespray-defaults</span><br><span class="line">│   ├── defaults</span><br><span class="line">│   ├── meta</span><br><span class="line">│   └── tasks</span><br></pre></td></tr></table></figure><p>在 kubespray 中，参数定义有三个位置：</p><ol><li>inventory/sample/group_vars</li><li>roles/kubespray-defaults/defaults</li><li>roles/<common>/defaults</common></li><li>使用 <code>set_fact</code> 关键字声明</li></ol><p>上述三个位置是按照参数粒度划分，参数粒度越细，越靠后。</p><p>举个例子：</p><ul><li><code>bin_dir</code> 这个参数，是一个全局参数，管理下载的二进制文件路径，它在 <code>inventory/samle/group_vars</code> 中定义； </li><li><code>etcd_kubeadm_enabled</code> 参数，决定是否通过 kubeadm 来创建 etcd 集群，是集群粒度的，在 <code>roles/kubesray-defaults/defaults</code> 中定义；</li><li><code>docker_fedora_repo_base_url</code> 是 docker 在下载镜像时指定的 repo，是一个容器运行时粒度的参数，那么它就在 <code>roles/container-ngine/docker/defaults</code> 中定义</li><li>执行某些命令后，我们希望对命令结果进行过滤或判断，那么我们通常会使用 regster，但是 register 的声明周期仅限于该 playbook，而且他们的优先级也是不同的，具体可以看下<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#ansible-variable-precedence" target="_blank" rel="noopener">官方文档</a></li></ul><p>可能跟大多数项目不同，kubespray 中涉及的参数数量很多，不知道是否是这个原因，专门使用了 <code>roles/kubespray-defaults</code> 这个 role 来声明参数。不过它的这种参数划分方式是我们值得学习的。</p><h2 id="Handler-触发"><a href="#Handler-触发" class="headerlink" title="Handler 触发"></a>Handler 触发</h2><p>当我们更新了配置文件之后，我们想要重启相应服务，这时候可以使用 notify 配合 handlers 来完成相应操作，而且 handler<br>的方式也可以保证我们代码最大程度的重用。</p><h2 id="loop-control"><a href="#loop-control" class="headerlink" title="loop_control"></a>loop_control</h2><p>当我们在 playbook 中定义了某个变量为 list 类型，我们想要遍历变量，并且针对每个变量的操作都进行错误处理，我们可以采用 loop_control 关键字配合 include 来实现，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- include_tasks: docker_plugin.yml</span><br><span class="line">  loop: &quot;&#123;&#123; docker_plugins &#125;&#125;&quot;</span><br><span class="line">  loop_control:</span><br><span class="line">    loop_var: docker_plugin</span><br></pre></td></tr></table></figure><p>循环 <code>device_plugins</code> ，每个变量为 <code>docker_plugin</code> ，将 <code>docker_plugin</code> 传递到 <code>docker_plugin.yml</code> 。</p><h2 id="不建议的操作"><a href="#不建议的操作" class="headerlink" title="不建议的操作"></a>不建议的操作</h2><h3 id="在-ansible-中使用复杂的语法规则"><a href="#在-ansible-中使用复杂的语法规则" class="headerlink" title="在 ansible 中使用复杂的语法规则"></a>在 ansible 中使用复杂的语法规则</h3><p>在 kubespray 中，随处可见一些很复杂的语法夹杂在 playbook 中，看到一个比较头疼的：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">279</span></span><br><span class="line"><span class="number">280</span><span class="attr">dashboard_image_repo:</span> <span class="string">"gcr.io/google_containers/kubernetes-dashboard-<span class="template-variable">&#123;&#123; image_arch &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">281</span><span class="attr">dashboard_image_tag:</span> <span class="string">"v1.10.1"</span></span><br><span class="line"><span class="number">282</span></span><br><span class="line"><span class="number">283</span><span class="attr">image_pull_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker pull"</span></span><br><span class="line"><span class="number">284</span><span class="attr">image_info_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker images -q | xargs <span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker inspect -f \"<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoTags <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoTags 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoDigests <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;,<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoDigests 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;\" | tr '\n' ','"</span></span><br><span class="line"><span class="number">285</span></span><br><span class="line"><span class="number">286</span><span class="attr">downloads:</span></span><br><span class="line"><span class="number">287</span>  <span class="attr">netcheck_server:</span></span><br><span class="line"><span class="number">288</span>    <span class="attr">enabled:</span> <span class="string">"<span class="template-variable">&#123;&#123; deploy_netchecker &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">289</span>    <span class="attr">container:</span> <span class="literal">true</span></span><br><span class="line"><span class="number">290</span>    <span class="attr">repo:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_repo &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">291</span>    <span class="attr">tag:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">292</span>    <span class="attr">sha256:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_digest_checksum|default(None) &#125;&#125;</span>"</span></span><br></pre></td></tr></table></figure><p>284 行获取 <code>image_info_command</code> ，这里真的是一点可维护性都没有，初步看很难看出这里到底是 ansible 语法，还是 shell 语法，还是 go template 语法，如果我们要通过这么复杂的方式来获取一个参数，为什么不干脆写一个脚本来完成这个事情呢？ 搞不懂。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>看完 kubespray 了解了 k8s 集群部署的步骤之外，Ansible 的一些高级用法或者说经验是之后需要改善的，收获多多。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;先介绍下 Kubespray，Kubespray 是 K8S SIG 下的项目，目标是帮助用户创建 &lt;code&gt;生产环境级别&lt;/code&gt; 
      
    
    </summary>
    
    
      <category term="Ansible" scheme="https://zdyxry.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Pod 可用性</title>
    <link href="https://zdyxry.github.io/2019/06/26/Kubernetes-%E5%AE%9E%E6%88%98-Pod-%E5%8F%AF%E7%94%A8%E6%80%A7/"/>
    <id>https://zdyxry.github.io/2019/06/26/Kubernetes-实战-Pod-可用性/</id>
    <published>2019-06-26T13:57:31.000Z</published>
    <updated>2019-06-27T12:52:37.478Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性相关知识。</p><h2 id="K8S-可用性相关参数"><a href="#K8S-可用性相关参数" class="headerlink" title="K8S 可用性相关参数"></a>K8S 可用性相关参数</h2><p>k8s 核心组件有 kubelet,kube-apiserver,kube-scheduler,kube-controller-manager，通过阅读官方文档中相关参数说明，我摘取了认为跟可用性相关的参数，具体列表如下：</p><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><h4 id="–housekeeping-interval-duration"><a href="#–housekeeping-interval-duration" class="headerlink" title="–housekeeping-interval duration"></a>–housekeeping-interval duration</h4><p>Default: 10s</p><p>Interval between container housekeepings.</p><p>kubelet 主动检测容器资源是否达到阈值的周期。</p><h4 id="–node-status-update-frequency-duration"><a href="#–node-status-update-frequency-duration" class="headerlink" title="–node-status-update-frequency duration"></a>–node-status-update-frequency duration</h4><p>Default: 10s</p><p>Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller. </p><p>kubelet 上报到 kube-apiserver 频率。</p><h3 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h3><h4 id="–node-eviction-rate-float32"><a href="#–node-eviction-rate-float32" class="headerlink" title="–node-eviction-rate float32"></a>–node-eviction-rate float32</h4><p>Default: 0.1 </p><p>Number of nodes per second on which pods are deleted in case of node failure when a zone is healthy (see –unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.</p><p>当 kube-controller-manager 判定节点故障，开始迁移（重建）pod 的速度，默认是 0.1，也就是 1Pod/10s 。</p><h4 id="–node-monitor-grace-period-duration"><a href="#–node-monitor-grace-period-duration" class="headerlink" title="–node-monitor-grace-period duration"></a>–node-monitor-grace-period duration</h4><p>Default: 40s </p><p>Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet’s nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.</p><p>kube-controller-manager 标记 kubelet(node) 为不健康的周期。</p><h4 id="–node-monitor-period-duration"><a href="#–node-monitor-period-duration" class="headerlink" title="–node-monitor-period duration"></a>–node-monitor-period duration</h4><p>Default: 5s </p><p>The period for syncing NodeStatus in NodeController.</p><p>kube-controller-manager 定期检查 kubelet(node) 状态周期。</p><h4 id="–node-startup-grace-period-duration"><a href="#–node-startup-grace-period-duration" class="headerlink" title="–node-startup-grace-period duration"></a>–node-startup-grace-period duration</h4><p>Default: 1m0s </p><p>Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.</p><p>kube-controller-manager 在标记节点为不健康之前允许无响应时间。</p><h4 id="–pod-eviction-timeout-duration"><a href="#–pod-eviction-timeout-duration" class="headerlink" title="–pod-eviction-timeout duration"></a>–pod-eviction-timeout duration</h4><p>Default: 5m0s </p><p>The grace period for deleting pods on failed nodes.</p><p>kube-controller-manager 判定节点故障，重建 Pod 的超时时间。</p><h2 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h2><p>看完了相关参数，我们来看下 kubelet 和 kube-controller-manager 是如何相互关联工作来保证 Pod 可用性的。</p><ol><li><p>kubelet 启动，若启动时间超过 node-startup-grace-period，则 kube-controller-manager 将其置为 unhealthy</p></li><li><p>kubelet 按照 –node-status-update-frequency 周期，定时与 kube-apiserver 通信将其状态记录到 etcd</p></li><li><p>若 kubelet 无法连接到 kube-apiserver，那么 kubelet 会尝试 nodeStatusUpdateRetry 次更新状态信息</p></li><li><p>kube-controller-manager 按照 –node-monitor-period 周期，定时从 etcd 中获取 kubelet 状态</p></li><li><p>若 kubelet 在 –node-monitor-grace-period 周期内均为非健康状态（这里如果 kubelet 未更新状态，等同），则该节点更新为 NotReady，将需要迁移（重建）的资源放置到队列中</p></li><li><p>当 kubelet NotReady 的 –pod-eviction-timeout 时间后， kube-controller-manager 开始进行 Pod 驱逐动作</p></li><li><p>驱逐速度为 –node-eviction-rate ，即每10s 迁移（重建）1个 Pod</p></li></ol><p>需要注意的是，上述 kubelet 和 kube-controller-manager 的操作是异步的，中间任何一个更新步骤都有可能出现延迟的情况，所以真实情况会比上述流程复杂（诡异）的多。比如 lijieao 的<a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html" target="_blank" rel="noopener">最新博客</a>中碰到的因为磁盘 IO 压力导致 kubelet NotReady 的情况，都是有可能出现的。</p><p>按照上面的流程，可以看到，当我们节点故障后，要花费 5min 的时间才会重建我们的业务，这种情况下大部分对稳定性要求较高的业务都无法忍受的，所以有同学可能会考虑修改默认配置，来减少业务宕机时间。</p><p>上述提到的参数的默认值，肯定是 k8s 社区经过多年的架构设计（或者经验？）的。之前看过社区中关于这部分的一篇文章提到，不建议修改默认的配置，担心修改了默认配置可能因为其他一些比较微小的故障导致 Pod 频繁的迁移，这是我们不想看到的。</p><p>上面提到的情况都是说：当节点发生故障后，我们希望我们的 Pod 能够第一时间迁移到正常运行的节点，那么有没有反过来的，不想迁移的呢？</p><p>其实是有的，比如有状态服务配合 kubelet 服务故障场景。我们的节点正常运行的情况下，容器运行时也都正常工作，唯独 kubelet 故障了，那么我们想想此时发生了什么？ </p><p>kube-controller-manager 在一段时间后标记节点故障，开始迁移（重建） Pod 操作，但是要注意，此时因为是有状态服务，而节点上的容器又在正常运行，kubelet 由于故障了，k8s 无法直接操作节点上的容器，只能在其他节点进行重建。这时候问题就出现了，集群中存在2个 Pod 同时读写一个 PV，这种错误是致命的。</p><p>在这种场景下，我们想要追求的是哪怕节点故障了，我们也要尽量的不迁移 Pod，来保证我们数据的读写正常。</p><p>那么上述情况我们应该怎么解决呢？ </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Operator 是一种方式。在 Operator 出现之前，有状态服务的运维工作都是靠着探针或者其他的基础运维工具来完成的，很多工作即使做到了自动化也不完美，现在有了 Operator 结合 CRD，我们完全可以自己在应用层面监控服务状态，从而主动的触发 Pod 的迁移（重建），保证我们业务的稳定。</p><p>另一种解决方式是 <code>Taint based Evictions</code>，在 1.13 版本中增加了该配置，我们可以在创建资源的时候，指定规则配置Pod 容忍节点异常的时间，加快触发 Pod 重建，这大大减缓了对集群配置的要求。</p><h2 id="测试-YAML"><a href="#测试-YAML" class="headerlink" title="测试 YAML"></a>测试 YAML</h2><p>在测试 Pod 可用性的时候，我们必须创建 Deployment、RS 类型资源，单纯的 Pod 资源是不被保证可用性的。同时我们也想观察 DaemonSet Pod 在节点故障场景下的表现，所以一同创建了。可以使用以下 YAML 用来创建测试环境。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@m1</span> <span class="string">ha]#</span> <span class="string">cat</span> <span class="string">ha.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/unreachable"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/not-ready"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">sleep</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">"apps/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">dp</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">          done</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string"></span><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md" target="_blank" rel="noopener">https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md</a></p><p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md</a></p><p><a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81" target="_blank" rel="noopener">https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-Helm 包管理器</title>
    <link href="https://zdyxry.github.io/2019/06/21/Kubernetes-%E5%AE%9E%E6%88%98-Helm-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8/"/>
    <id>https://zdyxry.github.io/2019/06/21/Kubernetes-实战-Helm-包管理器/</id>
    <published>2019-06-21T12:40:24.000Z</published>
    <updated>2019-06-22T01:40:50.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Helm 就是<code>k8s 的包管理器</code> 。常见的包管理器有：yum,apt,pip…</p><p>包管理器基础功能有：</p><ul><li>安装<ul><li>依赖安装</li></ul></li><li>升级</li><li>回滚</li><li>卸载</li><li>源管理</li><li>搜索</li><li>…</li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>Helm: Kubernetes的包管理工具，命令行同名</p></li><li><p>Tiller: Helmv2 的服务端，用于接收并处理 Helm 发送的请求，默认以 Deployment 形式部署在 k8s 集群中</p></li><li><p>Chart: Helm 包管理的基础单元，等同于 RPM</p></li><li><p>Repoistory: Helm的软件源仓库，是一个 Web 服务器，路径下除了响应的软件 Chart 包之外，维护了一个 index.yaml 用于索引</p></li><li><p>Release: Helm 安装在 Kubernetes 集群中的 Chart 实例</p></li></ul><h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><p>helm 截至06月20日最新稳定版本为 v2.14.1。</p><p>在05月16日发布了 v3.0 alpha 版本，根据相关文档描述，v2 无法平滑升级到 v3 版本。</p><p>注：存在部分小版本无法平滑升级情况。</p><p>helm v3 版本改进：</p><ol><li>在 v2 版本设计中，需要单独创建属于 Tiller 的 ServiceAccount，授权 clusteradmin 权限，以为着只要你有 helm 权限，那么你有操作 k8s全集群所有权限。在 v3 版本中删除 Tiller，直接与 k8s api 进行通信，权限管理更清晰</li><li>helm 提供 libary</li><li>模板引擎切换为 Lua</li><li>目前通过 Hook 方式创建的资源，helm 后续不会管理，在 v3 会增加管理 Hook 资源功能</li><li>目前所有配置保存在 cm 中，后续考虑保存到 secret</li><li>v2 需要单独维护仓库，v3 中可以将 Chart 推送到 Docker 镜像仓库中，提供 helm push/login 功能</li><li>…</li></ol><h2 id="Helm2-基本使用"><a href="#Helm2-基本使用" class="headerlink" title="Helm2 基本使用"></a>Helm2 基本使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在 Helm Github <a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">Release</a> 下载最新版本二进制文件，并在本地解压。</p><p>在 k8s master 节点，执行 <code>helm init --server-account tiller</code> 将 Tiller 部署在 k8s 集群中，指定 Service Account 为 Tiller。</p><p>在 k8s 1.6 版本之后，需要创建对应的 ServiceAccount 资源给 Tiller ，便于 Tiller 后续创建资源，参考官方文档：</p><p>rbac-config.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount <span class="string">"tiller"</span> created</span><br><span class="line">clusterrolebinding <span class="string">"tiller"</span> created</span><br></pre></td></tr></table></figure><p>通过 <code>helm version</code> 验证是否部署成功，成功会显示 client 和 server 对应版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="Chart构建"><a href="#Chart构建" class="headerlink" title="Chart构建"></a>Chart构建</h4><p>本地创建一个测试软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm create yiran-test</span></span><br><span class="line">Creating yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts                            <span class="comment"># yiran-test 所依赖的 Chart，此处为空</span></span><br><span class="line">├── Chart.yaml                        <span class="comment"># yiran-test 的基本信息，包含：名称，apiversion, appversion, version</span></span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml               <span class="comment"># 配置模板路径</span></span><br><span class="line">│   ├── _helpers.tpl                  <span class="comment"># 用于修改kubernetes objcet配置的模板</span></span><br><span class="line">│   ├── ingress.yaml                  <span class="comment"># </span></span><br><span class="line">│   ├── NOTES.txt                     <span class="comment"># 类似于 Chart README</span></span><br><span class="line">│   ├── service.yaml                  <span class="comment"># </span></span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml      <span class="comment"># 测试模板</span></span><br><span class="line">└── values.yaml                       <span class="comment"># 用于渲染模板的具体值</span></span><br><span class="line"></span><br><span class="line">3 directories, 8 files</span><br></pre></td></tr></table></figure><p>有了基础示例，我们可以先通过 <code>dry-run</code> 方式跑一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install --dry-run --debug ./</span></span><br><span class="line">[debug] Created tunnel using <span class="built_in">local</span> port: <span class="string">'39257'</span></span><br><span class="line"></span><br><span class="line">[debug] SERVER: <span class="string">"127.0.0.1:39257"</span></span><br><span class="line"></span><br><span class="line">[debug] Original chart version: <span class="string">""</span></span><br><span class="line">[debug] CHART PATH: /root/helm/yiran-test</span><br><span class="line"></span><br><span class="line">NAME:   torrid-rodent</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Thu Jun 20 19:10:08 2019</span><br><span class="line">CHART: yiran-test-0.1.0</span><br><span class="line">USER-SUPPLIED VALUES:</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">COMPUTED VALUES:</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line">fullnameOverride: <span class="string">""</span></span><br><span class="line">image:</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">imagePullSecrets: []</span><br><span class="line">ingress:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  enabled: <span class="literal">false</span></span><br><span class="line">  hosts:</span><br><span class="line">  - host: chart-example.local</span><br><span class="line">    paths: []</span><br><span class="line">  tls: []</span><br><span class="line">nameOverride: <span class="string">""</span></span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line">replicaCount: 1</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  port: 80</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># torrid-rodent-yiran-test-test-connection</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="string">"torrid-rodent-yiran-test-test-connection"</span></span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: yiran-test</span><br><span class="line">    helm.sh/chart: yiran-test-0.1.0</span><br><span class="line">    app.kubernetes.io/instance: torrid-rodent</span><br><span class="line">    app.kubernetes.io/version: <span class="string">"1.0"</span></span><br><span class="line">    app.kubernetes.io/managed-by: Tiller</span><br><span class="line">  annotations:</span><br><span class="line">    <span class="string">"helm.sh/hook"</span>: <span class="built_in">test</span>-success</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: wget</span><br><span class="line">      image: busybox</span><br><span class="line">      <span class="built_in">command</span>: [<span class="string">'wget'</span>]</span><br><span class="line">      args:  [<span class="string">'torrid-rodent-yiran-test:80'</span>]</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">MANIFEST:</span><br></pre></td></tr></table></figure><h4 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h4><p>可以看到生成的 YAML 文件就是拿 values.yaml 值渲染模板生成的，那么我们来安装一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install ./</span></span><br><span class="line">NAME:   precise-bear</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:12:01 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS             RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Running   0          7s</span><br></pre></td></tr></table></figure><p>注意，此时我们已经创建好了对应的资源，可以通过 <code>kubectl</code> 来查看状态。</p><h4 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h4><p>我们来修改一下 <code>yiran-test/Chart.yaml</code> ，调整下版本，进行升级：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># cat Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">heNAME          REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm upgrade precise-bear ./</span></span><br><span class="line">Release <span class="string">"precise-bear"</span> has been upgraded.</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:18:30 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  1/1    1           1          6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS   RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  1/1    Running  0         6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   6m30s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default</span><br></pre></td></tr></table></figure><h4 id="回滚"><a href="#回滚" class="headerlink" title="回滚"></a>回滚</h4><p>通过上述步骤，我们已经把我们的应用 <code>yiran-test</code> 从 <code>0.1.0</code> 版本升级到了 <code>0.2.0</code> 版本，那么我们现在来尝试下回滚。</p><p>helm 回滚操作需要指定 Release 名称和目标版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm rollback precise-bear 1</span></span><br><span class="line">Rollback was a success.</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    3           Thu Jun 20 19:23:04 2019    DEPLOYED    yiran-test-0.1.0    1.0         default</span><br></pre></td></tr></table></figure><p>嗯，可以看到，我们指定了回滚的目标 REVISION，回滚成功了，<code>yiran-test</code> 回到了 <code>0.1.0</code> 版本，但是，最恶心的来了，Release 的当前版本变成了 <code>3</code> ，而不是设想中的 <code>1</code> 。</p><h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><p>如果我们想要从 k8s 上卸载对应的软件，也就是我们的 <code>yiran-test</code> ，我们可以直接执行 <code>delete</code> 命令，会直接把相关资源全部删除掉。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">NAME            REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">precise-bear    3               Thu Jun 20 19:23:04 2019        DEPLOYED        yiran-test-0.1.0        1.0             default</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">kuNAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   1/1     Running   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes                ClusterIP   10.68.0.1       &lt;none&gt;        443/TCP   36d</span><br><span class="line">precise-bear-yiran-test   ClusterIP   10.68.142.106   &lt;none&gt;        80/TCP    139m</span><br><span class="line">[root@node1 ~]<span class="comment"># helm delete precise-bear</span></span><br><span class="line">release <span class="string">"precise-bear"</span> deleted</span><br><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">NAME                                       READY   STATUS        RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Terminating   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.68.0.1    &lt;none&gt;        443/TCP   36d</span><br></pre></td></tr></table></figure><h4 id="软件源管理"><a href="#软件源管理" class="headerlink" title="软件源管理"></a>软件源管理</h4><p>上面我们所有的操作都是针对本地包（路径），那么我们怎么才能通过网络下载别人已经构建好的软件包呢？ </p><p>helm 使用 <code>repo</code> 命令来管理软件源，使用上也很简单，简单列举下相应命令实用说明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME    URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>   http://127.0.0.1:8879/charts</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable-mirror https://burdenbear.github.io/kube-charts-mirror/</span></span><br><span class="line"><span class="string">"stable-mirror"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable https://kubernetes-charts.storage.googleapis.com</span></span><br><span class="line">helm repo list</span><br><span class="line"><span class="string">"stable"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>           http://127.0.0.1:8879/charts</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo remove local</span></span><br><span class="line"><span class="string">"local"</span> has been removed from your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br></pre></td></tr></table></figure><h4 id="软件搜索"><a href="#软件搜索" class="headerlink" title="软件搜索"></a>软件搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm search mongo</span></span><br><span class="line">NAME                                            CHART VERSION   APP VERSION     DESCRIPTION</span><br><span class="line">stable-mirror/mongodb                           5.20.0          4.0.10          NoSQL document-oriented database that stores JSON-like <span class="keyword">do</span>...</span><br><span class="line">stable-mirror/mongodb-replicaset                3.9.6           3.6             </span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="打包与本地软件源构建"><a href="#打包与本地软件源构建" class="headerlink" title="打包与本地软件源构建"></a>打包与本地软件源构建</h4><p>Helm v2 命令支持本地创建软件源，命令关键字是 <code>helm serve</code> ，下面来演示下相关操作：</p><p>启动本地源，并指定 IP 地址和 repo 路径：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">rbac-config.yaml  yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># helm serve --address 192.168.27.231:8879 --repo-path /root/helm/ </span></span><br><span class="line">Regenerating index. This may take a moment.</span><br><span class="line">Now serving you on 192.168.27.231:8879</span><br></pre></td></tr></table></figure></p><p>新开终端，通过修改 <code>yiran-test</code> 的 Chart.yaml 文件打包两个版本的 Chart：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: <span class="string">"1.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.1.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.1.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># vi yiran-test/Chart.yaml </span></span><br><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">index.yaml  rbac-config.yaml  yiran-test  yiran-test-0.1.0.tgz  yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure><p>此时访问浏览器，应该只能看到空的列表，我们更新一下软件源索引：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo index --url=http://192.168.27.231:8879 .</span></span><br><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># cat index.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">entries:</span><br><span class="line">  yiran-test:</span><br><span class="line">  - apiVersion: v2</span><br><span class="line">    appVersion: <span class="string">"2.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220764856+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: af54cfdc5f8e6463a91311496bab8fafd7364c3588b85b5e676fb930cd4e2754</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.2.0.tgz</span><br><span class="line">    version: 0.2.0</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    appVersion: <span class="string">"1.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220059406+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: 64b94c30827aab8e52f57cd3950645bb14ae2deb44e3100d48ecd64f1e706ea5</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.1.0.tgz</span><br><span class="line">    version: 0.1.0</span><br><span class="line">generated: <span class="string">"2019-06-21T13:43:41.218753007+08:00"</span></span><br></pre></td></tr></table></figure><p>可以看到在 repo 路径下生成了一个 index.yaml 文件，这个文件就是 repo 的索引文件，我们可以直接通过浏览器访问 <code>http://192.168.27.231:8879</code> 来浏览或下载所需软件包。</p><p>也可以将本地 repo 添加到 helm 中，使用 helm 命令将软件包部署到 k8s 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         URL                                             </span><br><span class="line">stable       https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirrorhttps://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo add local http://192.168.27.231:8879</span></span><br><span class="line"><span class="string">"local"</span> has been added to your repositories</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         URL                                             </span><br><span class="line">stable       https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirrorhttps://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line"><span class="built_in">local</span>        http://192.168.27.231:8879                      </span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            CHART VERSIONAPP VERSIONDESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test0.2.0        2.0        A Helm chart <span class="keyword">for</span> Kubernetes</span><br></pre></td></tr></table></figure><p>尝试从本地源安装应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            CHART VERSIONAPP VERSIONDESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test0.2.0        2.0        A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">[root@node1 helm]<span class="comment"># helm install local/yiran-test</span></span><br><span class="line">NAME:   orange-chicken</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 13:49:45 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">orange-chicken-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                        READY  STATUS             RESTARTS  AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                       TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">orange-chicken-yiran-test  ClusterIP  10.68.85.197  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=orange-chicken"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME          REVISIONUPDATED                 STATUS  CHART           APP VERSIONNAMESPACE</span><br><span class="line">orange-chicken1       Fri Jun 21 13:49:45 2019DEPLOYEDyiran-test-0.2.02.0        default </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp   1/1     Running   0          62s</span><br></pre></td></tr></table></figure><h4 id="Chart-依赖管理"><a href="#Chart-依赖管理" class="headerlink" title="Chart 依赖管理"></a>Chart 依赖管理</h4><p>包管理器一个比较钟要的功能就是依赖管理，当我安装 A，A 依赖于 B，那么 B 应该会自动安装完成。</p><p>我们在 <code>yiran-test</code> 中添加两个依赖，并构建：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/requirements.yaml   # 添加 apache 和 mysql 依赖</span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: apache</span><br><span class="line">    version: 4.3.2</span><br><span class="line">    repository: https://charts.bitnami.com</span><br><span class="line">  - name: mysql</span><br><span class="line">    version: 1.2.0</span><br><span class="line">    repository: https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 当前目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm dep up yiran-test/ # 下载依赖到本地</span></span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Successfully got an update from the <span class="string">"local"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"bitnami"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable-mirror"</span> chart repository</span><br><span class="line">Update Complete.</span><br><span class="line">Saving 2 charts</span><br><span class="line">Downloading apache from repo https://charts.bitnami.com</span><br><span class="line">Downloading mysql from repo https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">Deleting outdated charts</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 完整目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">│   ├── apache-4.3.2.tgz</span><br><span class="line">│   └── mysql-1.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 12 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure><p>可以看到 helm 对依赖的管理方式是将自己所依赖的所有 Chart，均下载到 <code>yiran-test/charts/</code> 路径下，我们打包的时候其实已经包含了所有依赖了。</p><p>再创建一个 Chart，依赖于 <code>yiran-test</code> ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># tree nested/</span></span><br><span class="line">nested/</span><br><span class="line">├── charts</span><br><span class="line">│   └── yiran-test-0.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 11 files</span><br><span class="line">[root@node1 helm]<span class="comment"># cat nested/requirements.yaml </span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: yiran-test</span><br><span class="line">    version: 0.2.0</span><br><span class="line">    repository: http://192.168.27.231:8879</span><br></pre></td></tr></table></figure><p>实际安装 <code>nested</code> ，来看下安装结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm install nested/</span></span><br><span class="line">NAME:   alliterating-gopher</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 18:22:52 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                            DATA  AGE</span><br><span class="line">alliterating-gopher-mysql-test  1     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                            READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-nested      0/1    1           0          1s</span><br><span class="line">alliterating-gopher-yiran-test  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/PersistentVolumeClaim</span><br><span class="line">NAME                       STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">alliterating-gopher-mysql  Pending  1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                             READY  STATUS             RESTARTS  AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg        0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm       0/1    Pending            0         1s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq      0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME                       TYPE    DATA  AGE</span><br><span class="line">alliterating-gopher-mysql  Opaque  2     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                            TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE</span><br><span class="line">alliterating-gopher-apac        LoadBalancer  10.68.35.233   &lt;pending&gt;    80:21195/TCP,443:26200/TCP  1s</span><br><span class="line">alliterating-gopher-mysql       ClusterIP     10.68.70.173   &lt;none&gt;       3306/TCP                    1s</span><br><span class="line">alliterating-gopher-nested      ClusterIP     10.68.190.252  &lt;none&gt;       80/TCP                      1s</span><br><span class="line">alliterating-gopher-yiran-test  ClusterIP     10.68.217.171  &lt;none&gt;       80/TCP                      1s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-apac   0/1    1           0          1s</span><br><span class="line">alliterating-gopher-mysql  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=nested,app.kubernetes.io/instance=alliterating-gopher"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME               REVISIONUPDATED                 STATUS  CHART       APP VERSIONNAMESPACE</span><br><span class="line">alliterating-gopher1       Fri Jun 21 18:22:52 2019DEPLOYEDnested-0.1.01.0        default  </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg         0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm        0/1     Pending   0          5s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq       0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm   1/1     Running   0          5s</span><br></pre></td></tr></table></figure><p>我们可以看到 nested 已经安装完成了，同时 nested 依赖得 yiran-test 及 yiran-test 依赖得 mysql &amp; apache 也已经安装了。</p><p>具体得依赖关系直接引用官方文档示例：</p><p>假设名为 “A” 的 chart 创建以下 Kubernetes 对象</p><blockquote><p>namespace “A-Namespace”<br>statefulset “A-StatefulSet”<br>service “A-Service”  </p></blockquote><p>此外，A 依赖于创建对象的 chart B.</p><blockquote><p>namespace “B-Namespace”<br>replicaset “B-ReplicaSet”<br>service “B-Service”  </p></blockquote><p>安装/升级 chart A 后，会创建/修改单个 Helm 版本。该版本将按以下顺序创建/更新所有上述 Kubernetes 对象：</p><blockquote><p>A-Namespace<br>B-Namespace<br>A-StatefulSet<br>B-ReplicaSet<br>A-Service<br>B-Service  </p></blockquote><p>这是因为当 Helm 安装 / 升级 charts 时，charts 中的 Kubernetes 对象及其所有依赖项都是如下</p><ol><li>聚合成一个单一的集合; </li><li>按类型排序，然后按名称排序; </li><li>按该顺序创建/更新。</li></ol><p>因此，单个 release 是使用 charts 及其依赖关系创建的所有对象，这意味着 Helm 不管理依赖服务的相关启动顺序，要由上层应用自己控制（比如创建响应探针）。</p><p>具体的资源创建顺序可以看 Tiller <a href="https://github.com/helm/helm/blob/master/pkg/tiller/kind_sorter.go#L29" target="_blank" rel="noopener">相关代码</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说 Helm 基础功能使用还是很方便的，关键在于我们如何划分我们应用的粒度，比如 openstack 是以组件为粒度划分的：nova Chart 包含 api、scheduler、conductor 等服务；比如 TiDB Operator 项目是以具体功能来划分的：tidb-backup、tidb-cluster、tidb-operator 。我们应该根据自己的业务需求，合理划分。</p><p>还有一点需要注意的是，Helm 项目还处于快速发展阶段（貌似涉及 k8s 的都变化太快），尤其是最近发布了 Helm3 alpha，如果是生产系统，需要考虑后续是否能够平滑升级的影响。</p><p>如果实在不喜欢 Helm Tiller 方式，单纯使用 Helm template 也是可以的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;Helm 就是&lt;code&gt;k8s 的包管理器&lt;/code&gt; 。常见的包管理器有：yum,apt,pip…&lt;/p&gt;
&lt;p&gt;包管理器基础功能有：
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>调整 arp 参数提高网络稳定性</title>
    <link href="https://zdyxry.github.io/2019/06/17/%E8%B0%83%E6%95%B4-arp-%E5%8F%82%E6%95%B0%E6%8F%90%E9%AB%98%E7%BD%91%E7%BB%9C%E7%A8%B3%E5%AE%9A%E6%80%A7/"/>
    <id>https://zdyxry.github.io/2019/06/17/调整-arp-参数提高网络稳定性/</id>
    <published>2019-06-17T13:20:04.000Z</published>
    <updated>2019-06-17T13:32:36.915Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在机房网络规划中，地区 A 和地区 B 是通过 OpenVPN 连接的，也就是说每个地区的网关是一台虚拟机，提供 DHCP 服务。<br>今天地区 A 的机器又无法连接地区 B 了，我登陆网关尝试从网关 ping 目标主机，发现直接提示 :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No buffer space available</span><br></pre></td></tr></table></figure><p>根据这个提示，感觉像是某些系统参数配置的小了，于是查了一下，发现跟 arp 有关。什么是 arp ？ </p><p>相信对网络稍微有些概念的同学都不陌生，这里我直接引用维基百科：</p><blockquote><p>地址解析协议（英语：Address Resolution Protocol，缩写：ARP）。在以太网协议中规定，同一局域网中的一台主机要和另一台主机进行直接通信，必须要知道目标主机的MAC地址。而在TCP/IP协议中，网络层和传输层只关心目标主机的IP地址。这就导致在以太网中使用IP协议时，数据链路层的以太网协议接到上层IP协议提供的数据中，只包含目的主机的IP地址。于是需要一种方法，根据目的主机的IP地址，获得其MAC地址。这就是ARP协议要做的事情。所谓地址解析（address resolution）就是主机在发送帧前将目标IP地址转换成目标MAC地址的过程。</p></blockquote><blockquote><p>另外，当发送主机和目的主机不在同一个局域网中时，即便知道对方的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为委托ARP或ARP代理（ARP Proxy）。</p></blockquote><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>知道了原因，那么我们来调整参数就好：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gc_thresh1 (since Linux 2.2)</span><br><span class="line">The minimum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will not run <span class="keyword">if</span> there are fewer than this number of entries <span class="keyword">in</span> the cache. Defaults to 128.</span><br><span class="line">gc_thresh2 (since Linux 2.2)</span><br><span class="line">The soft maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will allow the number of entries to exceed this <span class="keyword">for</span> 5 seconds before collection will be performed. Defaults to 512.</span><br><span class="line">gc_thresh3 (since Linux 2.2)</span><br><span class="line">The hard maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will always run <span class="keyword">if</span> there are more than this number of entries <span class="keyword">in</span> the cache. Defaults to 1024.</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yiran@<span class="built_in">test</span>:~$ cat /etc/sysctl.conf |grep -v ^<span class="comment"># |grep -v ^$</span></span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">yiran@<span class="built_in">test</span>:~$ sudo sysctl -p</span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。&lt;/p&gt;
&lt;h2 id=&quot;问题&quot;
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://zdyxry.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 实战-高可用集群部署</title>
    <link href="https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://zdyxry.github.io/2019/06/15/Kubernetes-实战-高可用集群部署/</id>
    <published>2019-06-14T17:44:28.000Z</published>
    <updated>2019-06-14T17:45:45.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>本文所有节点 OS 均为 CentOS 7.4 。</p><h3 id="1-关闭-selinux"><a href="#1-关闭-selinux" class="headerlink" title="1.关闭 selinux"></a>1.关闭 selinux</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/selinux/config </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of three two values:</span></span><br><span class="line"><span class="comment">#     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="comment">#     minimum - Modification of targeted policy. Only selected processes are protected. </span></span><br><span class="line"><span class="comment">#     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node211 ~]<span class="comment"># getenforce </span></span><br><span class="line">Disabled</span><br></pre></td></tr></table></figure><h3 id="2-关于-firewalld"><a href="#2-关于-firewalld" class="headerlink" title="2. 关于 firewalld"></a>2. 关于 firewalld</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br></pre></td></tr></table></figure><h3 id="3-安装必要-yum-源：epel-release"><a href="#3-安装必要-yum-源：epel-release" class="headerlink" title="3. 安装必要 yum 源：epel-release"></a>3. 安装必要 yum 源：epel-release</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install epel-release</span></span><br><span class="line">[root@node211 ~]<span class="comment"># ls /etc/yum.repos.d/epel.repo </span></span><br><span class="line">/etc/yum.repos.d/epel.repo</span><br></pre></td></tr></table></figure><h3 id="4-关闭节点-swap-空间"><a href="#4-关闭节点-swap-空间" class="headerlink" title="4. 关闭节点 swap 空间"></a>4. 关闭节点 swap 空间</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/fstab </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># /etc/fstab</span></span><br><span class="line"><span class="comment"># Created by anaconda on Thu Jun 13 09:45:52 2019</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Accessible filesystems, by reference, are maintained under '/dev/disk'</span></span><br><span class="line"><span class="comment"># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">/dev/mapper/centos-root /                       xfs     defaults        0 0</span><br><span class="line">UUID=c0f0a31a-0c36-42cf-b52a-8f3b027ef948 /boot                   xfs     defaults        0 0</span><br><span class="line"><span class="comment">#/dev/mapper/centos-swap swap                    swap    defaults        0 0</span></span><br><span class="line">[root@node211 ~]<span class="comment"># free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.7G        102M        3.3G        8.3M        230M        3.3G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure><h3 id="5-安装-docker-ce"><a href="#5-安装-docker-ce" class="headerlink" title="5. 安装 docker-ce"></a>5. 安装 docker-ce</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># </span></span><br><span class="line">[root@node211 ~]<span class="comment"># head -n 6 /etc/yum.repos.d/docker-ce.repo</span></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line">[root@node211 ~]<span class="comment"># rpm -q docker</span></span><br><span class="line">docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64</span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl start docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status docker</span></span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-06-14 19:48:40 CST; 3s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line"> Main PID: 11488 (dockerd-current)</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           ├─11488 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --<span class="built_in">exec</span>-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr...</span><br><span class="line">           └─11495 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-r...</span><br><span class="line"></span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.282909889+08:00"</span> level=info msg=<span class="string">"Docker daemon"</span> commit=<span class="string">"b2f74b2/1.13.1"</span> graphdriver=overlay2 version=1.13.1</span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.293315055+08:00"</span> level=info msg=<span class="string">"API listen on /var/run/docker.sock"</span></span><br><span class="line">Jun 14 19:48:40 node211 systemd[1]: Started Docker Application Container Engine.</span><br></pre></td></tr></table></figure><h3 id="6-开启必要系统参数-sysctl"><a href="#6-开启必要系统参数-sysctl" class="headerlink" title="6. 开启必要系统参数 sysctl"></a>6. 开启必要系统参数 sysctl</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># sysctl -p</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br></pre></td></tr></table></figure><h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>因为 kubeadm 官方文档中没有详细步骤，因此相关描述尽量具体到命令行。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.211</td><td>master</td></tr><tr><td>192.168.77.212</td><td>master</td></tr><tr><td>192.168.77.213</td><td>master </td></tr><tr><td>192.168.77.214</td><td>node</td></tr></tbody></table><h3 id="1-安装-kubeadm"><a href="#1-安装-kubeadm" class="headerlink" title="1. 安装 kubeadm"></a>1. 安装 kubeadm</h3><p>所有节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/yum.repos.d/kubernetes.repo </span></span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">proxy=socks5://127.0.0.1:1080</span><br><span class="line">[root@node211 ~]<span class="comment"># yum install kubeadm kubelet</span></span><br><span class="line">[root@node211 ~]<span class="comment"># which kubeadm </span></span><br><span class="line">/usr/bin/kubeadm</span><br></pre></td></tr></table></figure><h3 id="2-安装-keepalived"><a href="#2-安装-keepalived" class="headerlink" title="2. 安装 keepalived"></a>2. 安装 keepalived</h3><p>所有 master 节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install keepalived </span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install keepalived</span></span><br></pre></td></tr></table></figure><p>编辑 node211 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 100         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编辑 node212 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 90         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编辑 node213 配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 70         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>node211, node212, node213 重启 keepalived：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl restart keepalived</span></span><br></pre></td></tr></table></figure><p>因为 node211 优先级最高，此时 VIP 192.168.77.219 应该在 node211 节点，查看 node211 节点 IP：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># ip ad </span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 52:54:00:42:fd:a6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.77.211/20 brd 192.168.79.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.77.219/20 scope global secondary eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::5554:b212:7895:c8ad/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::3e97:25b9:cc1a:809c/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::7a4f:3726:af17:18bf/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </span><br><span class="line">    link/ether 02:42:6f:0e:81:59 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>配置无异常，node211,node212,node213 设置开机自启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br></pre></td></tr></table></figure><h3 id="3-安装-haproxy"><a href="#3-安装-haproxy" class="headerlink" title="3. 安装 haproxy"></a>3. 安装 haproxy</h3><p>所有 master 节点执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install haproxy</span></span><br></pre></td></tr></table></figure><p>编辑所有 master 节点配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/haproxy/haproxy.cfg </span></span><br><span class="line">global</span><br><span class="line">        chroot  /var/lib/haproxy</span><br><span class="line">        daemon</span><br><span class="line">        group haproxy</span><br><span class="line">        user haproxy</span><br><span class="line">        <span class="built_in">log</span> 127.0.0.1:514 local0 warning</span><br><span class="line">        pidfile /var/lib/haproxy.pid</span><br><span class="line">        maxconn 20000</span><br><span class="line">        spread-checks 3</span><br><span class="line">        nbproc 8</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">        <span class="built_in">log</span>     global</span><br><span class="line">        mode    tcp</span><br><span class="line">        retries 3</span><br><span class="line">        option redispatch</span><br><span class="line"></span><br><span class="line">listen https-apiserver</span><br><span class="line">        <span class="built_in">bind</span> *:8443</span><br><span class="line">        mode tcp</span><br><span class="line">        balance roundrobin</span><br><span class="line">        timeout server 900s</span><br><span class="line">        timeout connect 15s</span><br><span class="line"></span><br><span class="line">        server m1 192.168.77.211:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m2 192.168.77.212:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m3 192.168.77.213:6443 check port 6443 inter 5000 fall 5</span><br></pre></td></tr></table></figure><p>所有 master 节点启动 haproxy，并设置 开机自启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br></pre></td></tr></table></figure><h3 id="4-编写-kubeadm-配置文件"><a href="#4-编写-kubeadm-配置文件" class="headerlink" title="4. 编写 kubeadm 配置文件"></a>4. 编写 kubeadm 配置文件</h3><p>在 node211 节点编写 kubeadm 配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat kubeadm-init.yaml</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">"192.168.77.219:8443"</span></span><br><span class="line">dns:</span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">kubernetesVersion: v1.14.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: <span class="string">"10.123.0.0/16"</span></span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: <span class="string">"ipvs"</span></span><br></pre></td></tr></table></figure><h3 id="5-初始化"><a href="#5-初始化" class="headerlink" title="5. 初始化"></a>5. 初始化</h3><p>在 node211 节点执行初始化操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubeadm init --config=kubeadm-init.yaml --experimental-upload-certs</span></span><br><span class="line">[init] Using Kubernetes version: v1.14.3</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node211"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node211"</span>: lookup node211 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node211 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.211 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"sa"</span> key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"kubelet.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[etcd] Creating static Pod manifest <span class="keyword">for</span> <span class="built_in">local</span> etcd <span class="keyword">in</span> <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[<span class="built_in">wait</span>-control-plane] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="string">"/etc/kubernetes/manifests"</span>. This can take up to 4m0s</span><br><span class="line">[kubelet-check] Initial timeout of 40s passed.</span><br><span class="line">[apiclient] All control plane components are healthy after 107.014141 seconds</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap <span class="string">"kubelet-config-1.14"</span> <span class="keyword">in</span> namespace kube-system with the configuration <span class="keyword">for</span> the kubelets <span class="keyword">in</span> the cluster</span><br><span class="line">[upload-certs] Storing the certificates <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: ptuvy5.hl4rzxugpxpgkgkh</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstrap-token] creating the <span class="string">"cluster-info"</span> ConfigMap <span class="keyword">in</span> the <span class="string">"kube-public"</span> namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following <span class="built_in">command</span> on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">    --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted <span class="keyword">in</span> two hours; If necessary, you can use </span><br><span class="line"><span class="string">"kubeadm init phase upload-certs --experimental-upload-certs"</span> to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS     ROLES    AGE   VERSION</span><br><span class="line">node211   NotReady   master   82s   v1.14.3</span><br></pre></td></tr></table></figure><h3 id="6-部署-flannel-网络插件"><a href="#6-部署-flannel-网络插件" class="headerlink" title="6. 部署 flannel 网络插件"></a>6. 部署 flannel 网络插件</h3><p>在 node211 节点部署 flannel 插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure><p>查看部署状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            0/1     Pending   0          3m17s</span><br><span class="line">coredns-d5947d4b-zdptx            0/1     Pending   0          3m17s</span><br><span class="line">etcd-node211                      1/1     Running   0          2m48s</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          2m28s</span><br><span class="line">kube-controller-manager-node211   1/1     Running   0          2m59s</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          36s</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          3m16s</span><br><span class="line">kube-scheduler-node211            1/1     Running   0          2m41s</span><br></pre></td></tr></table></figure><h3 id="7-添加其他-master-节点"><a href="#7-添加其他-master-节点" class="headerlink" title="7. 添加其他 master 节点"></a>7. 添加其他 master 节点</h3><p>按照 node211 初始化提示，在 node212 节点及 node213 节点添加到集群，角色为 master：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node212"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node212"</span>: lookup node212 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_sh ip_vs_rr ip_vs_wrr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node212 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.212 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   7m48s   v1.14.3</span><br><span class="line">node212   Ready    master   66s     v1.14.3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node213"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node213"</span>: lookup node213 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node213 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.213 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>按照说明，拷贝 kubectl 配置文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   11m     v1.14.3</span><br><span class="line">node212   Ready    master   4m39s   v1.14.3</span><br><span class="line">node213   Ready    master   72s     v1.14.3</span><br></pre></td></tr></table></figure><h3 id="8-添加其他-node-节点"><a href="#8-添加其他-node-节点" class="headerlink" title="8. 添加其他 node 节点"></a>8. 添加其他 node 节点</h3><p>按照 node211 初始化提示，添加 node214 节点到集群，角色为 node：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@node214 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 </span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node214"</span> could not be reached</span><br><span class="line">[WARNING Hostname]: hostname <span class="string">"node214"</span>: lookup node214 on 192.168.64.215:53: no such host</span><br><span class="line">[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> on the control-plane to see this node join the cluster.</span><br></pre></td></tr></table></figure><p>至此 kubeadm 配合 keepalived &amp; haproxy 搭建高可用集群就完成了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   4h10m   v1.14.3</span><br><span class="line">node212   Ready    master   4h3m    v1.14.3</span><br><span class="line">node213   Ready    master   4h      v1.14.3</span><br><span class="line">node214   Ready    &lt;none&gt;   3h57m   v1.14.3</span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            1/1     Running   0          4h10m</span><br><span class="line">coredns-d5947d4b-zdptx            1/1     Running   0          4h10m</span><br><span class="line">etcd-node211                      1/1     Running   0          4h9m</span><br><span class="line">etcd-node212                      1/1     Running   0          4h3m</span><br><span class="line">etcd-node213                      1/1     Running   0          4h</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          4h9m</span><br><span class="line">kube-apiserver-node212            1/1     Running   1          4h3m</span><br><span class="line">kube-apiserver-node213            1/1     Running   0          3h59m</span><br><span class="line">kube-controller-manager-node211   1/1     Running   1          4h9m</span><br><span class="line">kube-controller-manager-node212   1/1     Running   0          4h2m</span><br><span class="line">kube-controller-manager-node213   1/1     Running   0          3h59m</span><br><span class="line">kube-flannel-ds-amd64-gchpj       1/1     Running   0          4h</span><br><span class="line">kube-flannel-ds-amd64-mx44p       1/1     Running   0          3h57m</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          4h7m</span><br><span class="line">kube-flannel-ds-amd64-x9rm7       1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-fj448                  1/1     Running   0          4h</span><br><span class="line">kube-proxy-jmhm7                  1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-s7jdf                  1/1     Running   0          3h57m</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          4h10m</span><br><span class="line">kube-scheduler-node211            1/1     Running   1          4h9m</span><br><span class="line">kube-scheduler-node212            1/1     Running   0          4h2m</span><br><span class="line">kube-scheduler-node213            1/1     Running   0          3h59m</span><br></pre></td></tr></table></figure><h3 id="HA-机制"><a href="#HA-机制" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>由集群节点上运行的 keepalived &amp; haproxy 提供 VIP &amp; LB，集群中所有节点的 kubelet 连接至 VIP:<haproxy port> EndPoints。</haproxy></p><p>当 VIP 所在节点发生故障，VIP 切换到集群中其他 master 节点，即可正常提供服务。</p><h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><ol><li>kubeadm 需要正常网络支持，需要确保自己处于正常网络环境下；</li><li>kubeadm 在添加节点时，有可能会 hang 住，未查明原因；</li><li>kubeadm 默认生成证书有效期为 1年，若想要修改，则需要手动生成证书替换；</li><li>…</li></ol><h2 id="kubespray"><a href="#kubespray" class="headerlink" title="kubespray"></a>kubespray</h2><p>因为 kubespray 项目主要使用 ansible 配合 kubeadm 部署，具体内容可以直接查看 github 文档，因此不详细记录具体步骤。</p><h3 id="环境信息-1"><a href="#环境信息-1" class="headerlink" title="环境信息"></a>环境信息</h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.201</td><td>master</td></tr><tr><td>192.168.77.202</td><td>master</td></tr><tr><td>192.168.77.203</td><td>master </td></tr><tr><td>192.168.77.204</td><td>node</td></tr></tbody></table><h3 id="1-安装-kubespray"><a href="#1-安装-kubespray" class="headerlink" title="1. 安装 kubespray"></a>1. 安装 kubespray</h3><p>在 GitHub <a href="https://github.com/kubernetes-sigs/kubespray/releases" target="_blank" rel="noopener">项目链接</a>上下载最新 Release 版本代码。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 ~]<span class="comment"># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.10.3.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="2-安装必要依赖"><a href="#2-安装必要依赖" class="headerlink" title="2. 安装必要依赖"></a>2. 安装必要依赖</h3><p>项目依赖于 Python3，所以这里采用 Python3.6 版本进行安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36-pip</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># pip3 install -r requirements.txt</span></span><br></pre></td></tr></table></figure><ol start="3"><li>生成 ansible inventory</li></ol><p>项目默认提供了一个 Python 脚本用于自动生成 inventory，该脚本生成 inventory 通常需要根据实际情况自己调整。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cp -rfp inventory/sample inventory/mycluster</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># declare -a IPS=(192.168.77.201 192.168.77.202 192.168.77.203 192.168.77.203)</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py $&#123;IPS[@]&#125;</span></span><br><span class="line">DEBUG: Adding group all</span><br><span class="line">DEBUG: Adding group kube-master</span><br><span class="line">DEBUG: Adding group kube-node</span><br><span class="line">DEBUG: Adding group etcd</span><br><span class="line">DEBUG: Adding group k8s-cluster</span><br><span class="line">DEBUG: Adding group calico-rr</span><br><span class="line">DEBUG: Skipping existing host 192.168.77.203.</span><br><span class="line">DEBUG: adding host node1 to group all</span><br><span class="line">DEBUG: adding host node2 to group all</span><br><span class="line">DEBUG: adding host node3 to group all</span><br><span class="line">DEBUG: adding host node1 to group etcd</span><br><span class="line">DEBUG: adding host node2 to group etcd</span><br><span class="line">DEBUG: adding host node3 to group etcd</span><br><span class="line">DEBUG: adding host node1 to group kube-master</span><br><span class="line">DEBUG: adding host node2 to group kube-master</span><br><span class="line">DEBUG: adding host node1 to group kube-node</span><br><span class="line">DEBUG: adding host node2 to group kube-node</span><br><span class="line">DEBUG: adding host node3 to group kube-node</span><br></pre></td></tr></table></figure><p>查看生成 inventory 结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cat inventory/mycluster/hosts.yml </span></span><br><span class="line">all:</span><br><span class="line">  hosts:</span><br><span class="line">    node1:</span><br><span class="line">      ansible_host: 192.168.77.201</span><br><span class="line">      ip: 192.168.77.201</span><br><span class="line">      access_ip: 192.168.77.201</span><br><span class="line">    node2:</span><br><span class="line">      ansible_host: 192.168.77.202</span><br><span class="line">      ip: 192.168.77.202</span><br><span class="line">      access_ip: 192.168.77.202</span><br><span class="line">    node3:</span><br><span class="line">      ansible_host: 192.168.77.203</span><br><span class="line">      ip: 192.168.77.203</span><br><span class="line">      access_ip: 192.168.77.203</span><br><span class="line">  children:</span><br><span class="line">    kube-master:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">    kube-node:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    etcd:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    k8s-cluster:</span><br><span class="line">      children:</span><br><span class="line">        kube-master:</span><br><span class="line">        kube-node:</span><br><span class="line">    calico-rr:</span><br><span class="line">      hosts: &#123;&#125;</span><br></pre></td></tr></table></figure><p>可以看到跟我们计划中的有所差别，根据实际情况调整 kube-master 数量即可。</p><h3 id="4-编写部署配置参数"><a href="#4-编写部署配置参数" class="headerlink" title="4. 编写部署配置参数"></a>4. 编写部署配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/all/all.yml</code> 路径下包含了一些全局配置，比如 proxy 之类的，可以手动调整。</p><h3 id="5-编写-k8s-配置参数"><a href="#5-编写-k8s-配置参数" class="headerlink" title="5. 编写 k8s 配置参数"></a>5. 编写 k8s 配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml</code> 路径下包含了 k8s 所有配置项，根据实际情况编辑修改。</p><h3 id="6-部署"><a href="#6-部署" class="headerlink" title="6. 部署"></a>6. 部署</h3><p>在所有准备工作完成后，执行部署操作。</p><p>注意， Kubespray 部署的前提条件是你的网络是一个正常的网络，可以正常访问所有网站，若无法访问，则根据自身实际情况，调整配置，配置路径为： <code>roles/download/defaults/main.yml</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml</span><br></pre></td></tr></table></figure><p>等待部署完成即可。</p><h3 id="HA-机制-1"><a href="#HA-机制-1" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>集群中所有的 Node 节点自己启动一个 Nginx Static Pod，用于代理转发，将所有指定 <code>127.0.0.1:6443</code> 的请求转发至所有 master 节点真实 apiserver ，这样所有的 kubelet 只需要自己节点即可，无需其他节点参与。</p><h3 id="坑-1"><a href="#坑-1" class="headerlink" title="坑"></a>坑</h3><ol><li>CentOS 默认 Python2.7，需要单独安装 Python3.6</li><li>通过 pip 安装依赖，部分软件包需要 gcc,python36-devel,openssl-devel 等依赖包，需要根据错误提示自行安装，文档中没有提到</li><li>默认会安装 docker &amp; containerd 服务，但是 containerd 服务未设置开机自启动，会导致 docker 无法自动运行</li><li>在安装过程中，会安装 selinux 相应 Python 库，但是该依赖未在 <code>requirements.txt</code> 声明</li><li>…</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论是直接只用 kubeadm + vip 方式部署 HA 集群，还是通过 Kubespray 部署，在网络正常情况下，是很快可以完成的。</p><p>在使用 kubeadm 过程中，因为无需引入第三方依赖库，导致整体流程顺畅，体验极佳。</p><p>在 Kubespray 过程中，因为采用 Python3 方式，但相关依赖又未显示声明，导致部署过程繁琐。但是也比较好理解，Kubespray 作为一个致力于部署企业级 k8s 集群的项目，需要处理大量的边界条件了，这个项目中 YAML 就写了 15k 行，可见一斑。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;p&gt;本文所有节点 OS 均为 CentOS 7.4 。&lt;/p&gt;
&lt;h3 id=&quot;1-关闭-selinux&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://zdyxry.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何查看磁盘扇区大小</title>
    <link href="https://zdyxry.github.io/2019/06/05/%E6%9F%A5%E7%9C%8B%E7%A3%81%E7%9B%98%E6%89%87%E5%8C%BA%E5%A4%A7%E5%B0%8F/"/>
    <id>https://zdyxry.github.io/2019/06/05/查看磁盘扇区大小/</id>
    <published>2019-06-05T12:45:06.000Z</published>
    <updated>2019-06-05T12:54:40.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="磁盘扇区"><a href="#磁盘扇区" class="headerlink" title="磁盘扇区"></a>磁盘扇区</h2><p>引用维基百科：</p><blockquote><p>In computer disk storage, a sector is a subdivision of a track on a magnetic disk or optical disc. Each sector stores a fixed amount of user-accessible data, traditionally 512 bytes for hard disk drives (HDDs) and 2048 bytes for CD-ROMs and DVD-ROMs. Newer HDDs use 4096-byte (4 KiB) sectors, which are known as the Advanced Format (AF).</p></blockquote><p>扇区大小常见的可以分为 512 bytes, 2048 bytes 和 4096 bytes。</p><h2 id="查看扇区大小"><a href="#查看扇区大小" class="headerlink" title="查看扇区大小"></a>查看扇区大小</h2><p>通过 lsblk 命令可以查看。</p><p>物理扇区 512 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:51:59 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdg</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdg           512     512</span><br><span class="line">├─sdg1        512     512</span><br><span class="line">├─sdg2        512     512</span><br><span class="line">├─sdg3        512     512</span><br><span class="line">└─sdg4        512     512</span><br></pre></td></tr></table></figure><p>物理扇区 4096 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:52:03 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdb</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdb          4096     512</span><br><span class="line">├─sdb1       4096     512</span><br><span class="line">├─sdb2       4096     512</span><br><span class="line">├─sdb3       4096     512</span><br><span class="line">└─sdb4       4096     512</span><br></pre></td></tr></table></figure><p>CDROM 比较特殊，是 2048 byte：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 21:07:35 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sr0</span><br><span class="line">NAME PHY-SEC LOG-SEC</span><br><span class="line">sr0     2048    2048</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;磁盘扇区&quot;&gt;&lt;a href=&quot;#磁盘扇区&quot; class=&quot;headerlink&quot; title=&quot;磁盘扇区&quot;&gt;&lt;/a&gt;磁盘扇区&lt;/h2&gt;&lt;p&gt;引用维基百科：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In computer disk storage, a sector
      
    
    </summary>
    
    
      <category term="Hardware" scheme="https://zdyxry.github.io/tags/Hardware/"/>
    
  </entry>
  
</feed>

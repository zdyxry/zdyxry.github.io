<!doctype html><html lang=en><head><title>Kubernetes 实战-高可用集群部署（无LB） · Yiran's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Yiran Zhou"><meta name=description content="背景 Link to heading 之前写过一篇《Kubernetes 实战-高可用集群部署》 博客讲 K8S 高可用配置，当时采用的方式是使用 keepalived 配合 HAProxy 自建 LB 的方式，但是最近发现无论是 Kubespray 还是 Rancher RKE 都没有采用这种方式，而是在 worker node 上配置 nginx/haproxy 代理 APIServer 达到目的。今天来手动配置下这种方式，了解下注意事项。
本文配置方式参考 Kubespray，主要是想加深自己对这方面的理解，如果只是单纯的想要搭建环境，那么直接使用 Kubespray 就好。
环境 Link to heading IP role 192.168.17.11 ControlPlane1 192.168.17.12 ControlPlane2 192.168.17.13 ControlPlane3 192.168.17.14 WorkerNode1 准备工作 Link to heading 软件安装 Link to heading 这部分准备工作与之前一样，没什么不同，因此不详细说明，需要安装 kubectl,kubeadm,kubelet 。
部署 Link to heading ControlPlane1 Link to heading kubeadm 配置文件如下：
[root@install1 17:55:09 tmp]$cat kubeadm."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 实战-高可用集群部署（无LB）"><meta name=twitter:description content="背景 Link to heading 之前写过一篇《Kubernetes 实战-高可用集群部署》 博客讲 K8S 高可用配置，当时采用的方式是使用 keepalived 配合 HAProxy 自建 LB 的方式，但是最近发现无论是 Kubespray 还是 Rancher RKE 都没有采用这种方式，而是在 worker node 上配置 nginx/haproxy 代理 APIServer 达到目的。今天来手动配置下这种方式，了解下注意事项。
本文配置方式参考 Kubespray，主要是想加深自己对这方面的理解，如果只是单纯的想要搭建环境，那么直接使用 Kubespray 就好。
环境 Link to heading IP role 192.168.17.11 ControlPlane1 192.168.17.12 ControlPlane2 192.168.17.13 ControlPlane3 192.168.17.14 WorkerNode1 准备工作 Link to heading 软件安装 Link to heading 这部分准备工作与之前一样，没什么不同，因此不详细说明，需要安装 kubectl,kubeadm,kubelet 。
部署 Link to heading ControlPlane1 Link to heading kubeadm 配置文件如下：
[root@install1 17:55:09 tmp]$cat kubeadm."><meta property="og:title" content="Kubernetes 实战-高可用集群部署（无LB）"><meta property="og:description" content="背景 Link to heading 之前写过一篇《Kubernetes 实战-高可用集群部署》 博客讲 K8S 高可用配置，当时采用的方式是使用 keepalived 配合 HAProxy 自建 LB 的方式，但是最近发现无论是 Kubespray 还是 Rancher RKE 都没有采用这种方式，而是在 worker node 上配置 nginx/haproxy 代理 APIServer 达到目的。今天来手动配置下这种方式，了解下注意事项。
本文配置方式参考 Kubespray，主要是想加深自己对这方面的理解，如果只是单纯的想要搭建环境，那么直接使用 Kubespray 就好。
环境 Link to heading IP role 192.168.17.11 ControlPlane1 192.168.17.12 ControlPlane2 192.168.17.13 ControlPlane3 192.168.17.14 WorkerNode1 准备工作 Link to heading 软件安装 Link to heading 这部分准备工作与之前一样，没什么不同，因此不详细说明，需要安装 kubectl,kubeadm,kubelet 。
部署 Link to heading ControlPlane1 Link to heading kubeadm 配置文件如下：
[root@install1 17:55:09 tmp]$cat kubeadm."><meta property="og:type" content="article"><meta property="og:url" content="https://zdyxry.github.io/2019/12/19/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%A0LB/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-12-19T21:38:16+00:00"><meta property="article:modified_time" content="2019-12-19T21:38:16+00:00"><link rel=canonical href=https://zdyxry.github.io/2019/12/19/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%A0LB/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.01bd429dda63a16d76996eaf0b8da061429b76e714515cb1b246aac7fe7f4b2a.css integrity="sha256-Ab1CndpjoW12mW6vC42gYUKbducUUVyxskaqx/5/Syo=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/yiran.png sizes=32x32><link rel=icon type=image/png href=/images/yiran.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.101.0"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Yiran's Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=/friends/>Friends</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://zdyxry.github.io/2019/12/19/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%A0LB/>Kubernetes 实战-高可用集群部署（无LB）</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-12-19T21:38:16Z>December 19, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
6-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/Kubernetes/>Kubernetes</a></span></div></div></header><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#环境>环境</a></li><li><a href=#准备工作>准备工作</a><ul><li><a href=#软件安装>软件安装</a></li></ul></li><li><a href=#部署>部署</a><ul><li><a href=#controlplane1>ControlPlane1</a></li><li><a href=#controlplane2>ControlPlane2</a></li><li><a href=#controlplane3>ControlPlane3</a></li><li><a href=#node1>node1</a></li></ul></li><li><a href=#配置修改>配置修改</a><ul><li><a href=#controlplane>ControlPlane</a></li><li><a href=#worker>Worker</a></li></ul></li><li><a href=#总结>总结</a></li><li><a href=#参考链接>参考链接</a></li></ul></nav><div class=post-content><h2 id=背景>背景
<a class=heading-link href=#%e8%83%8c%e6%99%af><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>之前写过一篇<a href=https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/>《Kubernetes 实战-高可用集群部署》</a> 博客讲 K8S 高可用配置，当时采用的方式是使用 keepalived 配合 HAProxy 自建 LB 的方式，但是最近发现无论是 Kubespray 还是 Rancher RKE 都没有采用这种方式，而是在 worker node 上配置 nginx/haproxy 代理 APIServer 达到目的。今天来手动配置下这种方式，了解下注意事项。</p><p>本文配置方式参考 Kubespray，主要是想加深自己对这方面的理解，如果只是单纯的想要搭建环境，那么直接使用 Kubespray 就好。</p><h2 id=环境>环境
<a class=heading-link href=#%e7%8e%af%e5%a2%83><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><table><thead><tr><th>IP</th><th>role</th></tr></thead><tbody><tr><td>192.168.17.11</td><td>ControlPlane1</td></tr><tr><td>192.168.17.12</td><td>ControlPlane2</td></tr><tr><td>192.168.17.13</td><td>ControlPlane3</td></tr><tr><td>192.168.17.14</td><td>WorkerNode1</td></tr></tbody></table><h2 id=准备工作>准备工作
<a class=heading-link href=#%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=软件安装>软件安装
<a class=heading-link href=#%e8%bd%af%e4%bb%b6%e5%ae%89%e8%a3%85><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>这部分准备工作与之前一样，没什么不同，因此不详细说明，需要安装 kubectl,kubeadm,kubelet 。</p><h2 id=部署>部署
<a class=heading-link href=#%e9%83%a8%e7%bd%b2><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=controlplane1>ControlPlane1
<a class=heading-link href=#controlplane1><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>kubeadm 配置文件如下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install1 17:55:09 tmp]$cat kubeadm.yaml
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiServer: {}
</span></span><span style=display:flex><span>apiVersion: kubeadm.k8s.io/v1beta1
</span></span><span style=display:flex><span>certificatesDir: <span style=font-style:italic>&#34;&#34;</span>
</span></span><span style=display:flex><span>clusterName: test
</span></span><span style=display:flex><span>controlPlaneEndpoint: <span style=font-style:italic>&#34;192.168.17.11:6443&#34;</span>
</span></span><span style=display:flex><span>controllerManager: {}
</span></span><span style=display:flex><span>dns:
</span></span><span style=display:flex><span>  type: <span style=font-style:italic>&#34;&#34;</span>
</span></span><span style=display:flex><span>etcd: {}
</span></span><span style=display:flex><span>imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
</span></span><span style=display:flex><span>kind: ClusterConfiguration
</span></span><span style=display:flex><span>kubernetesVersion: <span style=font-style:italic>&#34;&#34;</span>
</span></span><span style=display:flex><span>networking:
</span></span><span style=display:flex><span>  dnsDomain: <span style=font-style:italic>&#34;&#34;</span>
</span></span><span style=display:flex><span>  podSubnet: 10.244.0.0/16
</span></span><span style=display:flex><span>  serviceSubnet: <span style=font-style:italic>&#34;&#34;</span>
</span></span><span style=display:flex><span>scheduler: {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: kubeadm.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: InitConfiguration
</span></span><span style=display:flex><span>localAPIEndpoint:
</span></span><span style=display:flex><span>  advertiseAddress: <span style=font-style:italic>&#34;192.168.17.11&#34;</span>
</span></span><span style=display:flex><span>  bindPort: 6443
</span></span><span style=display:flex><span>nodeRegistration:
</span></span><span style=display:flex><span>  name: <span style=font-style:italic>&#39;install1&#39;</span>
</span></span></code></pre></div><p>执行部署动作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install1 17:55:10 tmp]$kubeadm init --config kubeadm.yaml --upload-certs
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>You should now deploy a pod network to the cluster.
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
</span></span><span style=display:flex><span>  https://kubernetes.io/docs/concepts/cluster-administration/addons/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You can now join any number of the control-plane node running the following command on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  kubeadm join 192.168.17.11:6443 --token 0xsbed.rcfk0ew0nlgrpjv0 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --discovery-token-ca-cert-hash sha256:1be9652019ef048fdd1d16385907c519e5559a1b786aa3ff1c46eeb489e1a6ef <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --control-plane --certificate-key 2368ee7ebe6697164c46812e358539638de1d7665b3111afde949bce73275029
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use 
</span></span><span style=display:flex><span><span style=font-style:italic>&#34;kubeadm init phase upload-certs --upload-certs&#34;</span> to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm join 192.168.17.11:6443 --token 0xsbed.rcfk0ew0nlgrpjv0 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --discovery-token-ca-cert-hash sha256:1be9652019ef048fdd1d16385907c519e5559a1b786aa3ff1c46eeb489e1a6ef 
</span></span></code></pre></div><h3 id=controlplane2>ControlPlane2
<a class=heading-link href=#controlplane2><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>kubeadm 配置文件如下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install2 17:40:52 tmp]$cat kubeadm.yaml
</span></span><span style=display:flex><span>apiVersion: kubeadm.k8s.io/v1beta2
</span></span><span style=display:flex><span>kind: JoinConfiguration
</span></span><span style=display:flex><span>discovery:
</span></span><span style=display:flex><span>  bootstrapToken:
</span></span><span style=display:flex><span>    apiServerEndpoint: 192.168.17.11:6443
</span></span><span style=display:flex><span>    token: 0xsbed.rcfk0ew0nlgrpjv0
</span></span><span style=display:flex><span>    unsafeSkipCAVerification: true
</span></span><span style=display:flex><span>  timeout: 5m0s
</span></span><span style=display:flex><span>  tlsBootstrapToken: 0xsbed.rcfk0ew0nlgrpjv0
</span></span><span style=display:flex><span>controlPlane:
</span></span><span style=display:flex><span>  localAPIEndpoint:
</span></span><span style=display:flex><span>    advertiseAddress: 192.168.17.12
</span></span><span style=display:flex><span>    bindPort: 6443
</span></span><span style=display:flex><span>  certificateKey: 2368ee7ebe6697164c46812e358539638de1d7665b3111afde949bce73275029
</span></span><span style=display:flex><span>nodeRegistration:
</span></span><span style=display:flex><span>  name: installer2
</span></span></code></pre></div><p>执行部署动作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install2 17:40:58 tmp]$kubeadm join --config kubeadm.yaml
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>This node has joined the cluster and a new control plane instance was created:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and approval was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>* Control plane (master) label and taint were applied to the new node.
</span></span><span style=display:flex><span>* The Kubernetes control plane instances scaled up.
</span></span><span style=display:flex><span>* A new etcd member was added to the local/stacked etcd cluster.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start administering your cluster from this node, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>        sudo chown <span style=font-weight:700>$(</span>id -u<span style=font-weight:700>)</span>:<span style=font-weight:700>$(</span>id -g<span style=font-weight:700>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> to see this node join the cluster.
</span></span></code></pre></div><h3 id=controlplane3>ControlPlane3
<a class=heading-link href=#controlplane3><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>kubeadm 配置文件如下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install3 17:47:24 tmp]$cat kubeadm.yaml
</span></span><span style=display:flex><span>apiVersion: kubeadm.k8s.io/v1beta2
</span></span><span style=display:flex><span>kind: JoinConfiguration
</span></span><span style=display:flex><span>discovery:
</span></span><span style=display:flex><span>  bootstrapToken:
</span></span><span style=display:flex><span>    apiServerEndpoint: 192.168.17.11:6443
</span></span><span style=display:flex><span>    token: 0xsbed.rcfk0ew0nlgrpjv0
</span></span><span style=display:flex><span>    unsafeSkipCAVerification: true
</span></span><span style=display:flex><span>  timeout: 5m0s
</span></span><span style=display:flex><span>  tlsBootstrapToken: 0xsbed.rcfk0ew0nlgrpjv0
</span></span><span style=display:flex><span>controlPlane:
</span></span><span style=display:flex><span>  localAPIEndpoint:
</span></span><span style=display:flex><span>    advertiseAddress: 192.168.17.13
</span></span><span style=display:flex><span>    bindPort: 6443
</span></span><span style=display:flex><span>  certificateKey: 2368ee7ebe6697164c46812e358539638de1d7665b3111afde949bce73275029
</span></span><span style=display:flex><span>nodeRegistration:
</span></span><span style=display:flex><span>  name: installer2
</span></span></code></pre></div><p>执行部署动作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install3 17:47:25 tmp]$kubeadm join --config kubeadm.yaml
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>This node has joined the cluster and a new control plane instance was created:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and approval was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>* Control plane (master) label and taint were applied to the new node.
</span></span><span style=display:flex><span>* The Kubernetes control plane instances scaled up.
</span></span><span style=display:flex><span>* A new etcd member was added to the local/stacked etcd cluster.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start administering your cluster from this node, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>        sudo chown <span style=font-weight:700>$(</span>id -u<span style=font-weight:700>)</span>:<span style=font-weight:700>$(</span>id -g<span style=font-weight:700>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> to see this node join the cluster.
</span></span></code></pre></div><h3 id=node1>node1
<a class=heading-link href=#node1><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>3 个 ControlPlane 节点已经部署完成，我们可以通过 kubectl 命令获取对应的状态信息，因为没有部署 CNI，所以这里还是 NotReady 的：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install1 18:00:50 tmp]$kubectl get node -o wide
</span></span><span style=display:flex><span>NAME         STATUS     ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                        CONTAINER-RUNTIME
</span></span><span style=display:flex><span>install1     NotReady   master   5m2s    v1.16.3   172.18.19.11    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.1.2.el7.smartx.1.x86_64   docker://19.3.5
</span></span><span style=display:flex><span>installer2   NotReady   master   4m22s   v1.16.3   192.168.17.12   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.1.2.el7.smartx.1.x86_64   docker://19.3.5
</span></span><span style=display:flex><span>installer3   NotReady   master   3m1s    v1.16.3   192.168.17.13   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.1.2.el7.smartx.1.x86_64   docker://19.3.5
</span></span><span style=display:flex><span>[root@install1 18:00:53 tmp]$
</span></span></code></pre></div><p>现在要来部署 worker node，当我们有可用的 LB，时，我们无需关心 APIServer 的可用性，由 LB 来决定。那么现在没有可用的 LB，我们的 worker node 想要与 k8s 进行通信，就需要每个 worker node 自身启动一个 proxy，将自己的 kubelet 连接到自己本地，然后通过 proxy 到集群中已有的 APIServer 。</p><h4 id=配置-proxy>配置 Proxy
<a class=heading-link href=#%e9%85%8d%e7%bd%ae-proxy><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>这里以 nginx 举例：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install4 18:05:03 ~]$mkdir /etc/nginx
</span></span><span style=display:flex><span>[root@install4 18:06:02 ~]$chmod 700 /etc/nginx
</span></span><span style=display:flex><span>[root@install4 18:14:23 ~]$cat /etc/nginx/nginx.conf
</span></span><span style=display:flex><span>error_log stderr notice;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>worker_processes 2;
</span></span><span style=display:flex><span>worker_rlimit_nofile 130048;
</span></span><span style=display:flex><span>worker_shutdown_timeout 10s;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>events {
</span></span><span style=display:flex><span>  multi_accept on;
</span></span><span style=display:flex><span>  use epoll;
</span></span><span style=display:flex><span>  worker_connections 16384;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>stream {
</span></span><span style=display:flex><span>  upstream kube_apiserver {
</span></span><span style=display:flex><span>    least_conn;
</span></span><span style=display:flex><span>    server 192.168.17.11:6443;
</span></span><span style=display:flex><span>    server 192.168.17.12:6443;
</span></span><span style=display:flex><span>    server 192.168.17.13:6443;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  server {
</span></span><span style=display:flex><span>    listen        127.0.0.1:6443;
</span></span><span style=display:flex><span>    proxy_pass    kube_apiserver;
</span></span><span style=display:flex><span>    proxy_timeout 10m;
</span></span><span style=display:flex><span>    proxy_connect_timeout 1s;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>http {
</span></span><span style=display:flex><span>  aio threads;
</span></span><span style=display:flex><span>  aio_write on;
</span></span><span style=display:flex><span>  tcp_nopush on;
</span></span><span style=display:flex><span>  tcp_nodelay on;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  keepalive_timeout 5m;
</span></span><span style=display:flex><span>  keepalive_requests 100;
</span></span><span style=display:flex><span>  reset_timedout_connection on;
</span></span><span style=display:flex><span>  server_tokens off;
</span></span><span style=display:flex><span>  autoindex off;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  server {
</span></span><span style=display:flex><span>    listen 8081;
</span></span><span style=display:flex><span>    location /healthz {
</span></span><span style=display:flex><span>      access_log off;
</span></span><span style=display:flex><span>      <span style=font-weight:700>return</span> 200;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    location /stub_status {
</span></span><span style=display:flex><span>      stub_status on;
</span></span><span style=display:flex><span>      access_log off;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>[root@install4 18:18:29 manifests]$vim /etc/sysctl.conf 
</span></span><span style=display:flex><span>[root@install4 18:18:54 manifests]$sysctl -p 
</span></span><span style=display:flex><span>net.ipv4.ip_forward = 1
</span></span><span style=display:flex><span>net.ipv4.ip_local_reserved_ports = 30000-32767
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-iptables = 1
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-arptables = 1
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></code></pre></div><h4 id=加入-k8s-集群>加入 K8s 集群
<a class=heading-link href=#%e5%8a%a0%e5%85%a5-k8s-%e9%9b%86%e7%be%a4><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>执行部署动作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install4 18:31:10 tmp]$kubeadm join 192.168.17.11:6443 --token 0xsbed.rcfk0ew0nlgrpjv0 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --discovery-token-ca-cert-hash sha256:1be9652019ef048fdd1d16385907c519e5559a1b786aa3ff1c46eeb489e1a6ef 
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks
</span></span><span style=display:flex><span>        [WARNING IsDockerSystemdCheck]: detected <span style=font-style:italic>&#34;cgroupfs&#34;</span> as the Docker cgroup driver. The recommended driver is <span style=font-style:italic>&#34;systemd&#34;</span>. Please follow the guide at https://kubernetes.io/docs/setup
</span></span><span style=display:flex><span>/cri/
</span></span><span style=display:flex><span>        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.5. Latest validated version: 18.09
</span></span><span style=display:flex><span>        [WARNING Hostname]: hostname <span style=font-style:italic>&#34;install4&#34;</span> could not be reached
</span></span><span style=display:flex><span>        [WARNING Hostname]: hostname <span style=font-style:italic>&#34;install4&#34;</span>: lookup install4 on 192.168.31.215:53: no such host
</span></span><span style=display:flex><span>[preflight] Reading configuration from the cluster...
</span></span><span style=display:flex><span>[preflight] FYI: You can look at this config file with <span style=font-style:italic>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span style=display:flex><span>[kubelet-start] Downloading configuration <span style=font-weight:700>for</span> the kubelet from the <span style=font-style:italic>&#34;kubelet-config-1.16&#34;</span> ConfigMap in the kube-system namespace
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet configuration to file <span style=font-style:italic>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet environment file with flags to file <span style=font-style:italic>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Activating the kubelet service
</span></span><span style=display:flex><span>[kubelet-start] Waiting <span style=font-weight:700>for</span> the kubelet to perform the TLS Bootstrap...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>This node has joined the cluster:
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and a response was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> on the control-plane to see this node join the cluster.
</span></span></code></pre></div><p>此时在 K8s 集群中存在 4个节点，其中 3 个是 ControlPlane 节点，1 个是 worker 节点，但是此时 worker 节点是连接的 192.168.17.11:6443 APIServer，如果 这个 APIServer 故障了，那么的 worker 节点就无法正常与集群通讯了，需要进行以下操作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@install4 18:37:51 kubernetes]$systemctl stop kubelet docker
</span></span><span style=display:flex><span>[root@install4 18:38:09 manifests]$pwd
</span></span><span style=display:flex><span>/etc/kubernetes/manifests
</span></span><span style=display:flex><span>[root@install4 18:38:12 manifests]$cat nginx-proxy.yml 
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx-proxy
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    addonmanager.kubernetes.io/mode: Reconcile
</span></span><span style=display:flex><span>    k8s-app: kube-nginx
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hostNetwork: true
</span></span><span style=display:flex><span>  dnsPolicy: ClusterFirstWithHostNet
</span></span><span style=display:flex><span>  nodeSelector:
</span></span><span style=display:flex><span>    beta.kubernetes.io/os: linux
</span></span><span style=display:flex><span>  priorityClassName: system-node-critical
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: nginx-proxy
</span></span><span style=display:flex><span>    image: docker.io/library/nginx:1.17
</span></span><span style=display:flex><span>    imagePullPolicy: IfNotPresent
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 25m
</span></span><span style=display:flex><span>        memory: 32M
</span></span><span style=display:flex><span>    securityContext:
</span></span><span style=display:flex><span>      privileged: true
</span></span><span style=display:flex><span>    livenessProbe:
</span></span><span style=display:flex><span>      httpGet:
</span></span><span style=display:flex><span>        path: /healthz
</span></span><span style=display:flex><span>        port: 8081
</span></span><span style=display:flex><span>    readinessProbe:
</span></span><span style=display:flex><span>      httpGet:
</span></span><span style=display:flex><span>        path: /healthz
</span></span><span style=display:flex><span>        port: 8081
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - mountPath: /etc/nginx
</span></span><span style=display:flex><span>      name: etc-nginx
</span></span><span style=display:flex><span>      readOnly: true
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: etc-nginx
</span></span><span style=display:flex><span>    hostPath:
</span></span><span style=display:flex><span>      path: /etc/nginx
</span></span></code></pre></div><p>修改 kubelet 配置文件，将 Server 字段替换为 <code>127.0.0.1:6443</code> ，启动 kubelet 和 docker 即可。</p><h2 id=配置修改>配置修改
<a class=heading-link href=#%e9%85%8d%e7%bd%ae%e4%bf%ae%e6%94%b9><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>最开始我以为到这里就结束了，结果在自己测试的过程中，发现了一点问题，使用 kubeadm 部署集群，在添加 ControlPlane 的时候，会将 kubelet 所连接的 APIServer 配置为 controlPlaneEndpoint 地址，这在没有 LB 的场景下是致命的，单点故障。</p><p>所以我们需要依次的修改所有节点的 kubelet 配置，修改内容如下：</p><h3 id=controlplane>ControlPlane
<a class=heading-link href=#controlplane><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>因为所有的 ControlPlane 节点都会自己运行 APIServer，那么 kubelet 连接自身即可。</p><h3 id=worker>Worker
<a class=heading-link href=#worker><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Worker 节点因为没有 APIServer，所以我们刚刚通过配置本地的 Proxy 来达到相同的目的，这里无需再次修改。</p><h2 id=总结>总结
<a class=heading-link href=#%e6%80%bb%e7%bb%93><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>到这里我们的集群已经配置完成了，无论是 ControlPlane 节点故障，还是 Worker 节点故障，都不是单点的，可以起到高可用的作用。这种方式其实是没有 LB 的一种妥协，这里引用 Kubespray 关于 HA 模式的文档：</p><blockquote><p>K8s components require a loadbalancer to access the apiservers via a reverse proxy. Kubespray includes support for an nginx-based proxy that resides on each non-master Kubernetes node. This is referred to as localhost loadbalancing. It is less efficient than a dedicated load balancer because it creates extra health checks on the Kubernetes apiserver, but is more practical for scenarios where an external LB or virtual IP management is inconvenient.</p></blockquote><p>如果有 LB 肯定还是优先 LB 的，但是在没有 LB 的情况下我们如何选择，就看我们自己的场景了，如果真的是自建 Keepalived & HAProxy ，那么之后的集群管理是一个很麻烦的事情，而且 Keepalived 在使用上没有那么的友好，尤其是网络环境复杂之后（我之前碰到过一次 ovs & keepalived 不工作的情况）。</p><p>目前社区中关于 K8S 生命周期管理的工具越来越多，但是如果仔细看看，其实大家的做法都是大同小异。还是要自己了解一下具体的问题点在哪里，如何解决才是硬道理。</p><h2 id=参考链接>参考链接
<a class=heading-link href=#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><a href=https://github.com/kubernetes-sigs/kubespray>https://github.com/kubernetes-sigs/kubespray</a></li></ul></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//zdyxry.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>©
2016 -
2023
Yiran Zhou
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G0433XDZ3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G0433XDZ3V",{anonymize_ip:!1})}</script></body></html>
<!doctype html><html lang=zh-cn><head><title>Kubernetes 实战-集群部署 · Yiran's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Yiran Zhou"><meta name=description content="背景 链接到标题 本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。
说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。
本文并不会介绍具体的部署步骤，望周知。
Kubernetes 主要组件 链接到标题 因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：
Master 链接到标题 apiserver： 集群中所有其他组件通过 apiserver 进行交互
scheduler： 按照 Pod 配置来对 Pod 进行节点调度
controller-manager：负责节点管理，资源的具体创建动作， desired state management 具体实行者
etcd：用于存储集群中数据的键值存储
Node 链接到标题 kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器
kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信
容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 实战-集群部署"><meta name=twitter:description content="背景 链接到标题 本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。
说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。
本文并不会介绍具体的部署步骤，望周知。
Kubernetes 主要组件 链接到标题 因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：
Master 链接到标题 apiserver： 集群中所有其他组件通过 apiserver 进行交互
scheduler： 按照 Pod 配置来对 Pod 进行节点调度
controller-manager：负责节点管理，资源的具体创建动作， desired state management 具体实行者
etcd：用于存储集群中数据的键值存储
Node 链接到标题 kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器
kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信
容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O"><meta property="og:title" content="Kubernetes 实战-集群部署"><meta property="og:description" content="背景 链接到标题 本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。
说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。
本文并不会介绍具体的部署步骤，望周知。
Kubernetes 主要组件 链接到标题 因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：
Master 链接到标题 apiserver： 集群中所有其他组件通过 apiserver 进行交互
scheduler： 按照 Pod 配置来对 Pod 进行节点调度
controller-manager：负责节点管理，资源的具体创建动作， desired state management 具体实行者
etcd：用于存储集群中数据的键值存储
Node 链接到标题 kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器
kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信
容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O"><meta property="og:type" content="article"><meta property="og:url" content="https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-05-31T21:22:51+00:00"><meta property="article:modified_time" content="2019-05-31T21:22:51+00:00"><link rel=canonical href=https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.76ce9bad7ac9bd368d486c6e91e7e0906fff71d9d35ccbf93959a375e2bf50e5.css integrity="sha256-ds6brXrJvTaNSGxukefgkG//cdnTXMv5OVmjdeK/UOU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/yiran.png sizes=32x32><link rel=icon type=image/png href=/images/yiran.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.101.0"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Yiran's Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=/friends/>Friends</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/>Kubernetes 实战-集群部署</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-05-31T21:22:51Z>May 31, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
阅读时间：2 分钟</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/Kubernetes/>Kubernetes</a></span></div></div></header><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#kubernetes-主要组件>Kubernetes 主要组件</a><ul><li><a href=#master>Master</a></li><li><a href=#node>Node</a></li></ul></li><li><a href=#kubernetes-集群准备>Kubernetes 集群准备</a><ul><li><a href=#所需资源>所需资源</a></li><li><a href=#部署方式>部署方式</a></li></ul></li><li><a href=#服务运行方式>服务运行方式</a><ul><li><a href=#host-service>Host Service</a></li><li><a href=#docker>Docker</a></li><li><a href=#static-pod>Static Pod</a></li></ul></li><li><a href=#ha-配置>HA 配置</a></li><li><a href=#总结>总结</a></li></ul></nav><div class=post-content><h2 id=背景>背景
<a class=heading-link href=#%e8%83%8c%e6%99%af><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。</p><p>说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。</p><p>本文并不会介绍具体的部署步骤，望周知。</p><h2 id=kubernetes-主要组件>Kubernetes 主要组件
<a class=heading-link href=#kubernetes-%e4%b8%bb%e8%a6%81%e7%bb%84%e4%bb%b6><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：</p><h3 id=master>Master
<a class=heading-link href=#master><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><ul><li><p>apiserver： 集群中所有其他组件通过 apiserver 进行交互</p></li><li><p>scheduler： 按照 Pod 配置来对 Pod 进行节点调度</p></li><li><p>controller-manager：负责节点管理，资源的具体创建动作， <code>desired state management</code> 具体实行者</p></li><li><p>etcd：用于存储集群中数据的键值存储</p></li></ul><h3 id=node>Node
<a class=heading-link href=#node><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><ul><li><p>kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器</p></li><li><p>kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信</p></li><li><p>容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O</p></li></ul><h2 id=kubernetes-集群准备>Kubernetes 集群准备
<a class=heading-link href=#kubernetes-%e9%9b%86%e7%be%a4%e5%87%86%e5%a4%87><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><h3 id=所需资源>所需资源
<a class=heading-link href=#%e6%89%80%e9%9c%80%e8%b5%84%e6%ba%90><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>大部分的安装文档中，都会先写明 os 要求，计算 & 存储资源需求，k8s 自身对资源消耗很低，通常的 2c4g + 30GiB 足够运行起来。</p><p>上面说完了硬件资源，那么我们来说下软件资源， k8s 作为一个容器编排系统，它需要的软件资源也是很重要的一部分，这里我们来说一下网络部分。</p><p>假设我们集群中存在 5 个节点，使用 <code>kubeadm init</code> 方式部署集群，那么最基本的，需要 5 个节点的 IP。那如果是高可用的集群呢？我们需要加一个 VIP，也就是 6 个 IP 地址。</p><p>在考虑了 IP 地址之后，我们来说下网段划分，以 flannel 为例，在创建网络时，每个 k8s 节点都会分配一段子网用于 Pod 分配 IP，这里的网段是不可以跟宿主机的网络重叠的，所以这里的网络划分也是一个很重要的资源。</p><p>具体其他网络类型，我会在之后的网络部分详细的写一下。</p><h3 id=部署方式>部署方式
<a class=heading-link href=#%e9%83%a8%e7%bd%b2%e6%96%b9%e5%bc%8f><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>好了，现在我们已经有了资源了（无论是硬件资源还是软件资源），那么我们可以部署了，那此时采用什么方式部署，或者说怎么部署成了问题。</p><hr><p>首先说下最特殊的服务，kubelet，它作为节点的实际管控者，它如果运行在容器中，那么谁来控制kubelet 容器的启停呢？当节点故障恢复后，又如何自启动呢？最开始我使用 Rancher 部署的时候发现他们是将 kubelet 直接部署在容器中的，那是因为他们在节点上还有其他 Agent 用于管理节点，Rancher 相当于 k8s 集群之外的上帝，管控着一切。</p><p>当我们没有 Rancher 这类管理工具时，还是老老实实将 kubelet 以服务的形式部署在宿主机上吧。</p><hr><p>官方推荐使用 <code>kubeadm</code> 进行集群部署，简单快捷，只是还在快速迭代中，存在较多不确定性，那么现在那些大厂是如何部署的呢？</p><p>我花了点时间阅读了下 Github 上面一些关于 k8s 部署项目，简单的罗列一下：</p><table><thead><tr><th>项目名称</th><th>项目地址</th><th>星</th><th>服务运行方式</th><th>ha</th></tr></thead><tbody><tr><td>ansible-kubeadm</td><td><a href=https://github.com/4admin2root/ansible-kubeadm>https://github.com/4admin2root/ansible-kubeadm</a></td><td>3</td><td>Static Pod</td><td>-</td></tr><tr><td>ansible-kubeadm-ha-cluster</td><td><a href=https://github.com/sv01a/ansible-kubeadm-ha-cluster>https://github.com/sv01a/ansible-kubeadm-ha-cluster</a></td><td>5</td><td>Docker</td><td>keepvalied</td></tr><tr><td>kubeadm-playbook</td><td><a href=https://github.com/ReSearchITEng/kubeadm-playbook>https://github.com/ReSearchITEng/kubeadm-playbook</a></td><td>117</td><td>Static Pod</td><td>keepalived</td></tr><tr><td>Kubernetes-ansible</td><td><a href=https://github.com/zhangguanzhang/Kubernetes-ansible>https://github.com/zhangguanzhang/Kubernetes-ansible</a></td><td>208</td><td>Service</td><td>keepalived & Haproxy</td></tr><tr><td>kubeadm-ansible</td><td><a href=https://github.com/kairen/kubeadm-ansible>https://github.com/kairen/kubeadm-ansible</a></td><td>281</td><td>Static Pod</td><td>-</td></tr><tr><td>kubeadm-ha</td><td><a href=https://github.com/cookeem/kubeadm-ha>https://github.com/cookeem/kubeadm-ha</a></td><td>502</td><td>Service</td><td>keepalived & nginx</td></tr><tr><td>kubeasz</td><td><a href=https://github.com/easzlab/kubeasz>https://github.com/easzlab/kubeasz</a></td><td>2987</td><td>Service</td><td>keepalived & Haproxy</td></tr></tbody></table><p>可以看到虽然有些细微的差别，但是大家做的都围绕着一个目的，就是把上一节提到的 k8s 所有必要组件部署到集群中，我们根据服务运行方式和 HA 方式来说一下。</p><h2 id=服务运行方式>服务运行方式
<a class=heading-link href=#%e6%9c%8d%e5%8a%a1%e8%bf%90%e8%a1%8c%e6%96%b9%e5%bc%8f><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>根据我上面总结的各个项目，大体分为 3 类，分别是：Host Service、Docker、Static Pod。我们一个一个的过一下。</p><h3 id=host-service>Host Service
<a class=heading-link href=#host-service><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在没有容器的时代，我们要部署一个服务，都是采用 Host Service 方式，我们在宿主机的 OS 上配置一个服务，管理方式可能是 init.d ，也可能是 systemd 。k8s 所有的服务都可以通过 Host Service 方式运行。</p><p>优点：</p><p>在 systemd 大法加持下，所有服务均可通过 systemd 统一管理，可以配置随着系统进行启停。</p><p>缺点：</p><p>如果通过 systemd 方式部署，那么服务配置修改、服务升级是一个大麻烦。</p><h3 id=docker>Docker
<a class=heading-link href=#docker><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>既然我们觉得 Host Service 的方式服务配置修改、升级都比较麻烦，那我们直接通过 Docker 启动就好了，升级直接更新 image 重新启动，一切问题放佛都解决了。</p><p>但是考虑一个问题，当集群全部掉电，系统开机后，k8s 如何自启动？以 Docker 作为容器运行时的基础上，Docker 比较让人诟病的一点就是它是一个 Daemon，在这里反而是优点，我们可以设置 Docker 随开机自启动，并将 k8s 所有服务对应镜像设置为 <code>--restart=always</code>，就可以解决这个问题。</p><p>优点：</p><p>服务配置、升级方便。</p><p>缺点：</p><p>依赖于 Docker，若更换其他 CRI，无法处理极端情况。</p><h3 id=static-pod>Static Pod
<a class=heading-link href=#static-pod><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>最后我们来说说 Static Pod，这种方式是 kubeadm 目前所采用的方式，也是 k8s 所推荐的方式。</p><p>什么是 Static Pod？其实就是字面意义， <code>静态 Pod</code>，k8s 不去调度的 Pod，而是由 kubelet 直接管理。前面我们在讨论 kubelet 提到，kubelet 是以服务的形式运行在宿主机上的，那么也就是 kubelet 的启停是通过 systemd 控制的，不受 k8s 控制。</p><p>Static Pod 由 kubelet 控制，当 kubelet 启动后，会自动拉起 Static Pod，且 Static Pod 不受 k8s 控制，无论是通过 kubectl 进行删除，还是通过 docker 对该 Pod 进行 stop 动作，kubelet 都会保证 Static Pod 正常运行。</p><p>这个方式很适合我们来处理 k8s 自身的服务，比如 apiserver/scheduler ，每个 master 节点上通过 Static Pod 配置，当 kubelet 启动后，自动拉起 apiserver/scheduler 等服务，那么 k8s 集群也就自启动了，也就解决了我们上面提到的集群全部掉电的情况。</p><p>优点：</p><p>跟随 kubelet 启停，完美适配 k8s 升级场景。</p><p>缺点：</p><p>因为随着 kubelet 启停，所以导致更新 kubelet 配置时需要指定 manifest 文件路径。</p><h2 id=ha-配置>HA 配置
<a class=heading-link href=#ha-%e9%85%8d%e7%bd%ae><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>k8s 作为一个基础架构服务，它的可用性关乎着我们整个业务的稳定，所以一定要保证不会出现单点故障。</p><p>在公有云场景下，由公有云提供 LB 的支持，只要我们部署了多个 master 节点，就不用担心了。</p><p>那在裸机场景下怎么办呢？</p><p>目前通用解决方案是 VIP 配合 LB（也就是 keepalived & haproxy/nginx）。</p><p>keepalived 提供了 VIP。在 k8s 集群部署过程中，所有节点都指定 <code>controlPlaneEndpoint</code> 为 VIP，而 VIP 在集群中的一个 master 节点上。当 VIP 所在节点故障时， VIP 自动漂浮到集群中其他 master 节点上，保证高可用。</p><p>haproxy/nginx 作为 LB，当我们 k8s 集群中所有节点都连接一个 apiserver 时，通过 LB 按照既定策略将请求分发到集群中多个 master 节点上，保证性能。</p><h2 id=总结>总结
<a class=heading-link href=#%e6%80%bb%e7%bb%93><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>关于 k8s 集群的部署大概就是上述这些内容，具体的操作步骤都可以通过官网或者 github 找到相关资料。目前如果个人学习的话，直接通过 kubeadm 部署就好；如果想要在生产环境中部署，那么需要根据自身业务类型，仔细考虑自己是否需要 HA，如何配置 HA。</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//zdyxry.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>©
2016 -
2024
Yiran Zhou
·
技术支持 <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G0433XDZ3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G0433XDZ3V",{anonymize_ip:!1})}</script></body></html>
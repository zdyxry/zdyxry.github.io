<!doctype html><html lang=zh-cn><head><title>Kubernetes 实战-Leader 选举 · Yiran's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Yiran Zhou"><meta name=description content="背景 链接到标题 最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 v0.2.1 版本 ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。
K8S Leader 链接到标题 看过之前 K8S 实战系列的朋友应该记得我写过一篇 K8S 高可用部署的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。
我们这里有一个 3 个master 节点的集群：
[root@node70 21:01:01 ~]$kubectl get node NAME STATUS ROLES AGE VERSION node70 Ready master 64d v1.15.0 node71 Ready master 64d v1.15.0 node72 Ready master 64d v1.15.0 我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。
但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 实战-Leader 选举"><meta name=twitter:description content="背景 链接到标题 最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 v0.2.1 版本 ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。
K8S Leader 链接到标题 看过之前 K8S 实战系列的朋友应该记得我写过一篇 K8S 高可用部署的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。
我们这里有一个 3 个master 节点的集群：
[root@node70 21:01:01 ~]$kubectl get node NAME STATUS ROLES AGE VERSION node70 Ready master 64d v1.15.0 node71 Ready master 64d v1.15.0 node72 Ready master 64d v1.15.0 我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。
但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？"><meta property="og:title" content="Kubernetes 实战-Leader 选举"><meta property="og:description" content="背景 链接到标题 最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 v0.2.1 版本 ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。
K8S Leader 链接到标题 看过之前 K8S 实战系列的朋友应该记得我写过一篇 K8S 高可用部署的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。
我们这里有一个 3 个master 节点的集群：
[root@node70 21:01:01 ~]$kubectl get node NAME STATUS ROLES AGE VERSION node70 Ready master 64d v1.15.0 node71 Ready master 64d v1.15.0 node72 Ready master 64d v1.15.0 我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。
但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？"><meta property="og:type" content="article"><meta property="og:url" content="https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-12T22:05:22+00:00"><meta property="article:modified_time" content="2019-09-12T22:05:22+00:00"><link rel=canonical href=https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.76ce9bad7ac9bd368d486c6e91e7e0906fff71d9d35ccbf93959a375e2bf50e5.css integrity="sha256-ds6brXrJvTaNSGxukefgkG//cdnTXMv5OVmjdeK/UOU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/yiran.png sizes=32x32><link rel=icon type=image/png href=/images/yiran.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.101.0"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Yiran's Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=/friends/>Friends</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/>Kubernetes 实战-Leader 选举</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-09-12T22:05:22Z>September 12, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
阅读时间：3 分钟</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/Kubernetes/>Kubernetes</a></span></div></div></header><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#k8s-leader>K8S Leader</a></li><li><a href=#deployment-leader>Deployment Leader</a></li><li><a href=#leader-选举实现方式>Leader 选举实现方式</a><ul><li><a href=#代码内嵌选主逻辑>代码内嵌选主逻辑</a></li><li><a href=#sidecar>SideCar</a></li></ul></li><li><a href=#是否开启选举参数>是否开启选举参数</a></li><li><a href=#总结>总结</a></li><li><a href=#参考链接>参考链接</a></li></ul></nav><div class=post-content><h2 id=背景>背景
<a class=heading-link href=#%e8%83%8c%e6%99%af><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 <a href=https://github.com/kubernetes-sigs/cluster-api/releases/tag/v0.2.1>v0.2.1 版本</a> ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。</p><h2 id=k8s-leader>K8S Leader
<a class=heading-link href=#k8s-leader><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>看过之前 K8S 实战系列的朋友应该记得我写过一篇 <a href=https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/>K8S 高可用部署</a>的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。</p><p>我们这里有一个 3 个master 节点的集群：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@node70 21:01:01 ~]$kubectl get node
</span></span><span style=display:flex><span>NAME     STATUS   ROLES    AGE   VERSION
</span></span><span style=display:flex><span>node70   Ready    master   64d   v1.15.0
</span></span><span style=display:flex><span>node71   Ready    master   64d   v1.15.0
</span></span><span style=display:flex><span>node72   Ready    master   64d   v1.15.0
</span></span></code></pre></div><p>我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。</p><p>但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？</p><p>k8s 所有功能都是通过 <code>services</code> 对外暴露接口，而 <code>services</code> 对应的是具体的 <code>endpoints</code> ，那么来看下 scheduler 和 controller-manager 的 <code>endpoints</code> 是什么：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>[root@node70 21:04:46 ~]$kubectl -n kube-system describe endpoints kube-scheduler
</span></span><span style=display:flex><span>Name:         kube-scheduler
</span></span><span style=display:flex><span>Namespace:    kube-system
</span></span><span style=display:flex><span>Labels:       &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:  control-plane.alpha.kubernetes.io/leader:
</span></span><span style=display:flex><span>                {<span style=font-style:italic>&#34;holderIdentity&#34;</span>:<span style=font-style:italic>&#34;node70_ed12bf09-7aa3-47d6-9546-97752bb589b5&#34;</span>,<span style=font-style:italic>&#34;leaseDurationSeconds&#34;</span>:15,<span style=font-style:italic>&#34;acquireTime&#34;</span>:<span style=font-style:italic>&#34;2019-09-11T05:31:58Z&#34;</span>,<span style=font-style:italic>&#34;renewTime&#34;</span>...
</span></span><span style=display:flex><span>Subsets:
</span></span><span style=display:flex><span>Events:  &lt;none&gt;
</span></span><span style=display:flex><span>[root@node70 21:05:25 ~]$kubectl -n kube-system describe endpoints kube-controller-manager
</span></span><span style=display:flex><span>Name:         kube-controller-manager
</span></span><span style=display:flex><span>Namespace:    kube-system
</span></span><span style=display:flex><span>Labels:       &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:  control-plane.alpha.kubernetes.io/leader:
</span></span><span style=display:flex><span>                {<span style=font-style:italic>&#34;holderIdentity&#34;</span>:<span style=font-style:italic>&#34;node71_c8deeaea-2d66-4459-90ee-65c28563062f&#34;</span>,<span style=font-style:italic>&#34;leaseDurationSeconds&#34;</span>:15,<span style=font-style:italic>&#34;acquireTime&#34;</span>:<span style=font-style:italic>&#34;2019-09-12T12:44:15Z&#34;</span>,<span style=font-style:italic>&#34;renewTime&#34;</span>...
</span></span><span style=display:flex><span>Subsets:
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason          Age   From                     Message
</span></span><span style=display:flex><span>  ----    ------          ----  ----                     -------
</span></span><span style=display:flex><span>  Normal  LeaderElection  22m   kube-controller-manager  node71_c8deeaea-2d66-4459-90ee-65c28563062f became leader
</span></span></code></pre></div><p>可以看到关键字 <code>control-plane.alpha.kubernetes.io/leader</code> ，这两个组件是通过 leader 选举来从集群中多个节点选择一个执行具体动作，如果我们去看 <code>/etc/kubernetes/manifests/</code> 下的配置文件，会看到这行配置：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=font-weight:700>apiVersion</span>: v1
</span></span><span style=display:flex><span><span style=font-weight:700>kind</span>: Pod
</span></span><span style=display:flex><span><span style=font-weight:700>metadata</span>:
</span></span><span style=display:flex><span><span style=font-weight:700>creationTimestamp</span>: <span style=font-weight:700>null</span>
</span></span><span style=display:flex><span><span style=font-weight:700>labels</span>:
</span></span><span style=display:flex><span>  <span style=font-weight:700>component</span>: kube-controller-manager
</span></span><span style=display:flex><span>  <span style=font-weight:700>tier</span>: control-plane
</span></span><span style=display:flex><span><span style=font-weight:700>name</span>: kube-controller-manager
</span></span><span style=display:flex><span><span style=font-weight:700>namespace</span>: kube-system
</span></span><span style=display:flex><span><span style=font-weight:700>spec</span>:
</span></span><span style=display:flex><span><span style=font-weight:700>containers</span>:
</span></span><span style=display:flex><span>- <span style=font-weight:700>command</span>:
</span></span><span style=display:flex><span>  - kube-controller-manager
</span></span><span style=display:flex><span>  - --allocate-node-cidrs=true
</span></span><span style=display:flex><span><span style=font-weight:700>...</span>
</span></span><span style=display:flex><span>  - --leader-elect=true
</span></span></code></pre></div><p>通过在 YAML 中添加 <code>leader-elect=true</code> 来决定是否进行选主逻辑。而这个参数也是在执行 <code>kubeadm</code> 部署集群时就自动配置好了，无需手动配置。</p><h2 id=deployment-leader>Deployment Leader
<a class=heading-link href=#deployment-leader><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>我们先来说说 Deployment，Deployment 是从 ReplicaSet 进化来的，主要增加的功能有滚动更新、回滚、扩容所容等，可以说是我们日常使用 K8S 最常见的资源类型了。</p><p>那么当我们通过创建 Deployment 间接创建 ReplicaSet 时，我们有时候并不想所有的 ReplicaSet 中的 Pod 运行统一的逻辑。这时候我们就需要一种方式来选择（通知）某一个 Pod ，来确定这个 Pod 提供特殊功能，其他的 Pod 提供普通功能，也就是跟上述 k8s 实现方式一样，通过Leader 选举完成需求。</p><p>Leader 选举有很多方式，或是代码中内嵌选举逻辑，或者通过第三方服务，但是有两个的特点：</p><ol><li>Leader 在同一时间内是唯一的</li><li>当 Leader 所在 Pod 发生异常时，其他 Pod 要可以随时变为 Leader 。</li></ol><h2 id=leader-选举实现方式>Leader 选举实现方式
<a class=heading-link href=#leader-%e9%80%89%e4%b8%be%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8f><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><h3 id=代码内嵌选主逻辑>代码内嵌选主逻辑
<a class=heading-link href=#%e4%bb%a3%e7%a0%81%e5%86%85%e5%b5%8c%e9%80%89%e4%b8%bb%e9%80%bb%e8%be%91><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 Golang 中，k8s client-go 这个package 针对 Leader 相关功能进行了封装，支持3种锁资源，endpoint，configmap，lease，方便使用。</p><p>代码仓库：https://github.com/kubernetes/client-go/tree/master/tools/leaderelection</p><p>因为这次主要不是说具体实现，再加上我也没看过代码，这里先掠过。</p><h3 id=sidecar>SideCar
<a class=heading-link href=#sidecar><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>相比于代码中内嵌选主逻辑，使用 sidecar 就不用担心跨语言的问题了，使用起来也简单许多，我们现在用的这个项目实现：https://github.com/kubernetes-retired/contrib/blob/master/election/README.md</p><p>使用方式：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=font-weight:700>apiVersion</span>: extensions/v1beta1
</span></span><span style=display:flex><span><span style=font-weight:700>kind</span>: DaemonSet
</span></span><span style=display:flex><span><span style=font-weight:700>metadata</span>:
</span></span><span style=display:flex><span>  <span style=font-weight:700>creationTimestamp</span>: <span style=font-style:italic>&#34;2019-07-12T07:35:32Z&#34;</span>
</span></span><span style=display:flex><span>  <span style=font-weight:700>generation</span>: 6
</span></span><span style=display:flex><span>  <span style=font-weight:700>labels</span>:
</span></span><span style=display:flex><span>    <span style=font-weight:700>app</span>: yiran-test
</span></span><span style=display:flex><span>  <span style=font-weight:700>name</span>: yiran-test
</span></span><span style=display:flex><span>  <span style=font-weight:700>namespace</span>: default
</span></span><span style=display:flex><span><span style=font-weight:700>spec</span>:
</span></span><span style=display:flex><span>  <span style=font-weight:700>selector</span>:
</span></span><span style=display:flex><span>    <span style=font-weight:700>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=font-weight:700>app</span>: yiran-test
</span></span><span style=display:flex><span>  <span style=font-weight:700>template</span>:
</span></span><span style=display:flex><span>    <span style=font-weight:700>metadata</span>:
</span></span><span style=display:flex><span>      <span style=font-weight:700>annotations</span>:
</span></span><span style=display:flex><span>        <span style=font-weight:700>kubectl.kubernetes.io/restartedAt</span>: <span style=font-style:italic>&#34;2019-09-09T10:58:51+08:00&#34;</span>
</span></span><span style=display:flex><span>      <span style=font-weight:700>creationTimestamp</span>: <span style=font-weight:700>null</span>
</span></span><span style=display:flex><span>      <span style=font-weight:700>labels</span>:
</span></span><span style=display:flex><span>        <span style=font-weight:700>app</span>: yiran-test
</span></span><span style=display:flex><span>    <span style=font-weight:700>spec</span>:
</span></span><span style=display:flex><span>      <span style=font-weight:700>containers</span>:
</span></span><span style=display:flex><span>      - <span style=font-weight:700>args</span>:
</span></span><span style=display:flex><span>        - --election=elect-yiran-test
</span></span><span style=display:flex><span>        - --http=0.0.0.0:4444
</span></span><span style=display:flex><span>        <span style=font-weight:700>image</span>: leader-elector:0.5
</span></span><span style=display:flex><span>        <span style=font-weight:700>imagePullPolicy</span>: IfNotPresent
</span></span><span style=display:flex><span>        <span style=font-weight:700>name</span>: leader-elector
</span></span><span style=display:flex><span>      - <span style=font-weight:700>args</span>:
</span></span><span style=display:flex><span>        ...
</span></span></code></pre></div><p>在 Deployment 或 DaemonSet 中，添加 sidecar 指定端口，最终会在所有的 Pod 中选择一个 Leader 。
在业务代码中，只需要访问端口 4444 ，即可获取当前 Pod 中 Leader 信息，目前是通过 hostname 作为唯一标示的。</p><p>如果看了这个项目的代码，会发现它也是使用的 client go 中的实现，只是增加了一层 http server：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=font-weight:700>func</span> getCurrentLeader(electionId, namespace <span>string</span>, c client.Interface) (<span>string</span>, *api.Endpoints, <span>error</span>) {
</span></span><span style=display:flex><span>	endpoints, err := c.Endpoints(namespace).Get(electionId)
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> err != <span style=font-weight:700>nil</span> {
</span></span><span style=display:flex><span>		<span style=font-weight:700>return</span> <span style=font-style:italic>&#34;&#34;</span>, <span style=font-weight:700>nil</span>, err
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	val, found := endpoints.Annotations[leaderelection.LeaderElectionRecordAnnotationKey]
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> !found {
</span></span><span style=display:flex><span>		<span style=font-weight:700>return</span> <span style=font-style:italic>&#34;&#34;</span>, endpoints, <span style=font-weight:700>nil</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	electionRecord := leaderelection.LeaderElectionRecord{}
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> err := json.Unmarshal([]byte(val), &amp;electionRecord); err != <span style=font-weight:700>nil</span> {
</span></span><span style=display:flex><span>		<span style=font-weight:700>return</span> <span style=font-style:italic>&#34;&#34;</span>, <span style=font-weight:700>nil</span>, err
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=font-weight:700>return</span> electionRecord.HolderIdentity, endpoints, err
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=font-weight:700>func</span> main() {
</span></span><span style=display:flex><span>	flags.Parse(os.Args)
</span></span><span style=display:flex><span>	validateFlags()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	kubeClient, err := makeClient()
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> err != <span style=font-weight:700>nil</span> {
</span></span><span style=display:flex><span>		glog.Fatalf(<span style=font-style:italic>&#34;error connecting to the client: %v&#34;</span>, err)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	fn := <span style=font-weight:700>func</span>(str <span>string</span>) {
</span></span><span style=display:flex><span>		leader.Name = str
</span></span><span style=display:flex><span>		fmt.Printf(<span style=font-style:italic>&#34;%s is the leader\n&#34;</span>, leader.Name)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	e, err := election.NewElection(*name, *id, *namespace, *ttl, fn, kubeClient)
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> err != <span style=font-weight:700>nil</span> {
</span></span><span style=display:flex><span>		glog.Fatalf(<span style=font-style:italic>&#34;failed to create election: %v&#34;</span>, err)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=font-weight:700>go</span> election.RunElection(e)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>if</span> len(*addr) &gt; 0 {
</span></span><span style=display:flex><span>		http.HandleFunc(<span style=font-style:italic>&#34;/&#34;</span>, webHandler)
</span></span><span style=display:flex><span>		http.ListenAndServe(*addr, <span style=font-weight:700>nil</span>)
</span></span><span style=display:flex><span>	} <span style=font-weight:700>else</span> {
</span></span><span style=display:flex><span>		<span style=font-weight:700>select</span> {}
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=是否开启选举参数>是否开启选举参数
<a class=heading-link href=#%e6%98%af%e5%90%a6%e5%bc%80%e5%90%af%e9%80%89%e4%b8%be%e5%8f%82%e6%95%b0><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>那么我们说了这么多，在实际使用中是否应该开启选举参数呢？
这里要说一下我在文章开头提到的有意思的事情，我们来看下cluster-api 的 YAML 文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=font-weight:700>apiVersion</span>: apps/v1
</span></span><span style=display:flex><span><span style=font-weight:700>kind</span>: Deployment
</span></span><span style=display:flex><span><span style=font-weight:700>metadata</span>:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span><span style=font-weight:700>spec</span>:
</span></span><span style=display:flex><span>  <span style=font-weight:700>replicas</span>: 1
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  <span style=font-weight:700>template</span>:
</span></span><span style=display:flex><span>    <span style=font-weight:700>metadata</span>:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    <span style=font-weight:700>spec</span>:
</span></span><span style=display:flex><span>      <span style=font-weight:700>containers</span>:
</span></span><span style=display:flex><span>      - <span style=font-weight:700>args</span>:
</span></span><span style=display:flex><span>        - --enable-leader-election
</span></span><span style=display:flex><span>        <span style=font-weight:700>command</span>:
</span></span><span style=display:flex><span>        - /manager
</span></span><span style=display:flex><span>        <span style=font-weight:700>image</span>: us.gcr.io/k8s-artifacts-prod/cluster-api/cluster-api-controller:v0.2.1
</span></span><span style=display:flex><span>        <span style=font-weight:700>name</span>: manager
</span></span><span style=display:flex><span>        ...
</span></span></code></pre></div><p>创建的资源类型是 Deployment，replicas 设置为1，传递了参数 <code>--enable-leader-election</code>，当时我觉得有些奇怪，replicas是1 啊，也就是说一共只有1个 Pod，为啥还要开启 Leader 选举逻辑呢？如果说是为了之后的扩容那可以理解，但是在这份配置文件里，应该完全没必要。把这段配置发给 <a href=https://liqiang.io/>liqiang同学</a> 看，他也觉得有些怪怪的。</p><p>秉着不懂就问的精神，我去 Slack #cluster-api 中提出了我的疑问，得到了以下回复，我格式化一下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>@cha： 
</span></span><span style=display:flex><span>it&#39;s mostly set for best practice. If you scale up the system will still work. If you don&#39;t enable leader election and scale up you will have race conditions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@cha：
</span></span><span style=display:flex><span>but you&#39;re right. If you&#39;re running a single manager you don&#39;t need to enable leader election
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@jdetiber:
</span></span><span style=display:flex><span>With a Deployment you do, because on rolling out a change it will spin up the new RS in parallel with the other so there will be 2 pods running at the same time for a short while
</span></span></code></pre></div><p>其中 @cha 的回复跟我预想的差不多，主要处于之后的扩容考虑，需要防止竞争情况出现。但是 @jdetiber 提到了一点很关键，当 Deployment 进行滚动升级的时候，哪怕设置了 Replicas 为1，也会存在短时间同时存在 2 个 Pod 的情况，那么肯定会导致我们的代码逻辑错误，很重要（感叹号）。</p><h2 id=总结>总结
<a class=heading-link href=#%e6%80%bb%e7%bb%93><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>其实今天这篇文章主要是想记录下上面这段话的，自己在平时使用 k8s 过程中，貌似也仅仅是知道这些功能并使用，真正要开发部分功能时，考虑的边界条件太少了。虽然知道 Deployment 滚动升级会有同时存在 2 个 Pod 的情况，却完全没有想到需要配置 Leader 选举参数来保证代码正确运行，涨知识了。</p><h2 id=参考链接>参考链接
<a class=heading-link href=#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><ul><li><a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li></ul></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//zdyxry.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>©
2016 -
2023
Yiran Zhou
·
技术支持 <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G0433XDZ3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G0433XDZ3V",{anonymize_ip:!1})}</script></body></html>
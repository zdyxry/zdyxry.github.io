<!doctype html><html lang=zh-cn><head><title>Kubernetes 实战-高可用集群部署 · Yiran's Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Yiran Zhou"><meta name=description content="准备工作 链接到标题 本文所有节点 OS 均为 CentOS 7.4 。
1.关闭 selinux 链接到标题 所有节点执行：
[root@node211 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of three two values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 实战-高可用集群部署"><meta name=twitter:description content="准备工作 链接到标题 本文所有节点 OS 均为 CentOS 7.4 。
1.关闭 selinux 链接到标题 所有节点执行：
[root@node211 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of three two values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy."><meta property="og:title" content="Kubernetes 实战-高可用集群部署"><meta property="og:description" content="准备工作 链接到标题 本文所有节点 OS 均为 CentOS 7.4 。
1.关闭 selinux 链接到标题 所有节点执行：
[root@node211 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of three two values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy."><meta property="og:type" content="article"><meta property="og:url" content="https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-15T01:44:28+00:00"><meta property="article:modified_time" content="2019-06-15T01:44:28+00:00"><link rel=canonical href=https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.01bd429dda63a16d76996eaf0b8da061429b76e714515cb1b246aac7fe7f4b2a.css integrity="sha256-Ab1CndpjoW12mW6vC42gYUKbducUUVyxskaqx/5/Syo=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/yiran.png sizes=32x32><link rel=icon type=image/png href=/images/yiran.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.101.0"></head><body class="preload-transitions colorscheme-dark"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Yiran's Blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=/friends/>Friends</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/>Kubernetes 实战-高可用集群部署</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-06-15T01:44:28Z>June 15, 2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
阅读时间：20 分钟</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/Kubernetes/>Kubernetes</a></span></div></div></header><nav id=TableOfContents><ul><li><a href=#准备工作>准备工作</a><ul><li><a href=#1关闭-selinux>1.关闭 selinux</a></li><li><a href=#2-关于-firewalld>2. 关于 firewalld</a></li><li><a href=#3-安装必要-yum-源epel-release>3. 安装必要 yum 源：epel-release</a></li><li><a href=#4-关闭节点-swap-空间>4. 关闭节点 swap 空间</a></li><li><a href=#5-安装-docker-ce>5. 安装 docker-ce</a></li><li><a href=#6-开启必要系统参数-sysctl>6. 开启必要系统参数 sysctl</a></li></ul></li><li><a href=#kubeadm>kubeadm</a><ul><li><a href=#环境信息>环境信息</a></li><li><a href=#1-安装-kubeadm>1. 安装 kubeadm</a></li><li><a href=#2-安装-keepalived>2. 安装 keepalived</a></li><li><a href=#3-安装-haproxy>3. 安装 haproxy</a></li><li><a href=#4-编写-kubeadm-配置文件>4. 编写 kubeadm 配置文件</a></li><li><a href=#5-初始化>5. 初始化</a></li><li><a href=#6-部署-flannel-网络插件>6. 部署 flannel 网络插件</a></li><li><a href=#7-添加其他-master-节点>7. 添加其他 master 节点</a></li><li><a href=#8-添加其他-node-节点>8. 添加其他 node 节点</a></li><li><a href=#ha-机制>HA 机制</a></li><li><a href=#坑>坑</a></li></ul></li><li><a href=#kubespray>kubespray</a><ul><li><a href=#环境信息-1>环境信息</a></li><li><a href=#1-安装-kubespray>1. 安装 kubespray</a></li><li><a href=#2-安装必要依赖>2. 安装必要依赖</a></li><li><a href=#4-编写部署配置参数>4. 编写部署配置参数</a></li><li><a href=#5-编写-k8s-配置参数>5. 编写 k8s 配置参数</a></li><li><a href=#6-部署>6. 部署</a></li><li><a href=#ha-机制-1>HA 机制</a></li><li><a href=#坑-1>坑</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav><div class=post-content><h2 id=准备工作>准备工作
<a class=heading-link href=#%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>本文所有节点 OS 均为 CentOS 7.4 。</p><h3 id=1关闭-selinux>1.关闭 selinux
<a class=heading-link href=#1%e5%85%b3%e9%97%ad-selinux><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat /etc/selinux/config </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic># This file controls the state of SELinux on the system.</span>
</span></span><span style=display:flex><span><span style=font-style:italic># SELINUX= can take one of these three values:</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     enforcing - SELinux security policy is enforced.</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     permissive - SELinux prints warnings instead of enforcing.</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     disabled - No SELinux policy is loaded.</span>
</span></span><span style=display:flex><span>SELINUX=disabled
</span></span><span style=display:flex><span><span style=font-style:italic># SELINUXTYPE= can take one of three two values:</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     targeted - Targeted processes are protected,</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     minimum - Modification of targeted policy. Only selected processes are protected. </span>
</span></span><span style=display:flex><span><span style=font-style:italic>#     mls - Multi Level Security protection.</span>
</span></span><span style=display:flex><span>SELINUXTYPE=targeted 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># getenforce </span>
</span></span><span style=display:flex><span>Disabled
</span></span></code></pre></div><h3 id=2-关于-firewalld>2. 关于 firewalld
<a class=heading-link href=#2-%e5%85%b3%e4%ba%8e-firewalld><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl disable firewalld</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl stop firewalld</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl status firewalld</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl status firewalld</span>
</span></span><span style=display:flex><span>● firewalld.service - firewalld - dynamic firewall daemon
</span></span><span style=display:flex><span>   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
</span></span><span style=display:flex><span>   Active: inactive (dead)
</span></span><span style=display:flex><span>     Docs: man:firewalld(1)
</span></span></code></pre></div><h3 id=3-安装必要-yum-源epel-release>3. 安装必要 yum 源：epel-release
<a class=heading-link href=#3-%e5%ae%89%e8%a3%85%e5%bf%85%e8%a6%81-yum-%e6%ba%90epel-release><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># yum install epel-release</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># ls /etc/yum.repos.d/epel.repo </span>
</span></span><span style=display:flex><span>/etc/yum.repos.d/epel.repo
</span></span></code></pre></div><h3 id=4-关闭节点-swap-空间>4. 关闭节点 swap 空间
<a class=heading-link href=#4-%e5%85%b3%e9%97%ad%e8%8a%82%e7%82%b9-swap-%e7%a9%ba%e9%97%b4><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat /etc/fstab </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=font-style:italic># /etc/fstab</span>
</span></span><span style=display:flex><span><span style=font-style:italic># Created by anaconda on Thu Jun 13 09:45:52 2019</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#</span>
</span></span><span style=display:flex><span><span style=font-style:italic># Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;</span>
</span></span><span style=display:flex><span><span style=font-style:italic># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info</span>
</span></span><span style=display:flex><span><span style=font-style:italic>#</span>
</span></span><span style=display:flex><span>/dev/mapper/centos-root /                       xfs     defaults        0 0
</span></span><span style=display:flex><span>UUID=c0f0a31a-0c36-42cf-b52a-8f3b027ef948 /boot                   xfs     defaults        0 0
</span></span><span style=display:flex><span><span style=font-style:italic>#/dev/mapper/centos-swap swap                    swap    defaults        0 0</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># free -h</span>
</span></span><span style=display:flex><span>              total        used        free      shared  buff/cache   available
</span></span><span style=display:flex><span>Mem:           3.7G        102M        3.3G        8.3M        230M        3.3G
</span></span><span style=display:flex><span>Swap:            0B          0B          0B
</span></span></code></pre></div><h3 id=5-安装-docker-ce>5. 安装 docker-ce
<a class=heading-link href=#5-%e5%ae%89%e8%a3%85-docker-ce><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># </span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># head -n 6 /etc/yum.repos.d/docker-ce.repo</span>
</span></span><span style=display:flex><span>[docker-ce-stable]
</span></span><span style=display:flex><span>name=Docker CE Stable - $basearch
</span></span><span style=display:flex><span>baseurl=https://download.docker.com/linux/centos/7/$basearch/stable
</span></span><span style=display:flex><span>enabled=1
</span></span><span style=display:flex><span>gpgcheck=1
</span></span><span style=display:flex><span>gpgkey=https://download.docker.com/linux/centos/gpg
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># rpm -q docker</span>
</span></span><span style=display:flex><span>docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl enable docker</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl start docker</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl status docker</span>
</span></span><span style=display:flex><span>● docker.service - Docker Application Container Engine
</span></span><span style=display:flex><span>   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
</span></span><span style=display:flex><span>   Active: active (running) since Fri 2019-06-14 19:48:40 CST; 3s ago
</span></span><span style=display:flex><span>     Docs: http://docs.docker.com
</span></span><span style=display:flex><span> Main PID: 11488 (dockerd-current)
</span></span><span style=display:flex><span>   CGroup: /system.slice/docker.service
</span></span><span style=display:flex><span>           ├─11488 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr...
</span></span><span style=display:flex><span>           └─11495 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-r...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span style=font-style:italic>&#34;2019-06-14T19:48:40.282909889+08:00&#34;</span> level=info msg=<span style=font-style:italic>&#34;Docker daemon&#34;</span> commit=<span style=font-style:italic>&#34;b2f74b2/1.13.1&#34;</span> graphdriver=overlay2 version=1.13.1
</span></span><span style=display:flex><span>Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span style=font-style:italic>&#34;2019-06-14T19:48:40.293315055+08:00&#34;</span> level=info msg=<span style=font-style:italic>&#34;API listen on /var/run/docker.sock&#34;</span>
</span></span><span style=display:flex><span>Jun 14 19:48:40 node211 systemd[1]: Started Docker Application Container Engine.
</span></span></code></pre></div><h3 id=6-开启必要系统参数-sysctl>6. 开启必要系统参数 sysctl
<a class=heading-link href=#6-%e5%bc%80%e5%90%af%e5%bf%85%e8%a6%81%e7%b3%bb%e7%bb%9f%e5%8f%82%e6%95%b0-sysctl><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># sysctl -p</span>
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-ip6tables = 1
</span></span><span style=display:flex><span>net.bridge.bridge-nf-call-iptables = 1
</span></span></code></pre></div><h2 id=kubeadm>kubeadm
<a class=heading-link href=#kubeadm><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>因为 kubeadm 官方文档中没有详细步骤，因此相关描述尽量具体到命令行。</p><h3 id=环境信息>环境信息
<a class=heading-link href=#%e7%8e%af%e5%a2%83%e4%bf%a1%e6%81%af><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.211</td><td>master</td></tr><tr><td>192.168.77.212</td><td>master</td></tr><tr><td>192.168.77.213</td><td>master</td></tr><tr><td>192.168.77.214</td><td>node</td></tr></tbody></table><h3 id=1-安装-kubeadm>1. 安装 kubeadm
<a class=heading-link href=#1-%e5%ae%89%e8%a3%85-kubeadm><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat /etc/yum.repos.d/kubernetes.repo </span>
</span></span><span style=display:flex><span>[kubernetes]
</span></span><span style=display:flex><span>name=Kubernetes
</span></span><span style=display:flex><span>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span></span><span style=display:flex><span>enabled=1
</span></span><span style=display:flex><span>gpgcheck=1
</span></span><span style=display:flex><span>repo_gpgcheck=1
</span></span><span style=display:flex><span>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span><span style=display:flex><span>proxy=socks5://127.0.0.1:1080
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># yum install kubeadm kubelet</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># which kubeadm </span>
</span></span><span style=display:flex><span>/usr/bin/kubeadm
</span></span></code></pre></div><h3 id=2-安装-keepalived>2. 安装 keepalived
<a class=heading-link href=#2-%e5%ae%89%e8%a3%85-keepalived><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有 master 节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># yum install keepalived</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># yum install keepalived </span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># yum install keepalived</span>
</span></span></code></pre></div><p>编辑 node211 配置文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat /etc/keepalived/keepalived.conf </span>
</span></span><span style=display:flex><span>! Configuration File <span style=font-weight:700>for</span> keepalived
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>global_defs {
</span></span><span style=display:flex><span>   notification_email {
</span></span><span style=display:flex><span>        feng110498@163.com
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>   notification_email_from Alexandre.Cassen@firewall.loc
</span></span><span style=display:flex><span>   smtp_server 127.0.0.1
</span></span><span style=display:flex><span>   smtp_connect_timeout 30
</span></span><span style=display:flex><span>   router_id LVS_1
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vrrp_instance VI_1 {
</span></span><span style=display:flex><span>    state MASTER          
</span></span><span style=display:flex><span>    interface eth0
</span></span><span style=display:flex><span>    lvs_sync_daemon_inteface eth0
</span></span><span style=display:flex><span>    virtual_router_id 79
</span></span><span style=display:flex><span>    advert_int 1
</span></span><span style=display:flex><span>    priority 100         
</span></span><span style=display:flex><span>    authentication {
</span></span><span style=display:flex><span>        auth_type PASS
</span></span><span style=display:flex><span>        auth_pass 1111
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    virtual_ipaddress {
</span></span><span style=display:flex><span>      192.168.77.219/20
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>编辑 node212 配置文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># cat /etc/keepalived/keepalived.conf </span>
</span></span><span style=display:flex><span>! Configuration File <span style=font-weight:700>for</span> keepalived
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>global_defs {
</span></span><span style=display:flex><span>   notification_email {
</span></span><span style=display:flex><span>        feng110498@163.com
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>   notification_email_from Alexandre.Cassen@firewall.loc
</span></span><span style=display:flex><span>   smtp_server 127.0.0.1
</span></span><span style=display:flex><span>   smtp_connect_timeout 30
</span></span><span style=display:flex><span>   router_id LVS_1
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vrrp_instance VI_1 {
</span></span><span style=display:flex><span>    state MASTER          
</span></span><span style=display:flex><span>    interface eth0
</span></span><span style=display:flex><span>    lvs_sync_daemon_inteface eth0
</span></span><span style=display:flex><span>    virtual_router_id 79
</span></span><span style=display:flex><span>    advert_int 1
</span></span><span style=display:flex><span>    priority 90         
</span></span><span style=display:flex><span>    authentication {
</span></span><span style=display:flex><span>        auth_type PASS
</span></span><span style=display:flex><span>        auth_pass 1111
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    virtual_ipaddress {
</span></span><span style=display:flex><span>      192.168.77.219/20
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>编辑 node213 配置文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># cat /etc/keepalived/keepalived.conf </span>
</span></span><span style=display:flex><span>! Configuration File <span style=font-weight:700>for</span> keepalived
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>global_defs {
</span></span><span style=display:flex><span>   notification_email {
</span></span><span style=display:flex><span>        feng110498@163.com
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>   notification_email_from Alexandre.Cassen@firewall.loc
</span></span><span style=display:flex><span>   smtp_server 127.0.0.1
</span></span><span style=display:flex><span>   smtp_connect_timeout 30
</span></span><span style=display:flex><span>   router_id LVS_1
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vrrp_instance VI_1 {
</span></span><span style=display:flex><span>    state MASTER          
</span></span><span style=display:flex><span>    interface eth0
</span></span><span style=display:flex><span>    lvs_sync_daemon_inteface eth0
</span></span><span style=display:flex><span>    virtual_router_id 79
</span></span><span style=display:flex><span>    advert_int 1
</span></span><span style=display:flex><span>    priority 70         
</span></span><span style=display:flex><span>    authentication {
</span></span><span style=display:flex><span>        auth_type PASS
</span></span><span style=display:flex><span>        auth_pass 1111
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    virtual_ipaddress {
</span></span><span style=display:flex><span>      192.168.77.219/20
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>node211, node212, node213 重启 keepalived：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl restart keepalived</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># systemctl restart keepalived</span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># systemctl restart keepalived</span>
</span></span></code></pre></div><p>因为 node211 优先级最高，此时 VIP 192.168.77.219 应该在 node211 节点，查看 node211 节点 IP：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># ip ad </span>
</span></span><span style=display:flex><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1
</span></span><span style=display:flex><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span style=display:flex><span>    inet 127.0.0.1/8 scope host lo
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 ::1/128 scope host 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
</span></span><span style=display:flex><span>    link/ether 52:54:00:42:fd:a6 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    inet 192.168.77.211/20 brd 192.168.79.255 scope global eth0
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet 192.168.77.219/20 scope global secondary eth0
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::5554:b212:7895:c8ad/64 scope link tentative dadfailed 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::3e97:25b9:cc1a:809c/64 scope link tentative dadfailed 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::7a4f:3726:af17:18bf/64 scope link tentative dadfailed 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN 
</span></span><span style=display:flex><span>    link/ether 02:42:6f:0e:81:59 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    inet 172.17.0.1/16 scope global docker0
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span></code></pre></div><p>配置无异常，node211,node212,node213 设置开机自启动：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl enable keepalived</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># systemctl enable keepalived</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># systemctl enable keepalived</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.
</span></span></code></pre></div><h3 id=3-安装-haproxy>3. 安装 haproxy
<a class=heading-link href=#3-%e5%ae%89%e8%a3%85-haproxy><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>所有 master 节点执行：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># yum install haproxy</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># yum install haproxy</span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># yum install haproxy</span>
</span></span></code></pre></div><p>编辑所有 master 节点配置文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat /etc/haproxy/haproxy.cfg </span>
</span></span><span style=display:flex><span>global
</span></span><span style=display:flex><span>        chroot  /var/lib/haproxy
</span></span><span style=display:flex><span>        daemon
</span></span><span style=display:flex><span>        group haproxy
</span></span><span style=display:flex><span>        user haproxy
</span></span><span style=display:flex><span>        log 127.0.0.1:514 local0 warning
</span></span><span style=display:flex><span>        pidfile /var/lib/haproxy.pid
</span></span><span style=display:flex><span>        maxconn 20000
</span></span><span style=display:flex><span>        spread-checks 3
</span></span><span style=display:flex><span>        nbproc 8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>defaults
</span></span><span style=display:flex><span>        log     global
</span></span><span style=display:flex><span>        mode    tcp
</span></span><span style=display:flex><span>        retries 3
</span></span><span style=display:flex><span>        option redispatch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>listen https-apiserver
</span></span><span style=display:flex><span>        bind *:8443
</span></span><span style=display:flex><span>        mode tcp
</span></span><span style=display:flex><span>        balance roundrobin
</span></span><span style=display:flex><span>        timeout server 900s
</span></span><span style=display:flex><span>        timeout connect 15s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        server m1 192.168.77.211:6443 check port 6443 inter 5000 fall 5
</span></span><span style=display:flex><span>        server m2 192.168.77.212:6443 check port 6443 inter 5000 fall 5
</span></span><span style=display:flex><span>        server m3 192.168.77.213:6443 check port 6443 inter 5000 fall 5
</span></span></code></pre></div><p>所有 master 节点启动 haproxy，并设置 开机自启动：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl start haproxy </span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># systemctl enable haproxy</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># systemctl start haproxy </span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># systemctl enable haproxy</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># systemctl start haproxy </span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># systemctl enable haproxy</span>
</span></span><span style=display:flex><span>Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
</span></span></code></pre></div><h3 id=4-编写-kubeadm-配置文件>4. 编写 kubeadm 配置文件
<a class=heading-link href=#4-%e7%bc%96%e5%86%99-kubeadm-%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 node211 节点编写 kubeadm 配置文件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># cat kubeadm-init.yaml</span>
</span></span><span style=display:flex><span>apiVersion: kubeadm.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ClusterConfiguration
</span></span><span style=display:flex><span>apiServer:
</span></span><span style=display:flex><span>  timeoutForControlPlane: 4m0s
</span></span><span style=display:flex><span>certificatesDir: /etc/kubernetes/pki
</span></span><span style=display:flex><span>clusterName: kubernetes
</span></span><span style=display:flex><span>controlPlaneEndpoint: <span style=font-style:italic>&#34;192.168.77.219:8443&#34;</span>
</span></span><span style=display:flex><span>dns:
</span></span><span style=display:flex><span>  type: CoreDNS
</span></span><span style=display:flex><span>etcd:
</span></span><span style=display:flex><span>  local:
</span></span><span style=display:flex><span>    dataDir: /var/lib/etcd
</span></span><span style=display:flex><span>imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
</span></span><span style=display:flex><span>kubernetesVersion: v1.14.3
</span></span><span style=display:flex><span>networking:
</span></span><span style=display:flex><span>  dnsDomain: cluster.local
</span></span><span style=display:flex><span>  podSubnet: <span style=font-style:italic>&#34;10.123.0.0/16&#34;</span>
</span></span><span style=display:flex><span>scheduler: {}
</span></span><span style=display:flex><span>controllerManager: {}
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: kubeproxy.config.k8s.io/v1alpha1
</span></span><span style=display:flex><span>kind: KubeProxyConfiguration
</span></span><span style=display:flex><span>mode: <span style=font-style:italic>&#34;ipvs&#34;</span>
</span></span></code></pre></div><h3 id=5-初始化>5. 初始化
<a class=heading-link href=#5-%e5%88%9d%e5%a7%8b%e5%8c%96><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 node211 节点执行初始化操作：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubeadm init --config=kubeadm-init.yaml --experimental-upload-certs</span>
</span></span><span style=display:flex><span>[init] Using Kubernetes version: v1.14.3
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node211&#34;</span> could not be reached
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node211&#34;</span>: lookup node211 on 192.168.64.215:53: no such host
</span></span><span style=display:flex><span>	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span style=font-style:italic>&#39;systemctl enable kubelet.service&#39;</span>
</span></span><span style=display:flex><span>	[WARNING RequiredIPVSKernelModulesAvailable]: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh]
</span></span><span style=display:flex><span>or no builtin kernel IPVS support was found: map[ip_vs:{} ip_vs_rr:{} ip_vs_sh:{} ip_vs_wrr:{} nf_conntrack_ipv4:{}].
</span></span><span style=display:flex><span>However, these modules may be loaded automatically by kube-proxy <span style=font-weight:700>if</span> they are available on your system.
</span></span><span style=display:flex><span>To verify IPVS support:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   Run <span style=font-style:italic>&#34;lsmod | grep &#39;ip_vs|nf_conntrack&#39;&#34;</span> and verify each of the above modules are listed.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If they are not listed, you can use the following methods to load them:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. For each missing module run <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> (e.g., <span style=font-style:italic>&#39;modprobe ip_vs&#39;</span>, <span style=font-style:italic>&#39;modprobe ip_vs_rr&#39;</span>, ...)
</span></span><span style=display:flex><span>2. If <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> returns an error, you will need to install the missing module support <span style=font-weight:700>for</span> your kernel.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[preflight] Pulling images required <span style=font-weight:700>for</span> setting up a Kubernetes cluster
</span></span><span style=display:flex><span>[preflight] This might take a minute or two, depending on the speed of your internet connection
</span></span><span style=display:flex><span>[preflight] You can also perform this action in beforehand using <span style=font-style:italic>&#39;kubeadm config images pull&#39;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet environment file with flags to file <span style=font-style:italic>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet configuration to file <span style=font-style:italic>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Activating the kubelet service
</span></span><span style=display:flex><span>[certs] Using certificateDir folder <span style=font-style:italic>&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;ca&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] apiserver serving cert is signed <span style=font-weight:700>for</span> DNS names [node211 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.211 192.168.77.219]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-kubelet-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;front-proxy-ca&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;front-proxy-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/ca&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/server&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/server serving cert is signed <span style=font-weight:700>for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/peer&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/peer serving cert is signed <span style=font-weight:700>for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/healthcheck-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-etcd-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;sa&#34;</span> key and public key
</span></span><span style=display:flex><span>[kubeconfig] Using kubeconfig folder <span style=font-style:italic>&#34;/etc/kubernetes&#34;</span>
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;admin.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;kubelet.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;controller-manager.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;scheduler.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[control-plane] Using manifest folder <span style=font-style:italic>&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-apiserver&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-controller-manager&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-scheduler&#34;</span>
</span></span><span style=display:flex><span>[etcd] Creating static Pod manifest <span style=font-weight:700>for</span> local etcd in <span style=font-style:italic>&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style=display:flex><span>[wait-control-plane] Waiting <span style=font-weight:700>for</span> the kubelet to boot up the control plane as static Pods from directory <span style=font-style:italic>&#34;/etc/kubernetes/manifests&#34;</span>. This can take up to 4m0s
</span></span><span style=display:flex><span>[kubelet-check] Initial timeout of 40s passed.
</span></span><span style=display:flex><span>[apiclient] All control plane components are healthy after 107.014141 seconds
</span></span><span style=display:flex><span>[upload-config] storing the configuration used in ConfigMap <span style=font-style:italic>&#34;kubeadm-config&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[kubelet] Creating a ConfigMap <span style=font-style:italic>&#34;kubelet-config-1.14&#34;</span> in namespace kube-system with the configuration <span style=font-weight:700>for</span> the kubelets in the cluster
</span></span><span style=display:flex><span>[upload-certs] Storing the certificates in ConfigMap <span style=font-style:italic>&#34;kubeadm-certs&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[upload-certs] Using certificate key:
</span></span><span style=display:flex><span>1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node211 as control-plane by adding the label <span style=font-style:italic>&#34;node-role.kubernetes.io/master=&#39;&#39;&#34;</span>
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node211 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
</span></span><span style=display:flex><span>[bootstrap-token] Using token: ptuvy5.hl4rzxugpxpgkgkh
</span></span><span style=display:flex><span>[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
</span></span><span style=display:flex><span>[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order <span style=font-weight:700>for</span> nodes to get long term certificate credentials
</span></span><span style=display:flex><span>[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
</span></span><span style=display:flex><span>[bootstrap-token] configured RBAC rules to allow certificate rotation <span style=font-weight:700>for</span> all node client certificates in the cluster
</span></span><span style=display:flex><span>[bootstrap-token] creating the <span style=font-style:italic>&#34;cluster-info&#34;</span> ConfigMap in the <span style=font-style:italic>&#34;kube-public&#34;</span> namespace
</span></span><span style=display:flex><span>[addons] Applied essential addon: CoreDNS
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[addons] Applied essential addon: kube-proxy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Your Kubernetes control-plane has initialized successfully!
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start using your cluster, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>  sudo chown <span style=font-weight:700>$(</span>id -u<span style=font-weight:700>)</span>:<span style=font-weight:700>$(</span>id -g<span style=font-weight:700>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You should now deploy a pod network to the cluster.
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
</span></span><span style=display:flex><span>  https://kubernetes.io/docs/concepts/cluster-administration/addons/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You can now join any number of the control-plane node running the following command on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use 
</span></span><span style=display:flex><span><span style=font-style:italic>&#34;kubeadm init phase upload-certs --experimental-upload-certs&#34;</span> to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4
</span></span></code></pre></div><p>按照说明，拷贝 kubectl 配置文件并验证：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># mkdir -p $HOME/.kube</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubectl get node</span>
</span></span><span style=display:flex><span>NAME      STATUS     ROLES    AGE   VERSION
</span></span><span style=display:flex><span>node211   NotReady   master   82s   v1.14.3
</span></span></code></pre></div><h3 id=6-部署-flannel-网络插件>6. 部署 flannel 网络插件
<a class=heading-link href=#6-%e9%83%a8%e7%bd%b2-flannel-%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 node211 节点部署 flannel 插件：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span>
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io/flannel created
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io/flannel created
</span></span><span style=display:flex><span>serviceaccount/flannel created
</span></span><span style=display:flex><span>configmap/kube-flannel-cfg created
</span></span><span style=display:flex><span>daemonset.extensions/kube-flannel-ds-amd64 created
</span></span><span style=display:flex><span>daemonset.extensions/kube-flannel-ds-arm64 created
</span></span><span style=display:flex><span>daemonset.extensions/kube-flannel-ds-arm created
</span></span><span style=display:flex><span>daemonset.extensions/kube-flannel-ds-ppc64le created
</span></span><span style=display:flex><span>daemonset.extensions/kube-flannel-ds-s390x created
</span></span></code></pre></div><p>查看部署状态：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubectl get pod -n kube-system</span>
</span></span><span style=display:flex><span>NAME                              READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>coredns-d5947d4b-rn2wl            0/1     Pending   0          3m17s
</span></span><span style=display:flex><span>coredns-d5947d4b-zdptx            0/1     Pending   0          3m17s
</span></span><span style=display:flex><span>etcd-node211                      1/1     Running   0          2m48s
</span></span><span style=display:flex><span>kube-apiserver-node211            1/1     Running   0          2m28s
</span></span><span style=display:flex><span>kube-controller-manager-node211   1/1     Running   0          2m59s
</span></span><span style=display:flex><span>kube-flannel-ds-amd64-vzk7c       1/1     Running   0          36s
</span></span><span style=display:flex><span>kube-proxy-w5gsg                  1/1     Running   0          3m16s
</span></span><span style=display:flex><span>kube-scheduler-node211            1/1     Running   0          2m41s
</span></span></code></pre></div><h3 id=7-添加其他-master-节点>7. 添加其他 master 节点
<a class=heading-link href=#7-%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96-master-%e8%8a%82%e7%82%b9><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>按照 node211 初始化提示，在 node212 节点及 node213 节点添加到集群，角色为 master：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span>
</span></span><span style=display:flex><span>&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node212&#34;</span> could not be reached
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node212&#34;</span>: lookup node212 on 192.168.64.215:53: no such host
</span></span><span style=display:flex><span>	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span style=font-style:italic>&#39;systemctl enable kubelet.service&#39;</span>
</span></span><span style=display:flex><span>[preflight] Reading configuration from the cluster...
</span></span><span style=display:flex><span>[preflight] FYI: You can look at this config file with <span style=font-style:italic>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span style=display:flex><span>	[WARNING RequiredIPVSKernelModulesAvailable]: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_sh ip_vs_rr ip_vs_wrr]
</span></span><span style=display:flex><span>or no builtin kernel IPVS support was found: map[ip_vs:{} ip_vs_rr:{} ip_vs_sh:{} ip_vs_wrr:{} nf_conntrack_ipv4:{}].
</span></span><span style=display:flex><span>However, these modules may be loaded automatically by kube-proxy <span style=font-weight:700>if</span> they are available on your system.
</span></span><span style=display:flex><span>To verify IPVS support:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   Run <span style=font-style:italic>&#34;lsmod | grep &#39;ip_vs|nf_conntrack&#39;&#34;</span> and verify each of the above modules are listed.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If they are not listed, you can use the following methods to load them:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. For each missing module run <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> (e.g., <span style=font-style:italic>&#39;modprobe ip_vs&#39;</span>, <span style=font-style:italic>&#39;modprobe ip_vs_rr&#39;</span>, ...)
</span></span><span style=display:flex><span>2. If <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> returns an error, you will need to install the missing module support <span style=font-weight:700>for</span> your kernel.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks before initializing the new control plane instance
</span></span><span style=display:flex><span>[preflight] Pulling images required <span style=font-weight:700>for</span> setting up a Kubernetes cluster
</span></span><span style=display:flex><span>[preflight] This might take a minute or two, depending on the speed of your internet connection
</span></span><span style=display:flex><span>[preflight] You can also perform this action in beforehand using <span style=font-style:italic>&#39;kubeadm config images pull&#39;</span>
</span></span><span style=display:flex><span>[download-certs] Downloading the certificates in Secret <span style=font-style:italic>&#34;kubeadm-certs&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[certs] Using certificateDir folder <span style=font-style:italic>&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/server&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/server serving cert is signed <span style=font-weight:700>for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-etcd-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/peer&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/peer serving cert is signed <span style=font-weight:700>for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/healthcheck-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] apiserver serving cert is signed <span style=font-weight:700>for</span> DNS names [node212 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.212 192.168.77.219]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-kubelet-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;front-proxy-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Valid certificates and keys now exist in <span style=font-style:italic>&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style=display:flex><span>[certs] Using the existing <span style=font-style:italic>&#34;sa&#34;</span> key
</span></span><span style=display:flex><span>[kubeconfig] Generating kubeconfig files
</span></span><span style=display:flex><span>[kubeconfig] Using kubeconfig folder <span style=font-style:italic>&#34;/etc/kubernetes&#34;</span>
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;admin.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;controller-manager.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;scheduler.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[control-plane] Using manifest folder <span style=font-style:italic>&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-apiserver&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-controller-manager&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-scheduler&#34;</span>
</span></span><span style=display:flex><span>[check-etcd] Checking that the etcd cluster is healthy
</span></span><span style=display:flex><span>[kubelet-start] Downloading configuration <span style=font-weight:700>for</span> the kubelet from the <span style=font-style:italic>&#34;kubelet-config-1.14&#34;</span> ConfigMap in the kube-system namespace
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet configuration to file <span style=font-style:italic>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet environment file with flags to file <span style=font-style:italic>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Activating the kubelet service
</span></span><span style=display:flex><span>[kubelet-start] Waiting <span style=font-weight:700>for</span> the kubelet to perform the TLS Bootstrap...
</span></span><span style=display:flex><span>[etcd] Announced new etcd member joining to the existing etcd cluster
</span></span><span style=display:flex><span>[etcd] Wrote Static Pod manifest <span style=font-weight:700>for</span> a local etcd member to <span style=font-style:italic>&#34;/etc/kubernetes/manifests/etcd.yaml&#34;</span>
</span></span><span style=display:flex><span>[etcd] Waiting <span style=font-weight:700>for</span> the new etcd member to join the cluster. This can take up to 40s
</span></span><span style=display:flex><span>[upload-config] storing the configuration used in ConfigMap <span style=font-style:italic>&#34;kubeadm-config&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node212 as control-plane by adding the label <span style=font-style:italic>&#34;node-role.kubernetes.io/master=&#39;&#39;&#34;</span>
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node212 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>This node has joined the cluster and a new control plane instance was created:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and approval was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>* Control plane (master) label and taint were applied to the new node.
</span></span><span style=display:flex><span>* The Kubernetes control plane instances scaled up.
</span></span><span style=display:flex><span>* A new etcd member was added to the local/stacked etcd cluster.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start administering your cluster from this node, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>	sudo chown <span style=font-weight:700>$(</span>id -u<span style=font-weight:700>)</span>:<span style=font-weight:700>$(</span>id -g<span style=font-weight:700>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> to see this node join the cluster.
</span></span></code></pre></div><p>按照说明，拷贝 kubectl 配置文件并验证：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># mkdir -p $HOME/.kube</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node212 ~]<span style=font-style:italic># kubectl get node</span>
</span></span><span style=display:flex><span>NAME      STATUS   ROLES    AGE     VERSION
</span></span><span style=display:flex><span>node211   Ready    master   7m48s   v1.14.3
</span></span><span style=display:flex><span>node212   Ready    master   66s     v1.14.3
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span>
</span></span><span style=display:flex><span>&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 <span style=font-weight:700;font-style:italic>\
</span></span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic></span>&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node213&#34;</span> could not be reached
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node213&#34;</span>: lookup node213 on 192.168.64.215:53: no such host
</span></span><span style=display:flex><span>	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span style=font-style:italic>&#39;systemctl enable kubelet.service&#39;</span>
</span></span><span style=display:flex><span>[preflight] Reading configuration from the cluster...
</span></span><span style=display:flex><span>[preflight] FYI: You can look at this config file with <span style=font-style:italic>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span style=display:flex><span>	[WARNING RequiredIPVSKernelModulesAvailable]: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]
</span></span><span style=display:flex><span>or no builtin kernel IPVS support was found: map[ip_vs:{} ip_vs_rr:{} ip_vs_sh:{} ip_vs_wrr:{} nf_conntrack_ipv4:{}].
</span></span><span style=display:flex><span>However, these modules may be loaded automatically by kube-proxy <span style=font-weight:700>if</span> they are available on your system.
</span></span><span style=display:flex><span>To verify IPVS support:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   Run <span style=font-style:italic>&#34;lsmod | grep &#39;ip_vs|nf_conntrack&#39;&#34;</span> and verify each of the above modules are listed.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If they are not listed, you can use the following methods to load them:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. For each missing module run <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> (e.g., <span style=font-style:italic>&#39;modprobe ip_vs&#39;</span>, <span style=font-style:italic>&#39;modprobe ip_vs_rr&#39;</span>, ...)
</span></span><span style=display:flex><span>2. If <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> returns an error, you will need to install the missing module support <span style=font-weight:700>for</span> your kernel.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks before initializing the new control plane instance
</span></span><span style=display:flex><span>[preflight] Pulling images required <span style=font-weight:700>for</span> setting up a Kubernetes cluster
</span></span><span style=display:flex><span>[preflight] This might take a minute or two, depending on the speed of your internet connection
</span></span><span style=display:flex><span>[preflight] You can also perform this action in beforehand using <span style=font-style:italic>&#39;kubeadm config images pull&#39;</span>
</span></span><span style=display:flex><span>[download-certs] Downloading the certificates in Secret <span style=font-style:italic>&#34;kubeadm-certs&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[certs] Using certificateDir folder <span style=font-style:italic>&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;front-proxy-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/server&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/server serving cert is signed <span style=font-weight:700>for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/peer&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] etcd/peer serving cert is signed <span style=font-weight:700>for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;etcd/healthcheck-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-etcd-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] apiserver serving cert is signed <span style=font-weight:700>for</span> DNS names [node213 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.213 192.168.77.219]
</span></span><span style=display:flex><span>[certs] Generating <span style=font-style:italic>&#34;apiserver-kubelet-client&#34;</span> certificate and key
</span></span><span style=display:flex><span>[certs] Valid certificates and keys now exist in <span style=font-style:italic>&#34;/etc/kubernetes/pki&#34;</span>
</span></span><span style=display:flex><span>[certs] Using the existing <span style=font-style:italic>&#34;sa&#34;</span> key
</span></span><span style=display:flex><span>[kubeconfig] Generating kubeconfig files
</span></span><span style=display:flex><span>[kubeconfig] Using kubeconfig folder <span style=font-style:italic>&#34;/etc/kubernetes&#34;</span>
</span></span><span style=display:flex><span>[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;admin.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;controller-manager.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[kubeconfig] Writing <span style=font-style:italic>&#34;scheduler.conf&#34;</span> kubeconfig file
</span></span><span style=display:flex><span>[control-plane] Using manifest folder <span style=font-style:italic>&#34;/etc/kubernetes/manifests&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-apiserver&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-controller-manager&#34;</span>
</span></span><span style=display:flex><span>[control-plane] Creating static Pod manifest <span style=font-weight:700>for</span> <span style=font-style:italic>&#34;kube-scheduler&#34;</span>
</span></span><span style=display:flex><span>[check-etcd] Checking that the etcd cluster is healthy
</span></span><span style=display:flex><span>[kubelet-start] Downloading configuration <span style=font-weight:700>for</span> the kubelet from the <span style=font-style:italic>&#34;kubelet-config-1.14&#34;</span> ConfigMap in the kube-system namespace
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet configuration to file <span style=font-style:italic>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet environment file with flags to file <span style=font-style:italic>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Activating the kubelet service
</span></span><span style=display:flex><span>[kubelet-start] Waiting <span style=font-weight:700>for</span> the kubelet to perform the TLS Bootstrap...
</span></span><span style=display:flex><span>[etcd] Announced new etcd member joining to the existing etcd cluster
</span></span><span style=display:flex><span>[etcd] Wrote Static Pod manifest <span style=font-weight:700>for</span> a local etcd member to <span style=font-style:italic>&#34;/etc/kubernetes/manifests/etcd.yaml&#34;</span>
</span></span><span style=display:flex><span>[etcd] Waiting <span style=font-weight:700>for</span> the new etcd member to join the cluster. This can take up to 40s
</span></span><span style=display:flex><span>[upload-config] storing the configuration used in ConfigMap <span style=font-style:italic>&#34;kubeadm-config&#34;</span> in the <span style=font-style:italic>&#34;kube-system&#34;</span> Namespace
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node213 as control-plane by adding the label <span style=font-style:italic>&#34;node-role.kubernetes.io/master=&#39;&#39;&#34;</span>
</span></span><span style=display:flex><span>[mark-control-plane] Marking the node node213 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>This node has joined the cluster and a new control plane instance was created:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and approval was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>* Control plane (master) label and taint were applied to the new node.
</span></span><span style=display:flex><span>* The Kubernetes control plane instances scaled up.
</span></span><span style=display:flex><span>* A new etcd member was added to the local/stacked etcd cluster.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start administering your cluster from this node, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>	sudo chown <span style=font-weight:700>$(</span>id -u<span style=font-weight:700>)</span>:<span style=font-weight:700>$(</span>id -g<span style=font-weight:700>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> to see this node join the cluster.
</span></span></code></pre></div><p>按照说明，拷贝 kubectl 配置文件并验证：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># mkdir -p $HOME/.kube</span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span>
</span></span><span style=display:flex><span>[root@node213 ~]<span style=font-style:italic># kubectl get node</span>
</span></span><span style=display:flex><span>NAME      STATUS   ROLES    AGE     VERSION
</span></span><span style=display:flex><span>node211   Ready    master   11m     v1.14.3
</span></span><span style=display:flex><span>node212   Ready    master   4m39s   v1.14.3
</span></span><span style=display:flex><span>node213   Ready    master   72s     v1.14.3
</span></span></code></pre></div><h3 id=8-添加其他-node-节点>8. 添加其他 node 节点
<a class=heading-link href=#8-%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96-node-%e8%8a%82%e7%82%b9><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>按照 node211 初始化提示，添加 node214 节点到集群，角色为 node：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node214 ~]<span style=font-style:italic># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span>
</span></span><span style=display:flex><span>&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 
</span></span><span style=display:flex><span>[preflight] Running pre-flight checks
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node214&#34;</span> could not be reached
</span></span><span style=display:flex><span>	[WARNING Hostname]: hostname <span style=font-style:italic>&#34;node214&#34;</span>: lookup node214 on 192.168.64.215:53: no such host
</span></span><span style=display:flex><span>	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span style=font-style:italic>&#39;systemctl enable kubelet.service&#39;</span>
</span></span><span style=display:flex><span>[preflight] Reading configuration from the cluster...
</span></span><span style=display:flex><span>[preflight] FYI: You can look at this config file with <span style=font-style:italic>&#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span>
</span></span><span style=display:flex><span>	[WARNING RequiredIPVSKernelModulesAvailable]: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]
</span></span><span style=display:flex><span>or no builtin kernel IPVS support was found: map[ip_vs:{} ip_vs_rr:{} ip_vs_sh:{} ip_vs_wrr:{} nf_conntrack_ipv4:{}].
</span></span><span style=display:flex><span>However, these modules may be loaded automatically by kube-proxy <span style=font-weight:700>if</span> they are available on your system.
</span></span><span style=display:flex><span>To verify IPVS support:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   Run <span style=font-style:italic>&#34;lsmod | grep &#39;ip_vs|nf_conntrack&#39;&#34;</span> and verify each of the above modules are listed.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If they are not listed, you can use the following methods to load them:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1. For each missing module run <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> (e.g., <span style=font-style:italic>&#39;modprobe ip_vs&#39;</span>, <span style=font-style:italic>&#39;modprobe ip_vs_rr&#39;</span>, ...)
</span></span><span style=display:flex><span>2. If <span style=font-style:italic>&#39;modprobe $modulename&#39;</span> returns an error, you will need to install the missing module support <span style=font-weight:700>for</span> your kernel.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[kubelet-start] Downloading configuration <span style=font-weight:700>for</span> the kubelet from the <span style=font-style:italic>&#34;kubelet-config-1.14&#34;</span> ConfigMap in the kube-system namespace
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet configuration to file <span style=font-style:italic>&#34;/var/lib/kubelet/config.yaml&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Writing kubelet environment file with flags to file <span style=font-style:italic>&#34;/var/lib/kubelet/kubeadm-flags.env&#34;</span>
</span></span><span style=display:flex><span>[kubelet-start] Activating the kubelet service
</span></span><span style=display:flex><span>[kubelet-start] Waiting <span style=font-weight:700>for</span> the kubelet to perform the TLS Bootstrap...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>This node has joined the cluster:
</span></span><span style=display:flex><span>* Certificate signing request was sent to apiserver and a response was received.
</span></span><span style=display:flex><span>* The Kubelet was informed of the new secure connection details.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Run <span style=font-style:italic>&#39;kubectl get nodes&#39;</span> on the control-plane to see this node join the cluster.
</span></span></code></pre></div><p>至此 kubeadm 配合 keepalived & haproxy 搭建高可用集群就完成了。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubectl get node</span>
</span></span><span style=display:flex><span>NAME      STATUS   ROLES    AGE     VERSION
</span></span><span style=display:flex><span>node211   Ready    master   4h10m   v1.14.3
</span></span><span style=display:flex><span>node212   Ready    master   4h3m    v1.14.3
</span></span><span style=display:flex><span>node213   Ready    master   4h      v1.14.3
</span></span><span style=display:flex><span>node214   Ready    &lt;none&gt;   3h57m   v1.14.3
</span></span><span style=display:flex><span>[root@node211 ~]<span style=font-style:italic># kubectl get pod -n kube-system</span>
</span></span><span style=display:flex><span>NAME                              READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>coredns-d5947d4b-rn2wl            1/1     Running   0          4h10m
</span></span><span style=display:flex><span>coredns-d5947d4b-zdptx            1/1     Running   0          4h10m
</span></span><span style=display:flex><span>etcd-node211                      1/1     Running   0          4h9m
</span></span><span style=display:flex><span>etcd-node212                      1/1     Running   0          4h3m
</span></span><span style=display:flex><span>etcd-node213                      1/1     Running   0          4h
</span></span><span style=display:flex><span>kube-apiserver-node211            1/1     Running   0          4h9m
</span></span><span style=display:flex><span>kube-apiserver-node212            1/1     Running   1          4h3m
</span></span><span style=display:flex><span>kube-apiserver-node213            1/1     Running   0          3h59m
</span></span><span style=display:flex><span>kube-controller-manager-node211   1/1     Running   1          4h9m
</span></span><span style=display:flex><span>kube-controller-manager-node212   1/1     Running   0          4h2m
</span></span><span style=display:flex><span>kube-controller-manager-node213   1/1     Running   0          3h59m
</span></span><span style=display:flex><span>kube-flannel-ds-amd64-gchpj       1/1     Running   0          4h
</span></span><span style=display:flex><span>kube-flannel-ds-amd64-mx44p       1/1     Running   0          3h57m
</span></span><span style=display:flex><span>kube-flannel-ds-amd64-vzk7c       1/1     Running   0          4h7m
</span></span><span style=display:flex><span>kube-flannel-ds-amd64-x9rm7       1/1     Running   0          4h3m
</span></span><span style=display:flex><span>kube-proxy-fj448                  1/1     Running   0          4h
</span></span><span style=display:flex><span>kube-proxy-jmhm7                  1/1     Running   0          4h3m
</span></span><span style=display:flex><span>kube-proxy-s7jdf                  1/1     Running   0          3h57m
</span></span><span style=display:flex><span>kube-proxy-w5gsg                  1/1     Running   0          4h10m
</span></span><span style=display:flex><span>kube-scheduler-node211            1/1     Running   1          4h9m
</span></span><span style=display:flex><span>kube-scheduler-node212            1/1     Running   0          4h2m
</span></span><span style=display:flex><span>kube-scheduler-node213            1/1     Running   0          3h59m
</span></span></code></pre></div><h3 id=ha-机制>HA 机制
<a class=heading-link href=#ha-%e6%9c%ba%e5%88%b6><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>由集群节点上运行的 keepalived & haproxy 提供 VIP & LB，集群中所有节点的 kubelet 连接至 VIP: EndPoints。</p><p>当 VIP 所在节点发生故障，VIP 切换到集群中其他 master 节点，即可正常提供服务。</p><h3 id=坑>坑
<a class=heading-link href=#%e5%9d%91><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><ol><li>kubeadm 需要正常网络支持，需要确保自己处于正常网络环境下；</li><li>kubeadm 在添加节点时，有可能会 hang 住，未查明原因；</li><li>kubeadm 默认生成证书有效期为 1年，若想要修改，则需要手动生成证书替换；</li><li>&mldr;</li></ol><h2 id=kubespray>kubespray
<a class=heading-link href=#kubespray><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>因为 kubespray 项目主要使用 ansible 配合 kubeadm 部署，具体内容可以直接查看 github 文档，因此不详细记录具体步骤。</p><h3 id=环境信息-1>环境信息
<a class=heading-link href=#%e7%8e%af%e5%a2%83%e4%bf%a1%e6%81%af-1><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><table><thead><tr><th>ip</th><th>role</th></tr></thead><tbody><tr><td>192.168.77.201</td><td>master</td></tr><tr><td>192.168.77.202</td><td>master</td></tr><tr><td>192.168.77.203</td><td>master</td></tr><tr><td>192.168.77.204</td><td>node</td></tr></tbody></table><h3 id=1-安装-kubespray>1. 安装 kubespray
<a class=heading-link href=#1-%e5%ae%89%e8%a3%85-kubespray><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 GitHub <a href=https://github.com/kubernetes-sigs/kubespray/releases>项目链接</a>上下载最新 Release 版本代码。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node201 ~]<span style=font-style:italic># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.10.3.tar.gz</span>
</span></span></code></pre></div><h3 id=2-安装必要依赖>2. 安装必要依赖
<a class=heading-link href=#2-%e5%ae%89%e8%a3%85%e5%bf%85%e8%a6%81%e4%be%9d%e8%b5%96><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>项目依赖于 Python3，所以这里采用 Python3.6 版本进行安装。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># yum install python36</span>
</span></span><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># yum install python36-pip</span>
</span></span><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># pip3 install -r requirements.txt </span>
</span></span></code></pre></div><ol start=3><li>生成 ansible inventory</li></ol><p>项目默认提供了一个 Python 脚本用于自动生成 inventory，该脚本生成 inventory 通常需要根据实际情况自己调整。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># cp -rfp inventory/sample inventory/mycluster</span>
</span></span><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># declare -a IPS=(192.168.77.201 192.168.77.202 192.168.77.203 192.168.77.203)</span>
</span></span><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]}</span>
</span></span><span style=display:flex><span>DEBUG: Adding group all
</span></span><span style=display:flex><span>DEBUG: Adding group kube-master
</span></span><span style=display:flex><span>DEBUG: Adding group kube-node
</span></span><span style=display:flex><span>DEBUG: Adding group etcd
</span></span><span style=display:flex><span>DEBUG: Adding group k8s-cluster
</span></span><span style=display:flex><span>DEBUG: Adding group calico-rr
</span></span><span style=display:flex><span>DEBUG: Skipping existing host 192.168.77.203.
</span></span><span style=display:flex><span>DEBUG: adding host node1 to group all
</span></span><span style=display:flex><span>DEBUG: adding host node2 to group all
</span></span><span style=display:flex><span>DEBUG: adding host node3 to group all
</span></span><span style=display:flex><span>DEBUG: adding host node1 to group etcd
</span></span><span style=display:flex><span>DEBUG: adding host node2 to group etcd
</span></span><span style=display:flex><span>DEBUG: adding host node3 to group etcd
</span></span><span style=display:flex><span>DEBUG: adding host node1 to group kube-master
</span></span><span style=display:flex><span>DEBUG: adding host node2 to group kube-master
</span></span><span style=display:flex><span>DEBUG: adding host node1 to group kube-node
</span></span><span style=display:flex><span>DEBUG: adding host node2 to group kube-node
</span></span><span style=display:flex><span>DEBUG: adding host node3 to group kube-node
</span></span></code></pre></div><p>查看生成 inventory 结果：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>[root@node201 kubespray-2.10.3]<span style=font-style:italic># cat inventory/mycluster/hosts.yml </span>
</span></span><span style=display:flex><span>all:
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>    node1:
</span></span><span style=display:flex><span>      ansible_host: 192.168.77.201
</span></span><span style=display:flex><span>      ip: 192.168.77.201
</span></span><span style=display:flex><span>      access_ip: 192.168.77.201
</span></span><span style=display:flex><span>    node2:
</span></span><span style=display:flex><span>      ansible_host: 192.168.77.202
</span></span><span style=display:flex><span>      ip: 192.168.77.202
</span></span><span style=display:flex><span>      access_ip: 192.168.77.202
</span></span><span style=display:flex><span>    node3:
</span></span><span style=display:flex><span>      ansible_host: 192.168.77.203
</span></span><span style=display:flex><span>      ip: 192.168.77.203
</span></span><span style=display:flex><span>      access_ip: 192.168.77.203
</span></span><span style=display:flex><span>  children:
</span></span><span style=display:flex><span>    kube-master:
</span></span><span style=display:flex><span>      hosts:
</span></span><span style=display:flex><span>        node1:
</span></span><span style=display:flex><span>        node2:
</span></span><span style=display:flex><span>    kube-node:
</span></span><span style=display:flex><span>      hosts:
</span></span><span style=display:flex><span>        node1:
</span></span><span style=display:flex><span>        node2:
</span></span><span style=display:flex><span>        node3:
</span></span><span style=display:flex><span>    etcd:
</span></span><span style=display:flex><span>      hosts:
</span></span><span style=display:flex><span>        node1:
</span></span><span style=display:flex><span>        node2:
</span></span><span style=display:flex><span>        node3:
</span></span><span style=display:flex><span>    k8s-cluster:
</span></span><span style=display:flex><span>      children:
</span></span><span style=display:flex><span>        kube-master:
</span></span><span style=display:flex><span>        kube-node:
</span></span><span style=display:flex><span>    calico-rr:
</span></span><span style=display:flex><span>      hosts: {}
</span></span></code></pre></div><p>可以看到跟我们计划中的有所差别，根据实际情况调整 kube-master 数量即可。</p><h3 id=4-编写部署配置参数>4. 编写部署配置参数
<a class=heading-link href=#4-%e7%bc%96%e5%86%99%e9%83%a8%e7%bd%b2%e9%85%8d%e7%bd%ae%e5%8f%82%e6%95%b0><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/all/all.yml</code> 路径下包含了一些全局配置，比如 proxy 之类的，可以手动调整。</p><h3 id=5-编写-k8s-配置参数>5. 编写 k8s 配置参数
<a class=heading-link href=#5-%e7%bc%96%e5%86%99-k8s-%e9%85%8d%e7%bd%ae%e5%8f%82%e6%95%b0><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml</code> 路径下包含了 k8s 所有配置项，根据实际情况编辑修改。</p><h3 id=6-部署>6. 部署
<a class=heading-link href=#6-%e9%83%a8%e7%bd%b2><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>在所有准备工作完成后，执行部署操作。</p><p>注意， Kubespray 部署的前提条件是你的网络是一个正常的网络，可以正常访问所有网站，若无法访问，则根据自身实际情况，调整配置，配置路径为： <code>roles/download/defaults/main.yml</code> 。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml
</span></span></code></pre></div><p>等待部署完成即可。</p><h3 id=ha-机制-1>HA 机制
<a class=heading-link href=#ha-%e6%9c%ba%e5%88%b6-1><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><p>集群中所有的 Node 节点自己启动一个 Nginx Static Pod，用于代理转发，将所有指定 <code>127.0.0.1:6443</code> 的请求转发至所有 master 节点真实 apiserver ，这样所有的 kubelet 只需要自己节点即可，无需其他节点参与。</p><h3 id=坑-1>坑
<a class=heading-link href=#%e5%9d%91-1><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h3><ol><li>CentOS 默认 Python2.7，需要单独安装 Python3.6</li><li>通过 pip 安装依赖，部分软件包需要 gcc,python36-devel,openssl-devel 等依赖包，需要根据错误提示自行安装，文档中没有提到</li><li>默认会安装 docker & containerd 服务，但是 containerd 服务未设置开机自启动，会导致 docker 无法自动运行</li><li>在安装过程中，会安装 selinux 相应 Python 库，但是该依赖未在 <code>requirements.txt</code> 声明</li><li>&mldr;</li></ol><h2 id=总结>总结
<a class=heading-link href=#%e6%80%bb%e7%bb%93><i class="fa fa-link" aria-hidden=true title=链接到标题></i>
<span class=sr-only>链接到标题</span></a></h2><p>无论是直接只用 kubeadm + vip 方式部署 HA 集群，还是通过 Kubespray 部署，在网络正常情况下，是很快可以完成的。</p><p>在使用 kubeadm 过程中，因为无需引入第三方依赖库，导致整体流程顺畅，体验极佳。</p><p>在 Kubespray 过程中，因为采用 Python3 方式，但相关依赖又未显示声明，导致部署过程繁琐。但是也比较好理解，Kubespray 作为一个致力于部署企业级 k8s 集群的项目，需要处理大量的边界条件了，这个项目中 YAML 就写了 15k 行，可见一斑。</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//zdyxry.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article></section></div><footer class=footer><section class=container>©
2016 -
2023
Yiran Zhou
·
技术支持 <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G0433XDZ3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G0433XDZ3V",{anonymize_ip:!1})}</script></body></html>
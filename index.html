<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="cR4Tgq6nOHr_Wo0dm8HUK3feA45_XLr5RkA2UC-tXxc">














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="Yiran's Blog" type="application/atom+xml">






<meta name="description" content="Normal is boring">
<meta name="keywords" content="Linux,KVM,Ops">
<meta property="og:type" content="website">
<meta property="og:title" content="Yiran&#39;s Blog">
<meta property="og:url" content="https://zdyxry.github.io/index.html">
<meta property="og:site_name" content="Yiran&#39;s Blog">
<meta property="og:description" content="Normal is boring">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yiran&#39;s Blog">
<meta name="twitter:description" content="Normal is boring">
<meta name="twitter:creator" content="@zhouyiran1994">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zdyxry.github.io/">





  <title>Yiran's Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-136220198-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yiran's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/07/21/CentOS定制-软件源错误/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/21/CentOS定制-软件源错误/" itemprop="url">CentOS定制-软件源错误</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-21T19:58:43+08:00">
                2019-07-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/21/CentOS定制-软件源错误/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/21/CentOS定制-软件源错误/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无法安装 OS 的问题，记录一下解决方式。</p>
<h2 id="定制-OS"><a href="#定制-OS" class="headerlink" title="定制 OS"></a>定制 OS</h2><p>关于定制 OS，在之前的博客中已经提到过几次了，CentOS 是比较容易改动的一个发行版，因为有着 RHEL 红（爸）帽（爸），有着完善的文档可以参考。</p>
<p>主要需要注意的是两点：  </p>
<ol>
<li>分区方式</li>
<li>软件包选择</li>
</ol>
<p>今天遇到的问题是第二点。</p>
<p>先说下前提，由于是 2B 产品，所以对于每次的 BaseOS 版本升级都非常谨慎，每次 BaseOS 版本都会进行各种测试。但是如果仅仅是升级部分所需要的软件包，就不用这么麻烦了，我们可以定制自己所需要的软件组（group），来进行安装/升级。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这次接到的需要是升级 Kernel、libiscsi、qemu 三个软件，后两个是虚拟化相关的，相关依赖较少；kernel 是跟 BaseOS 版本关联性很大的。</p>
<p>比如 CentOS 7.6 中，kernel 版本为：kernel-3.10.0-957.el7.x86_64.rpm，这个版本对 selinux 等相关软件是有依赖要求的，我在这里翻车了。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>像往常一样，将对应的 rpm 放置到了对应的 yum 源中，更新 yum 源，制作 ISO，在安装过程中报错：</p>
<img src="/2019/07/21/CentOS定制-软件源错误/os1.png" title="OS1">
<p>报错显示是软件源出了问题，但是没有更多的信息了，这时候我们可以通过 console 连接到其他的 pty 中，查看对应的日志，比如 CentOS 默认的日志在： <code>/tmp/packaging.log</code> 中：</p>
<img src="/2019/07/21/CentOS定制-软件源错误/os2.png" title="OS2">
<p>我们可以看到日志中提示 kernel 与当前软件源中的 selinux-policy-targeted 冲突，因为安装 OS 所用的软件源就是 ISO ，所以这里肯定是我们打包 ISO 时遗漏了依赖关系导致的，我们将对应的 Kernel 所需依赖更新，重新构建 ISO 就可以了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/" itemprop="url">Kubernetes 实战-踩坑记录（持续更新）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-13T09:34:45+08:00">
                2019-07-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/13/Kubernetes-实战-踩坑记录（持续更新）/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更新。</p>
<h3 id="更新记录"><a href="#更新记录" class="headerlink" title="更新记录"></a>更新记录</h3><ul>
<li>2019.07.13 02:00 来自加班中的 yiran</li>
<li>2019.07.19 06:52 早起不想去公司的 yiran</li>
</ul>
<h2 id="coredns-无法解析域名"><a href="#coredns-无法解析域名" class="headerlink" title="coredns 无法解析域名"></a>coredns 无法解析域名</h2><p>在 Kubernetes 环境中，使用 kubeadm 工具部署的集群，会自动部署 coredns 作为集群的域名服务，每当我们创建了自己的 service，都可以通过域名直接访问，不用再考虑自己多个 Pod 的 IP 不同如何连接的问题。</p>
<p>最近遇到多个环境出现无法解析域名的问题，具体现象如下：</p>
<ol>
<li>集群部署完成后，部署 daemonset 资源，每个节点均运行一个 busybox；</li>
<li>在 busybox 中对 <code>kubernetes</code> 默认域名进行解析，查看解析结果。</li>
</ol>
<p>正常情况应该是所有的 busybox 都可以正常解析才对，但是最近几个环境中均出现了 3 个node 中1个node 上的 pod 无法解析的问题，示例代码如下：</p>
<p>daemonset.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:1.28.4"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 21:28:40 ~]<span class="variable">$for</span> i <span class="keyword">in</span> `kubectl get pod  -o wide  |grep ds | awk <span class="string">'&#123;print $1&#125;'</span>`;<span class="keyword">do</span> kubectl <span class="built_in">exec</span> <span class="variable">$i</span> nslookup kubernetes;<span class="built_in">echo</span> ;<span class="keyword">done</span></span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10</span><br><span class="line"></span><br><span class="line">nslookup: can<span class="string">'t resolve '</span>kubernetes<span class="string">'</span></span><br><span class="line"><span class="string">command terminated with exit code 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Server:    10.96.0.10</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Name:      kubernetes</span></span><br><span class="line"><span class="string">Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local</span></span><br></pre></td></tr></table></figure>
<p>在第一个节点的 Pod 解析时失效，最后命令执行 1min 超时退出。</p>
<p>经过查看发现节点的 NetFilter 相关系统配置未生效，导致 iptables 相关功能失效，具体可以参考 <a href="https://github.com/kubernetes/kubernetes/issues/21613" target="_blank" rel="noopener">issue</a>。</p>
<p>解决方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br></pre></td></tr></table></figure>
<h2 id="Flannel-OOM"><a href="#Flannel-OOM" class="headerlink" title="Flannel OOM"></a>Flannel OOM</h2><p>在配置好集群业务后，发现业务时不时的出现中断情况，最开始排查业务自身问题，未发现 Pod 出现重启或异常的日志，开始排查 k8s 状态，发现在节点 <code>/var/log/messages</code> 日志中，Flannel 一直处于 OOM 状态，惨不忍睹。</p>
<p>之前还略微惊奇，Flannel 默认的计算资源中，内存只要 50MiB，且上限也是 50MiB，没有给自己留一丝余地，看到 <a href="https://github.com/coreos/flannel/issues/963" target="_blank" rel="noopener">issue</a> 中的描述，感觉这个不是一个偶发事件，最终我将 Flannel 的内存调整为 250MiB 后，未出现 OOM 情况。</p>
<p>issue 中提到的 <code>kubectl patch</code> 命令未自动生效，我通过更新 ds 配置，然后依次手动删除节点上的 Flannel Pod 使其生效。</p>
<h2 id="Nginx-Ingress"><a href="#Nginx-Ingress" class="headerlink" title="Nginx Ingress"></a>Nginx Ingress</h2><p>Nginx Ingress 有多个版本，在编写 Ingress 规则的时候一定要看清自己集群中的 Nginx Ingress 版本，我最开始就是因为这个看错了文档。。</p>
<p>主要的版本有： <code>kubernetes/ingress-nginx</code> , <code>nginxinc/kubernetes-ingress with NGINX</code> 和 <code>nginxinc/kubernetes-ingress with NGINX PLUS</code> ，具体的对比规则可以在 <a href="https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md" target="_blank" rel="noopener">Github</a> 中了解。</p>
<p>在 <code>kubernetes/ingress-nginx</code> 中，默认 <code>ssl-redirect</code> 参数是 <code>true</code> ，如果自己的服务不支持 https，那么需要显示的声明该参数为 false 才可以，这里需要注意一下。</p>
<p>在配置 nginx 参数的时候，要注意语法，正确的书写方式如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      &#123;"apiVersion":"networking.k8s.io/v1beta1","kind":"Ingress","metadata":&#123;"annotations":&#123;"kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/proxy-read-timeout":"3600","nginx.ingress.kubernetes.io/proxy-send-timeout":"3600","nginx.ingress.kubernetes.io/ssl-redirect":"true","nginx.ingress.kubernetes.io/use-regex":"true","nginx.org/websocket-services":"websockify"&#125;,"name":"websockify","namespace":"default"&#125;,"spec":&#123;"rules":[&#123;"http":&#123;"paths":[&#123;"backend":&#123;"serviceName":"websockify","servicePort":8000&#125;,"path":"/websockify"&#125;]&#125;&#125;]&#125;&#125;</span></span><br><span class="line"><span class="string">    kubernetes.io/ingress.class: nginx</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"</span></span><br><span class="line"><span class="string">    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"</span></span><br></pre></td></tr></table></figure>
<p>相关 issue 链接： <a href="https://github.com/kubernetes/ingress-nginx/issues/2007" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/issues/2007</a> </p>
<h2 id="Docker-稳定性"><a href="#Docker-稳定性" class="headerlink" title="Docker 稳定性"></a>Docker 稳定性</h2><p>在修改 Docker 配置后，需要重启 Docker.service 使配置生效，在一次重启操作中，直接导致物理节点宕机，自动重启了。。。</p>
<p>重启后观察物理节点日志，未发现异常日志，目前待复现调查，很坑很诡异。</p>
<h2 id="Helm-values-为空更新错误"><a href="#Helm-values-为空更新错误" class="headerlink" title="Helm values 为空更新错误"></a>Helm values 为空更新错误</h2><p>今天在给应用编写 Helm Charts 的时候，在 Values 中通过 resources.requests.cpu 方式指定了 cpu 和内存，在测试的时候忘记填写具体数值了，像这面这样：</p>
<p>values:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span></span><br><span class="line"><span class="attr">   memory:</span></span><br></pre></td></tr></table></figure></p>
<p>在 helm templates 中定义 daemonset，指定使用 resources 字段。</p>
<p>直接执行 helm 命令安装成功了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:44:09 <span class="built_in">test</span>]<span class="variable">$helm</span> install . --name-template <span class="built_in">test</span></span><br><span class="line">NAME: <span class="built_in">test</span></span><br><span class="line">LAST DEPLOYED: 2019-07-15 20:44:18.151073881 +0800 CST m=+0.092592446</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods -l <span class="string">"app=test,release=test"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br></pre></td></tr></table></figure>
<p>我们查看创建出来的 daemonset 资源状态：</p>
<p>daemonset/test</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="attr">Name:</span>           <span class="string">test</span></span><br><span class="line"><span class="attr">Selector:</span>       <span class="string">app=test</span></span><br><span class="line"><span class="attr">Node-Selector:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="string">Pods</span> <span class="attr">Status:</span>  <span class="number">3</span> <span class="string">Running</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Waiting</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Succeeded</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Failed</span></span><br><span class="line"><span class="string">Pod</span> <span class="attr">Template:</span></span><br><span class="line"><span class="attr">  Labels:</span>  <span class="string">app=test</span></span><br><span class="line"><span class="attr">  Containers:</span></span><br><span class="line"><span class="attr">   test:</span></span><br><span class="line"><span class="attr">    Image:</span>      <span class="string">harbor.zdyxry.com/test/test:0.1.2</span></span><br><span class="line"><span class="attr">    Port:</span>       <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line">    <span class="string">Host</span> <span class="attr">Port:</span>  <span class="number">10402</span><span class="string">/TCP</span></span><br><span class="line"><span class="attr">    Command:</span></span><br><span class="line">      <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">      -</span><span class="string">c</span></span><br><span class="line"><span class="attr">    Args:</span></span><br><span class="line">      <span class="string">gunicorn</span> <span class="bullet">-b</span> <span class="string">:10402</span> <span class="bullet">-k</span> <span class="string">gevent</span> <span class="string">test.main:flask_app</span> <span class="bullet">-w</span> <span class="number">2</span> <span class="bullet">--timeout</span> <span class="number">40</span> <span class="bullet">--pid</span> <span class="string">/var/run/test.pid</span></span><br><span class="line"><span class="attr">    Limits:</span></span><br><span class="line"><span class="attr">      cpu:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>  <span class="number">0</span></span><br><span class="line"><span class="attr">    Requests:</span></span><br><span class="line"><span class="attr">      cpu:</span>        <span class="number">0</span></span><br><span class="line"><span class="attr">      memory:</span>     <span class="number">0</span></span><br><span class="line"><span class="attr">    Environment:</span>  <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure>
<p>这时候我在检查资源的时候发现自己忘记设置资源了，我计划通过更新 values 数值来更新 daemonset：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Default values for test.</span></span><br><span class="line"><span class="comment"># This is a YAML-formatted file.</span></span><br><span class="line"><span class="comment"># Declare variables to be passed into your templates.</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">   cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">   memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br></pre></td></tr></table></figure>
<p>执行 helm upgrade 时候报错：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node11 20:49:36 <span class="built_in">test</span>]<span class="variable">$helm</span> upgrade <span class="built_in">test</span> .</span><br><span class="line">Error: UPGRADE FAILED: error validating <span class="string">""</span>: error validating data: [unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.limits.memory, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.cpu, unknown object <span class="built_in">type</span> <span class="string">"nil"</span> <span class="keyword">in</span> DaemonSet.spec.template.spec.containers[0].resources.requests.memory]</span><br></pre></td></tr></table></figure>
<p>根据报错信息可以看到这个字段之前是 <code>nil</code> ，现在我们要更新为有效类型更新失败，只能通过 <code>helm uninstall</code> 卸载后再次安装修复该问题。</p>
<p>这个问题只在 daemonset 类型下会出现。</p>
<h2 id="Flannel-网卡丢失"><a href="#Flannel-网卡丢失" class="headerlink" title="Flannel 网卡丢失"></a>Flannel 网卡丢失</h2><p>在通常情况下，我们的 k8s 节点都只有单一的网络环境，也就是有一块网卡，在部署 Flannel 插件的时候，默认会找默认路由所在的网卡，并将其绑定在上面。</p>
<p>由于内部测试环境较为特殊，我将其绑定在一个 ovs port 上，这个具体配置在 flannel yaml 中：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">kube-flannel</span></span><br><span class="line"><span class="attr">  image:</span> <span class="string">quay.io/coreos/flannel:v0.11.0-amd64</span></span><br><span class="line"><span class="attr">  command:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">/opt/bin/flanneld</span></span><br><span class="line"><span class="attr">  args:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--ip-masq</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--kube-subnet-mgr</span></span><br><span class="line"><span class="bullet">  -</span> <span class="bullet">--iface=port-storage</span>  <span class="comment"># 在这里我强制指定了 iface</span></span><br></pre></td></tr></table></figure>
<p>正常运行时时没有问题的，但是对 ovs port 进行了 <code>ifdown</code> 操作后，在 OS 层面就无法找到这个 ovs port 了，flannel 默认的 <code>flannel.1</code> 这个 link 也丢失了，当我尝试 <code>ifup</code> ovs port，这个 port 正常恢复工作了，但是 <code>flannel.1</code> 无法自动恢复，目前找到的办法是手动重建 flannel pod。</p>
<p>猜测这个动作在 flannel 的init 相关步骤执行的，在之后 container 正常运行时没有考虑 <code>flannel.1</code> 不存在的情况。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用经验通常是踩了一个又一个坑过来的~ </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/07/05/关于Ansible的一点经验/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/05/关于Ansible的一点经验/" itemprop="url">关于Ansible的一点经验</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-05T20:55:19+08:00">
                2019-07-05
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/05/关于Ansible的一点经验/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/07/05/关于Ansible的一点经验/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>先介绍下 Kubespray，Kubespray 是 K8S SIG 下的项目，目标是帮助用户创建 <code>生产环境级别</code> 的 k8s 集群。</p>
<p>是通过 Ansible Playbook 实现的，是的，这又是一个 Ansible 项目，其中 YAML 文件就有 15k 行，名副其实的大项目。</p>
<p>花费了几天时间陆陆续续看完了整个项目，大概了解了其中的工作流程，具体内容不提，感觉 Ansible 90% 的使用例子都可以在这个项目中找到，是一个值得阅读的项目。</p>
<p>之前写过一篇当时理解的最佳实践，今天趁此机会再总结下最近使用 Ansible 的一些经验。</p>
<h2 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h2><p>使用 tag 对 ansible task 进行划分，比如在重启某些服务的时候，我们只希望在初次安装的时候重启，在后续升级的时候不进行重启，那么我们就可以对这个重启服务的 task 进行tag 区分。</p>
<p>tag 使用示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:35:56 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">1 directory, 4 files</span><br><span class="line">[root@node111 16:35:58 ansible]<span class="variable">$cat</span> test.yaml </span><br><span class="line">- hosts: cluster</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: root</span><br><span class="line">  become_method: sudo</span><br><span class="line">  tasks:</span><br><span class="line">  - yum:</span><br><span class="line">      name: <span class="string">"&#123;&#123; item &#125;&#125;"</span></span><br><span class="line">      state: present</span><br><span class="line">    loop:</span><br><span class="line">    - httpd</span><br><span class="line">    - memcached</span><br><span class="line">    tags:</span><br><span class="line">    - packages</span><br><span class="line">  </span><br><span class="line">  - template:</span><br><span class="line">      src: templates/src.j2</span><br><span class="line">      dest: /etc/foo.conf</span><br><span class="line">    tags:</span><br><span class="line">    - configuration</span><br><span class="line">[root@node111 16:36:01 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml --tags configuration -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [template] *****************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;<span class="string">"changed"</span>: <span class="literal">false</span>, <span class="string">"checksum"</span>: <span class="string">"7b4cbb07f7e174316e4d892321682317e43a206c"</span>, <span class="string">"dest"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"gid"</span>: 0, <span class="string">"group"</span>: <span class="string">"root"</span>, <span class="string">"mode"</span>: <span class="string">"0644"</span>, <span class="string">"owner"</span>: <span class="string">"root"</span>, <span class="string">"path"</span>: <span class="string">"/etc/foo.conf"</span>, <span class="string">"size"</span>: 14, <span class="string">"state"</span>: <span class="string">"file"</span>, <span class="string">"uid"</span>: 0&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=1    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=1    changed=0    unreachable=0    failed=0</span><br></pre></td></tr></table></figure>
<h2 id="roles-meta-管理依赖"><a href="#roles-meta-管理依赖" class="headerlink" title="roles/meta 管理依赖"></a>roles/meta 管理依赖</h2><p>在 playbook 中存在多个 roles，且其中有相互依赖关系时，合理使用 meta 配置，填写其所依赖的 roles。注意，被依赖的 roles 会优先执行，示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">[root@node111 16:49:12 ansible]<span class="variable">$tree</span> . </span><br><span class="line">.</span><br><span class="line">├── ansible.cfg</span><br><span class="line">├── inventory</span><br><span class="line">├── roles</span><br><span class="line">│   ├── a</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   │   └── main.yaml</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── b</span><br><span class="line">│       ├── defaults</span><br><span class="line">│       ├── files</span><br><span class="line">│       ├── handlers</span><br><span class="line">│       ├── meta</span><br><span class="line">│       ├── tasks</span><br><span class="line">│       │   └── main.yaml</span><br><span class="line">│       ├── templates</span><br><span class="line">│       └── vars</span><br><span class="line">├── templates</span><br><span class="line">│   └── src.j2</span><br><span class="line">└── test.yaml</span><br><span class="line"></span><br><span class="line">18 directories, 7 files</span><br><span class="line">[root@node111 16:49:18 ansible]<span class="variable">$cat</span> roles/a/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: a</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"a"</span></span><br><span class="line">[root@node111 16:49:25 ansible]<span class="variable">$cat</span> roles/a/meta/main.yaml </span><br><span class="line">---</span><br><span class="line">dependencies:</span><br><span class="line">  - &#123; role: b &#125;</span><br><span class="line">[root@node111 16:49:33 ansible]<span class="variable">$cat</span> roles/b/tasks/main.yaml </span><br><span class="line">---</span><br><span class="line">- name: b</span><br><span class="line">  debug:</span><br><span class="line">    msg: <span class="string">"b"</span></span><br><span class="line">[root@node111 16:49:40 ansible]<span class="variable">$ansible</span>-playbook -i inventory  test.yaml -v</span><br><span class="line">Using /root/ansible/ansible.cfg as config file</span><br><span class="line"></span><br><span class="line">PLAY [cluster] ******************************************************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [b : b] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"b"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TASK [a : a] ********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [172.16.30.111] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.112] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line">ok: [172.16.30.113] =&gt; &#123;</span><br><span class="line">    <span class="string">"msg"</span>: <span class="string">"a"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP **********************************************************************************************************************************************************************************************************************************************************</span><br><span class="line">172.16.30.111              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.112              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line">172.16.30.113              : ok=2    changed=0    unreachable=0    failed=0   </span><br><span class="line"></span><br><span class="line">You have new mail <span class="keyword">in</span> /var/spool/mail/root</span><br><span class="line">[root@node111 16:50:05 ansible]$</span><br></pre></td></tr></table></figure>
<h2 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h2><p>在一个大型项目中，我们无论是服务的数量还是各个服务对应的参数数量都是极其惊人的，那么我们就要合理的管理相应参数，这里 kubespray 项目给出了一个很好的示例，先来看下 kubespray 的组织结构（简化版）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">inventory/</span><br><span class="line">├── <span class="built_in">local</span></span><br><span class="line">│   └── group_vars -&gt; ../sample/group_vars</span><br><span class="line">└── sample</span><br><span class="line">    └── group_vars</span><br><span class="line">        ├── all</span><br><span class="line">        └── k8s-cluster</span><br><span class="line">roles</span><br><span class="line">├── container-engine</span><br><span class="line">│   ├── containerd</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   └── templates</span><br><span class="line">│   ├── docker</span><br><span class="line">│   │   ├── defaults</span><br><span class="line">│   │   ├── files</span><br><span class="line">│   │   ├── handlers</span><br><span class="line">│   │   ├── meta</span><br><span class="line">│   │   ├── tasks</span><br><span class="line">│   │   ├── templates</span><br><span class="line">│   │   └── vars</span><br><span class="line">│   └── meta</span><br><span class="line">├── kubespray-defaults</span><br><span class="line">│   ├── defaults</span><br><span class="line">│   ├── meta</span><br><span class="line">│   └── tasks</span><br></pre></td></tr></table></figure>
<p>在 kubespray 中，参数定义有三个位置：</p>
<ol>
<li>inventory/sample/group_vars</li>
<li>roles/kubespray-defaults/defaults</li>
<li>roles/<common>/defaults</common></li>
<li>使用 <code>set_fact</code> 关键字声明</li>
</ol>
<p>上述三个位置是按照参数粒度划分，参数粒度越细，越靠后。</p>
<p>举个例子：</p>
<ul>
<li><code>bin_dir</code> 这个参数，是一个全局参数，管理下载的二进制文件路径，它在 <code>inventory/samle/group_vars</code> 中定义； </li>
<li><code>etcd_kubeadm_enabled</code> 参数，决定是否通过 kubeadm 来创建 etcd 集群，是集群粒度的，在 <code>roles/kubesray-defaults/defaults</code> 中定义；</li>
<li><code>docker_fedora_repo_base_url</code> 是 docker 在下载镜像时指定的 repo，是一个容器运行时粒度的参数，那么它就在 <code>roles/container-ngine/docker/defaults</code> 中定义</li>
<li>执行某些命令后，我们希望对命令结果进行过滤或判断，那么我们通常会使用 regster，但是 register 的声明周期仅限于该 playbook，而且他们的优先级也是不同的，具体可以看下<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#ansible-variable-precedence" target="_blank" rel="noopener">官方文档</a></li>
</ul>
<p>可能跟大多数项目不同，kubespray 中涉及的参数数量很多，不知道是否是这个原因，专门使用了 <code>roles/kubespray-defaults</code> 这个 role 来声明参数。不过它的这种参数划分方式是我们值得学习的。</p>
<h2 id="Handler-触发"><a href="#Handler-触发" class="headerlink" title="Handler 触发"></a>Handler 触发</h2><p>当我们更新了配置文件之后，我们想要重启相应服务，这时候可以使用 notify 配合 handlers 来完成相应操作，而且 handler<br>的方式也可以保证我们代码最大程度的重用。</p>
<h2 id="loop-control"><a href="#loop-control" class="headerlink" title="loop_control"></a>loop_control</h2><p>当我们在 playbook 中定义了某个变量为 list 类型，我们想要遍历变量，并且针对每个变量的操作都进行错误处理，我们可以采用 loop_control 关键字配合 include 来实现，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- include_tasks: docker_plugin.yml</span><br><span class="line">  loop: &quot;&#123;&#123; docker_plugins &#125;&#125;&quot;</span><br><span class="line">  loop_control:</span><br><span class="line">    loop_var: docker_plugin</span><br></pre></td></tr></table></figure>
<p>循环 <code>device_plugins</code> ，每个变量为 <code>docker_plugin</code> ，将 <code>docker_plugin</code> 传递到 <code>docker_plugin.yml</code> 。</p>
<h2 id="不建议的操作"><a href="#不建议的操作" class="headerlink" title="不建议的操作"></a>不建议的操作</h2><h3 id="在-ansible-中使用复杂的语法规则"><a href="#在-ansible-中使用复杂的语法规则" class="headerlink" title="在 ansible 中使用复杂的语法规则"></a>在 ansible 中使用复杂的语法规则</h3><p>在 kubespray 中，随处可见一些很复杂的语法夹杂在 playbook 中，看到一个比较头疼的：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">279</span>	</span><br><span class="line"><span class="number">280</span>	<span class="attr">dashboard_image_repo:</span> <span class="string">"gcr.io/google_containers/kubernetes-dashboard-<span class="template-variable">&#123;&#123; image_arch &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">281</span>	<span class="attr">dashboard_image_tag:</span> <span class="string">"v1.10.1"</span></span><br><span class="line"><span class="number">282</span>	</span><br><span class="line"><span class="number">283</span>	<span class="attr">image_pull_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker pull"</span></span><br><span class="line"><span class="number">284</span>	<span class="attr">image_info_command:</span> <span class="string">"<span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker images -q | xargs <span class="template-variable">&#123;&#123; docker_bin_dir &#125;&#125;</span>/docker inspect -f \"<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoTags <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoTags 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> if .RepoDigests <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;,<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> (index .RepoDigests 0) <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;<span class="template-variable">&#123;&#123; '&#123;&#123;' &#125;&#125;</span> end <span class="template-variable">&#123;&#123; '&#125;&#125;</span>' &#125;&#125;\" | tr '\n' ','"</span></span><br><span class="line"><span class="number">285</span>	</span><br><span class="line"><span class="number">286</span>	<span class="attr">downloads:</span></span><br><span class="line"><span class="number">287</span>	  <span class="attr">netcheck_server:</span></span><br><span class="line"><span class="number">288</span>	    <span class="attr">enabled:</span> <span class="string">"<span class="template-variable">&#123;&#123; deploy_netchecker &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">289</span>	    <span class="attr">container:</span> <span class="literal">true</span></span><br><span class="line"><span class="number">290</span>	    <span class="attr">repo:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_repo &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">291</span>	    <span class="attr">tag:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_image_tag &#125;&#125;</span>"</span></span><br><span class="line"><span class="number">292</span>	    <span class="attr">sha256:</span> <span class="string">"<span class="template-variable">&#123;&#123; netcheck_server_digest_checksum|default(None) &#125;&#125;</span>"</span></span><br></pre></td></tr></table></figure>
<p>284 行获取 <code>image_info_command</code> ，这里真的是一点可维护性都没有，初步看很难看出这里到底是 ansible 语法，还是 shell 语法，还是 go template 语法，如果我们要通过这么复杂的方式来获取一个参数，为什么不干脆写一个脚本来完成这个事情呢？ 搞不懂。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>看完 kubespray 了解了 k8s 集群部署的步骤之外，Ansible 的一些高级用法或者说经验是之后需要改善的，收获多多。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/06/26/Kubernetes-实战-Pod-可用性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/26/Kubernetes-实战-Pod-可用性/" itemprop="url">Kubernetes 实战-Pod 可用性</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-26T21:57:31+08:00">
                2019-06-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/26/Kubernetes-实战-Pod-可用性/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/26/Kubernetes-实战-Pod-可用性/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性相关知识。</p>
<h2 id="K8S-可用性相关参数"><a href="#K8S-可用性相关参数" class="headerlink" title="K8S 可用性相关参数"></a>K8S 可用性相关参数</h2><p>k8s 核心组件有 kubelet,kube-apiserver,kube-scheduler,kube-controller-manager，通过阅读官方文档中相关参数说明，我摘取了认为跟可用性相关的参数，具体列表如下：</p>
<h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><h4 id="–housekeeping-interval-duration"><a href="#–housekeeping-interval-duration" class="headerlink" title="–housekeeping-interval duration"></a>–housekeeping-interval duration</h4><p>Default: 10s</p>
<p>Interval between container housekeepings.</p>
<p>kubelet 主动检测容器资源是否达到阈值的周期。</p>
<h4 id="–node-status-update-frequency-duration"><a href="#–node-status-update-frequency-duration" class="headerlink" title="–node-status-update-frequency duration"></a>–node-status-update-frequency duration</h4><p>Default: 10s</p>
<p>Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller. </p>
<p>kubelet 上报到 kube-apiserver 频率。</p>
<h3 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h3><h4 id="–node-eviction-rate-float32"><a href="#–node-eviction-rate-float32" class="headerlink" title="–node-eviction-rate float32"></a>–node-eviction-rate float32</h4><p>Default: 0.1 </p>
<p>Number of nodes per second on which pods are deleted in case of node failure when a zone is healthy (see –unhealthy-zone-threshold for definition of healthy/unhealthy). Zone refers to entire cluster in non-multizone clusters.</p>
<p>当 kube-controller-manager 判定节点故障，开始迁移（重建）pod 的速度，默认是 0.1，也就是 1Pod/10s 。</p>
<h4 id="–node-monitor-grace-period-duration"><a href="#–node-monitor-grace-period-duration" class="headerlink" title="–node-monitor-grace-period duration"></a>–node-monitor-grace-period duration</h4><p>Default: 40s </p>
<p>Amount of time which we allow running Node to be unresponsive before marking it unhealthy. Must be N times more than kubelet’s nodeStatusUpdateFrequency, where N means number of retries allowed for kubelet to post node status.</p>
<p>kube-controller-manager 标记 kubelet(node) 为不健康的周期。</p>
<h4 id="–node-monitor-period-duration"><a href="#–node-monitor-period-duration" class="headerlink" title="–node-monitor-period duration"></a>–node-monitor-period duration</h4><p>Default: 5s </p>
<p>The period for syncing NodeStatus in NodeController.</p>
<p>kube-controller-manager 定期检查 kubelet(node) 状态周期。</p>
<h4 id="–node-startup-grace-period-duration"><a href="#–node-startup-grace-period-duration" class="headerlink" title="–node-startup-grace-period duration"></a>–node-startup-grace-period duration</h4><p>Default: 1m0s </p>
<p>Amount of time which we allow starting Node to be unresponsive before marking it unhealthy.</p>
<p>kube-controller-manager 在标记节点为不健康之前允许无响应时间。</p>
<h4 id="–pod-eviction-timeout-duration"><a href="#–pod-eviction-timeout-duration" class="headerlink" title="–pod-eviction-timeout duration"></a>–pod-eviction-timeout duration</h4><p>Default: 5m0s </p>
<p>The grace period for deleting pods on failed nodes.</p>
<p>kube-controller-manager 判定节点故障，重建 Pod 的超时时间。</p>
<h2 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h2><p>看完了相关参数，我们来看下 kubelet 和 kube-controller-manager 是如何相互关联工作来保证 Pod 可用性的。</p>
<ol>
<li><p>kubelet 启动，若启动时间超过 node-startup-grace-period，则 kube-controller-manager 将其置为 unhealthy</p>
</li>
<li><p>kubelet 按照 –node-status-update-frequency 周期，定时与 kube-apiserver 通信将其状态记录到 etcd</p>
</li>
<li><p>若 kubelet 无法连接到 kube-apiserver，那么 kubelet 会尝试 nodeStatusUpdateRetry 次更新状态信息</p>
</li>
<li><p>kube-controller-manager 按照 –node-monitor-period 周期，定时从 etcd 中获取 kubelet 状态</p>
</li>
<li><p>若 kubelet 在 –node-monitor-grace-period 周期内均为非健康状态（这里如果 kubelet 未更新状态，等同），则该节点更新为 NotReady，将需要迁移（重建）的资源放置到队列中</p>
</li>
<li><p>当 kubelet NotReady 的 –pod-eviction-timeout 时间后， kube-controller-manager 开始进行 Pod 驱逐动作</p>
</li>
<li><p>驱逐速度为 –node-eviction-rate ，即每10s 迁移（重建）1个 Pod</p>
</li>
</ol>
<p>需要注意的是，上述 kubelet 和 kube-controller-manager 的操作是异步的，中间任何一个更新步骤都有可能出现延迟的情况，所以真实情况会比上述流程复杂（诡异）的多。比如 lijieao 的<a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html" target="_blank" rel="noopener">最新博客</a>中碰到的因为磁盘 IO 压力导致 kubelet NotReady 的情况，都是有可能出现的。</p>
<p>按照上面的流程，可以看到，当我们节点故障后，要花费 5min 的时间才会重建我们的业务，这种情况下大部分对稳定性要求较高的业务都无法忍受的，所以有同学可能会考虑修改默认配置，来减少业务宕机时间。</p>
<p>上述提到的参数的默认值，肯定是 k8s 社区经过多年的架构设计（或者经验？）的。之前看过社区中关于这部分的一篇文章提到，不建议修改默认的配置，担心修改了默认配置可能因为其他一些比较微小的故障导致 Pod 频繁的迁移，这是我们不想看到的。</p>
<p>上面提到的情况都是说：当节点发生故障后，我们希望我们的 Pod 能够第一时间迁移到正常运行的节点，那么有没有反过来的，不想迁移的呢？</p>
<p>其实是有的，比如有状态服务配合 kubelet 服务故障场景。我们的节点正常运行的情况下，容器运行时也都正常工作，唯独 kubelet 故障了，那么我们想想此时发生了什么？ </p>
<p>kube-controller-manager 在一段时间后标记节点故障，开始迁移（重建） Pod 操作，但是要注意，此时因为是有状态服务，而节点上的容器又在正常运行，kubelet 由于故障了，k8s 无法直接操作节点上的容器，只能在其他节点进行重建。这时候问题就出现了，集群中存在2个 Pod 同时读写一个 PV，这种错误是致命的。</p>
<p>在这种场景下，我们想要追求的是哪怕节点故障了，我们也要尽量的不迁移 Pod，来保证我们数据的读写正常。</p>
<p>那么上述情况我们应该怎么解决呢？ </p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Operator 是一种方式。在 Operator 出现之前，有状态服务的运维工作都是靠着探针或者其他的基础运维工具来完成的，很多工作即使做到了自动化也不完美，现在有了 Operator 结合 CRD，我们完全可以自己在应用层面监控服务状态，从而主动的触发 Pod 的迁移（重建），保证我们业务的稳定。</p>
<p>另一种解决方式是 <code>Taint based Evictions</code>，在 1.13 版本中增加了该配置，我们可以在创建资源的时候，指定规则配置Pod 容忍节点异常的时间，加快触发 Pod 重建，这大大减缓了对集群配置的要求。</p>
<h2 id="测试-YAML"><a href="#测试-YAML" class="headerlink" title="测试 YAML"></a>测试 YAML</h2><p>在测试 Pod 可用性的时候，我们必须创建 Deployment、RS 类型资源，单纯的 Pod 资源是不被保证可用性的。同时我们也想观察 DaemonSet Pod 在节点故障场景下的表现，所以一同创建了。可以使用以下 YAML 用来创建测试环境。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@m1</span> <span class="string">ha]#</span> <span class="string">cat</span> <span class="string">ha.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/unreachable"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">"node.kubernetes.io/not-ready"</span></span><br><span class="line"><span class="attr">        operator:</span> <span class="string">"Exists"</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">"NoExecute"</span></span><br><span class="line"><span class="attr">        tolerationSeconds:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">sleep</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">      restartPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">"apps/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">6</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">dp</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"dp"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">          done</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string"></span><span class="attr">apiVersion:</span> <span class="string">"extensions/v1beta1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"DaemonSet"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"ds"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ds</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">        effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">"apply-sysctl"</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"busybox:latest"</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">|</span></span><br><span class="line"><span class="string">          set -o errexit</span></span><br><span class="line"><span class="string">          set -o xtrace</span></span><br><span class="line"><span class="string">          while true</span></span><br><span class="line"><span class="string">          do</span></span><br><span class="line"><span class="string">            sleep 2s</span></span><br><span class="line"><span class="string">            date</span></span><br><span class="line"><span class="string">            echo "diu~"</span></span><br><span class="line"><span class="string">          done</span></span><br></pre></td></tr></table></figure>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md" target="_blank" rel="noopener">https://github.com/Kevin-fqh/learning-k8s-source-code/blob/master/kubelet/(05)kubelet%E8%B5%84%E6%BA%90%E4%B8%8A%E6%8A%A5%26Evition%E6%9C%BA%E5%88%B6.md</a></p>
<p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/kubernetes-reliability.md</a></p>
<p><a href="https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81" target="_blank" rel="noopener">https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2019/05/27/kubernetes-node-frequently-not-ready.html#%E8%A7%82%E5%AF%9F-kubelet-%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/06/21/Kubernetes-实战-Helm-包管理器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/21/Kubernetes-实战-Helm-包管理器/" itemprop="url">Kubernetes 实战-Helm 包管理器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-21T20:40:24+08:00">
                2019-06-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/21/Kubernetes-实战-Helm-包管理器/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/21/Kubernetes-实战-Helm-包管理器/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Helm 就是<code>k8s 的包管理器</code> 。常见的包管理器有：yum,apt,pip…</p>
<p>包管理器基础功能有：</p>
<ul>
<li>安装<ul>
<li>依赖安装</li>
</ul>
</li>
<li>升级</li>
<li>回滚</li>
<li>卸载</li>
<li>源管理</li>
<li>搜索</li>
<li>…</li>
</ul>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p>Helm: Kubernetes的包管理工具，命令行同名</p>
</li>
<li><p>Tiller: Helmv2 的服务端，用于接收并处理 Helm 发送的请求，默认以 Deployment 形式部署在 k8s 集群中</p>
</li>
<li><p>Chart: Helm 包管理的基础单元，等同于 RPM</p>
</li>
<li><p>Repoistory: Helm的软件源仓库，是一个 Web 服务器，路径下除了响应的软件 Chart 包之外，维护了一个 index.yaml 用于索引</p>
</li>
<li><p>Release: Helm 安装在 Kubernetes 集群中的 Chart 实例</p>
</li>
</ul>
<h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><p>helm 截至06月20日最新稳定版本为 v2.14.1。</p>
<p>在05月16日发布了 v3.0 alpha 版本，根据相关文档描述，v2 无法平滑升级到 v3 版本。</p>
<p>注：存在部分小版本无法平滑升级情况。</p>
<p>helm v3 版本改进：</p>
<ol>
<li>在 v2 版本设计中，需要单独创建属于 Tiller 的 ServiceAccount，授权 clusteradmin 权限，以为着只要你有 helm 权限，那么你有操作 k8s全集群所有权限。在 v3 版本中删除 Tiller，直接与 k8s api 进行通信，权限管理更清晰</li>
<li>helm 提供 libary</li>
<li>模板引擎切换为 Lua</li>
<li>目前通过 Hook 方式创建的资源，helm 后续不会管理，在 v3 会增加管理 Hook 资源功能</li>
<li>目前所有配置保存在 cm 中，后续考虑保存到 secret</li>
<li>v2 需要单独维护仓库，v3 中可以将 Chart 推送到 Docker 镜像仓库中，提供 helm push/login 功能</li>
<li>…</li>
</ol>
<h2 id="Helm2-基本使用"><a href="#Helm2-基本使用" class="headerlink" title="Helm2 基本使用"></a>Helm2 基本使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在 Helm Github <a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">Release</a> 下载最新版本二进制文件，并在本地解压。</p>
<p>在 k8s master 节点，执行 <code>helm init --server-account tiller</code> 将 Tiller 部署在 k8s 集群中，指定 Service Account 为 Tiller。</p>
<p>在 k8s 1.6 版本之后，需要创建对应的 ServiceAccount 资源给 Tiller ，便于 Tiller 后续创建资源，参考官方文档：</p>
<p>rbac-config.yaml<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount <span class="string">"tiller"</span> created</span><br><span class="line">clusterrolebinding <span class="string">"tiller"</span> created</span><br></pre></td></tr></table></figure>
<p>通过 <code>helm version</code> 验证是否部署成功，成功会显示 client 和 server 对应版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.1"</span>, GitCommit:<span class="string">"5270352a09c7e8b6e8c9593002a73535276507c0"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="Chart构建"><a href="#Chart构建" class="headerlink" title="Chart构建"></a>Chart构建</h4><p>本地创建一个测试软件包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm create yiran-test</span></span><br><span class="line">Creating yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts                            <span class="comment"># yiran-test 所依赖的 Chart，此处为空</span></span><br><span class="line">├── Chart.yaml                        <span class="comment"># yiran-test 的基本信息，包含：名称，apiversion, appversion, version</span></span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml               <span class="comment"># 配置模板路径</span></span><br><span class="line">│   ├── _helpers.tpl                  <span class="comment"># 用于修改kubernetes objcet配置的模板</span></span><br><span class="line">│   ├── ingress.yaml                  <span class="comment"># </span></span><br><span class="line">│   ├── NOTES.txt                     <span class="comment"># 类似于 Chart README</span></span><br><span class="line">│   ├── service.yaml                  <span class="comment"># </span></span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml      <span class="comment"># 测试模板</span></span><br><span class="line">└── values.yaml                       <span class="comment"># 用于渲染模板的具体值</span></span><br><span class="line"></span><br><span class="line">3 directories, 8 files</span><br></pre></td></tr></table></figure>
<p>有了基础示例，我们可以先通过 <code>dry-run</code> 方式跑一下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install --dry-run --debug ./</span></span><br><span class="line">[debug] Created tunnel using <span class="built_in">local</span> port: <span class="string">'39257'</span></span><br><span class="line"></span><br><span class="line">[debug] SERVER: <span class="string">"127.0.0.1:39257"</span></span><br><span class="line"></span><br><span class="line">[debug] Original chart version: <span class="string">""</span></span><br><span class="line">[debug] CHART PATH: /root/helm/yiran-test</span><br><span class="line"></span><br><span class="line">NAME:   torrid-rodent</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Thu Jun 20 19:10:08 2019</span><br><span class="line">CHART: yiran-test-0.1.0</span><br><span class="line">USER-SUPPLIED VALUES:</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">COMPUTED VALUES:</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line">fullnameOverride: <span class="string">""</span></span><br><span class="line">image:</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">imagePullSecrets: []</span><br><span class="line">ingress:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  enabled: <span class="literal">false</span></span><br><span class="line">  hosts:</span><br><span class="line">  - host: chart-example.local</span><br><span class="line">    paths: []</span><br><span class="line">  tls: []</span><br><span class="line">nameOverride: <span class="string">""</span></span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line">replicaCount: 1</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  port: 80</span><br><span class="line">  <span class="built_in">type</span>: ClusterIP</span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># torrid-rodent-yiran-test-test-connection</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="string">"torrid-rodent-yiran-test-test-connection"</span></span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: yiran-test</span><br><span class="line">    helm.sh/chart: yiran-test-0.1.0</span><br><span class="line">    app.kubernetes.io/instance: torrid-rodent</span><br><span class="line">    app.kubernetes.io/version: <span class="string">"1.0"</span></span><br><span class="line">    app.kubernetes.io/managed-by: Tiller</span><br><span class="line">  annotations:</span><br><span class="line">    <span class="string">"helm.sh/hook"</span>: <span class="built_in">test</span>-success</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: wget</span><br><span class="line">      image: busybox</span><br><span class="line">      <span class="built_in">command</span>: [<span class="string">'wget'</span>]</span><br><span class="line">      args:  [<span class="string">'torrid-rodent-yiran-test:80'</span>]</span><br><span class="line">  restartPolicy: Never</span><br><span class="line">MANIFEST:</span><br></pre></td></tr></table></figure>
<h4 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h4><p>可以看到生成的 YAML 文件就是拿 values.yaml 值渲染模板生成的，那么我们来安装一下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm install ./</span></span><br><span class="line">NAME:   precise-bear</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:12:01 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS             RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Running   0          7s</span><br></pre></td></tr></table></figure>
<p>注意，此时我们已经创建好了对应的资源，可以通过 <code>kubectl</code> 来查看状态。</p>
<h4 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h4><p>我们来修改一下 <code>yiran-test/Chart.yaml</code> ，调整下版本，进行升级：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># cat Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">heNAME          REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    1           Thu Jun 20 19:12:01 2019    DEPLOYED    yiran-test-0.1.0    1.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm upgrade precise-bear ./</span></span><br><span class="line">Release <span class="string">"precise-bear"</span> has been upgraded.</span><br><span class="line">LAST DEPLOYED: Thu Jun 20 19:18:30 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                     READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">precise-bear-yiran-test  1/1    1           1          6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                      READY  STATUS   RESTARTS  AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6  1/1    Running  0         6m30s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                     TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">precise-bear-yiran-test  ClusterIP  10.68.142.106  &lt;none&gt;       80/TCP   6m30s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=precise-bear"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default</span><br></pre></td></tr></table></figure>
<h4 id="回滚"><a href="#回滚" class="headerlink" title="回滚"></a>回滚</h4><p>通过上述步骤，我们已经把我们的应用 <code>yiran-test</code> 从 <code>0.1.0</code> 版本升级到了 <code>0.2.0</code> 版本，那么我们现在来尝试下回滚。</p>
<p>helm 回滚操作需要指定 Release 名称和目标版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    2           Thu Jun 20 19:18:30 2019    DEPLOYED    yiran-test-0.2.0    2.0         default  </span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm rollback precise-bear 1</span></span><br><span class="line">Rollback was a success.</span><br><span class="line">[root@node1 yiran-test]<span class="comment"># helm list </span></span><br><span class="line">NAME            REVISION    UPDATED                     STATUS      CHART               APP VERSION NAMESPACE</span><br><span class="line">precise-bear    3           Thu Jun 20 19:23:04 2019    DEPLOYED    yiran-test-0.1.0    1.0         default</span><br></pre></td></tr></table></figure>
<p>嗯，可以看到，我们指定了回滚的目标 REVISION，回滚成功了，<code>yiran-test</code> 回到了 <code>0.1.0</code> 版本，但是，最恶心的来了，Release 的当前版本变成了 <code>3</code> ，而不是设想中的 <code>1</code> 。</p>
<h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><p>如果我们想要从 k8s 上卸载对应的软件，也就是我们的 <code>yiran-test</code> ，我们可以直接执行 <code>delete</code> 命令，会直接把相关资源全部删除掉。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">NAME            REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">precise-bear    3               Thu Jun 20 19:23:04 2019        DEPLOYED        yiran-test-0.1.0        1.0             default</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">kuNAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   1/1     Running   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes                ClusterIP   10.68.0.1       &lt;none&gt;        443/TCP   36d</span><br><span class="line">precise-bear-yiran-test   ClusterIP   10.68.142.106   &lt;none&gt;        80/TCP    139m</span><br><span class="line">[root@node1 ~]<span class="comment"># helm delete precise-bear</span></span><br><span class="line">release <span class="string">"precise-bear"</span> deleted</span><br><span class="line">[root@node1 ~]<span class="comment"># helm list</span></span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get pod</span></span><br><span class="line">NAME                                       READY   STATUS        RESTARTS   AGE</span><br><span class="line">precise-bear-yiran-test-785f967587-9rll6   0/1     Terminating   0          139m</span><br><span class="line">[root@node1 ~]<span class="comment"># kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.68.0.1    &lt;none&gt;        443/TCP   36d</span><br></pre></td></tr></table></figure>
<h4 id="软件源管理"><a href="#软件源管理" class="headerlink" title="软件源管理"></a>软件源管理</h4><p>上面我们所有的操作都是针对本地包（路径），那么我们怎么才能通过网络下载别人已经构建好的软件包呢？ </p>
<p>helm 使用 <code>repo</code> 命令来管理软件源，使用上也很简单，简单列举下相应命令实用说明：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME    URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>   http://127.0.0.1:8879/charts</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable-mirror https://burdenbear.github.io/kube-charts-mirror/</span></span><br><span class="line"><span class="string">"stable-mirror"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo add stable https://kubernetes-charts.storage.googleapis.com</span></span><br><span class="line">helm repo list</span><br><span class="line"><span class="string">"stable"</span> has been added to your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>           http://127.0.0.1:8879/charts</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo remove local</span></span><br><span class="line"><span class="string">"local"</span> has been removed from your repositories</span><br><span class="line">[root@node1 ~]<span class="comment"># helm repo list</span></span><br><span class="line">NAME            URL</span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirror   https://burdenbear.github.io/kube-charts-mirror/</span><br></pre></td></tr></table></figure>
<h4 id="软件搜索"><a href="#软件搜索" class="headerlink" title="软件搜索"></a>软件搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># helm search mongo</span></span><br><span class="line">NAME                                            CHART VERSION   APP VERSION     DESCRIPTION</span><br><span class="line">stable-mirror/mongodb                           5.20.0          4.0.10          NoSQL document-oriented database that stores JSON-like <span class="keyword">do</span>...</span><br><span class="line">stable-mirror/mongodb-replicaset                3.9.6           3.6             </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="打包与本地软件源构建"><a href="#打包与本地软件源构建" class="headerlink" title="打包与本地软件源构建"></a>打包与本地软件源构建</h4><p>Helm v2 命令支持本地创建软件源，命令关键字是 <code>helm serve</code> ，下面来演示下相关操作：</p>
<p>启动本地源，并指定 IP 地址和 repo 路径：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">rbac-config.yaml  yiran-test</span><br><span class="line">[root@node1 helm]<span class="comment"># helm serve --address 192.168.27.231:8879 --repo-path /root/helm/ </span></span><br><span class="line">Regenerating index. This may take a moment.</span><br><span class="line">Now serving you on 192.168.27.231:8879</span><br></pre></td></tr></table></figure></p>
<p>新开终端，通过修改 <code>yiran-test</code> 的 Chart.yaml 文件打包两个版本的 Chart：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: <span class="string">"1.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.1.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.1.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># vi yiran-test/Chart.yaml </span></span><br><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/Chart.yaml</span></span><br><span class="line">apiVersion: v2</span><br><span class="line">appVersion: <span class="string">"2.0"</span></span><br><span class="line">description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">name: yiran-test</span><br><span class="line">version: 0.2.0</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br><span class="line">[root@node1 helm]<span class="comment"># ls </span></span><br><span class="line">index.yaml  rbac-config.yaml  yiran-test  yiran-test-0.1.0.tgz  yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure>
<p>此时访问浏览器，应该只能看到空的列表，我们更新一下软件源索引：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo index --url=http://192.168.27.231:8879 .</span></span><br><span class="line">[root@node1 helm]<span class="comment"># pwd</span></span><br><span class="line">/root/helm</span><br><span class="line">[root@node1 helm]<span class="comment"># cat index.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">entries:</span><br><span class="line">  yiran-test:</span><br><span class="line">  - apiVersion: v2</span><br><span class="line">    appVersion: <span class="string">"2.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220764856+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: af54cfdc5f8e6463a91311496bab8fafd7364c3588b85b5e676fb930cd4e2754</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.2.0.tgz</span><br><span class="line">    version: 0.2.0</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    appVersion: <span class="string">"1.0"</span></span><br><span class="line">    created: <span class="string">"2019-06-21T13:43:41.220059406+08:00"</span></span><br><span class="line">    description: A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">    digest: 64b94c30827aab8e52f57cd3950645bb14ae2deb44e3100d48ecd64f1e706ea5</span><br><span class="line">    name: yiran-test</span><br><span class="line">    urls:</span><br><span class="line">    - http://192.168.27.231:8879/yiran-test-0.1.0.tgz</span><br><span class="line">    version: 0.1.0</span><br><span class="line">generated: <span class="string">"2019-06-21T13:43:41.218753007+08:00"</span></span><br></pre></td></tr></table></figure>
<p>可以看到在 repo 路径下生成了一个 index.yaml 文件，这个文件就是 repo 的索引文件，我们可以直接通过浏览器访问 <code>http://192.168.27.231:8879</code> 来浏览或下载所需软件包。</p>
<p>也可以将本地 repo 添加到 helm 中，使用 helm 命令将软件包部署到 k8s 中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         	URL                                             </span><br><span class="line">stable       	https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirror	https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo add local http://192.168.27.231:8879</span></span><br><span class="line"><span class="string">"local"</span> has been added to your repositories</span><br><span class="line">[root@node1 helm]<span class="comment"># helm repo list </span></span><br><span class="line">NAME         	URL                                             </span><br><span class="line">stable       	https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">stable-mirror	https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line"><span class="built_in">local</span>        	http://192.168.27.231:8879                      </span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            	CHART VERSION	APP VERSION	DESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test	0.2.0        	2.0        	A Helm chart <span class="keyword">for</span> Kubernetes</span><br></pre></td></tr></table></figure>
<p>尝试从本地源安装应用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">[root@node1 helm]<span class="comment"># helm search yiran</span></span><br><span class="line">NAME            	CHART VERSION	APP VERSION	DESCRIPTION                </span><br><span class="line"><span class="built_in">local</span>/yiran-test	0.2.0        	2.0        	A Helm chart <span class="keyword">for</span> Kubernetes</span><br><span class="line">[root@node1 helm]<span class="comment"># helm install local/yiran-test</span></span><br><span class="line">NAME:   orange-chicken</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 13:49:45 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">orange-chicken-yiran-test  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                        READY  STATUS             RESTARTS  AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                       TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">orange-chicken-yiran-test  ClusterIP  10.68.85.197  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=yiran-test,app.kubernetes.io/instance=orange-chicken"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME          	REVISION	UPDATED                 	STATUS  	CHART           	APP VERSION	NAMESPACE</span><br><span class="line">orange-chicken	1       	Fri Jun 21 13:49:45 2019	DEPLOYED	yiran-test-0.2.0	2.0        	default </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">orange-chicken-yiran-test-7f494c67b5-q9kwp   1/1     Running   0          62s</span><br></pre></td></tr></table></figure>
<h4 id="Chart-依赖管理"><a href="#Chart-依赖管理" class="headerlink" title="Chart 依赖管理"></a>Chart 依赖管理</h4><p>包管理器一个比较钟要的功能就是依赖管理，当我安装 A，A 依赖于 B，那么 B 应该会自动安装完成。</p>
<p>我们在 <code>yiran-test</code> 中添加两个依赖，并构建：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># cat yiran-test/requirements.yaml   # 添加 apache 和 mysql 依赖</span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: apache</span><br><span class="line">    version: 4.3.2</span><br><span class="line">    repository: https://charts.bitnami.com</span><br><span class="line">  - name: mysql</span><br><span class="line">    version: 1.2.0</span><br><span class="line">    repository: https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 当前目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm dep up yiran-test/ # 下载依赖到本地</span></span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Successfully got an update from the <span class="string">"local"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"bitnami"</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable-mirror"</span> chart repository</span><br><span class="line">Update Complete.</span><br><span class="line">Saving 2 charts</span><br><span class="line">Downloading apache from repo https://charts.bitnami.com</span><br><span class="line">Downloading mysql from repo https://burdenbear.github.io/kube-charts-mirror/</span><br><span class="line">Deleting outdated charts</span><br><span class="line">[root@node1 helm]<span class="comment"># tree yiran-test/ # 完整目录结构</span></span><br><span class="line">yiran-test/</span><br><span class="line">├── charts</span><br><span class="line">│   ├── apache-4.3.2.tgz</span><br><span class="line">│   └── mysql-1.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 12 files</span><br><span class="line">[root@node1 helm]<span class="comment"># helm package yiran-test</span></span><br><span class="line">Successfully packaged chart and saved it to: /root/helm/yiran-test-0.2.0.tgz</span><br></pre></td></tr></table></figure>
<p>可以看到 helm 对依赖的管理方式是将自己所依赖的所有 Chart，均下载到 <code>yiran-test/charts/</code> 路径下，我们打包的时候其实已经包含了所有依赖了。</p>
<p>再创建一个 Chart，依赖于 <code>yiran-test</code> ：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># tree nested/</span></span><br><span class="line">nested/</span><br><span class="line">├── charts</span><br><span class="line">│   └── yiran-test-0.2.0.tgz</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── requirements.lock</span><br><span class="line">├── requirements.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── <span class="built_in">test</span>-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 11 files</span><br><span class="line">[root@node1 helm]<span class="comment"># cat nested/requirements.yaml </span></span><br><span class="line">dependencies:</span><br><span class="line">  - name: yiran-test</span><br><span class="line">    version: 0.2.0</span><br><span class="line">    repository: http://192.168.27.231:8879</span><br></pre></td></tr></table></figure>
<p>实际安装 <code>nested</code> ，来看下安装结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]<span class="comment"># helm install nested/</span></span><br><span class="line">NAME:   alliterating-gopher</span><br><span class="line">LAST DEPLOYED: Fri Jun 21 18:22:52 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                            DATA  AGE</span><br><span class="line">alliterating-gopher-mysql-test  1     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                            READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-nested      0/1    1           0          1s</span><br><span class="line">alliterating-gopher-yiran-test  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/PersistentVolumeClaim</span><br><span class="line">NAME                       STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">alliterating-gopher-mysql  Pending  1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                             READY  STATUS             RESTARTS  AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg        0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm       0/1    Pending            0         1s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq      0/1    ContainerCreating  0         1s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME                       TYPE    DATA  AGE</span><br><span class="line">alliterating-gopher-mysql  Opaque  2     1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                            TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE</span><br><span class="line">alliterating-gopher-apac        LoadBalancer  10.68.35.233   &lt;pending&gt;    80:21195/TCP,443:26200/TCP  1s</span><br><span class="line">alliterating-gopher-mysql       ClusterIP     10.68.70.173   &lt;none&gt;       3306/TCP                    1s</span><br><span class="line">alliterating-gopher-nested      ClusterIP     10.68.190.252  &lt;none&gt;       80/TCP                      1s</span><br><span class="line">alliterating-gopher-yiran-test  ClusterIP     10.68.217.171  &lt;none&gt;       80/TCP                      1s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                       READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">alliterating-gopher-apac   0/1    1           0          1s</span><br><span class="line">alliterating-gopher-mysql  0/1    1           0          1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  <span class="built_in">export</span> POD_NAME=$(kubectl get pods --namespace default -l <span class="string">"app.kubernetes.io/name=nested,app.kubernetes.io/instance=alliterating-gopher"</span> -o jsonpath=<span class="string">"&#123;.items[0].metadata.name&#125;"</span>)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Visit http://127.0.0.1:8080 to use your application"</span></span><br><span class="line">  kubectl port-forward <span class="variable">$POD_NAME</span> 8080:80</span><br><span class="line"></span><br><span class="line">[root@node1 helm]<span class="comment"># helm list </span></span><br><span class="line">NAME               	REVISION	UPDATED                 	STATUS  	CHART       	APP VERSION	NAMESPACE</span><br><span class="line">alliterating-gopher	1       	Fri Jun 21 18:22:52 2019	DEPLOYED	nested-0.1.0	1.0        	default  </span><br><span class="line">[root@node1 helm]<span class="comment"># kubectl get pod </span></span><br><span class="line">NAME                                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">alliterating-gopher-apac-7bf7cc75d5-mhdsg         0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-mysql-5db64c59d9-987vm        0/1     Pending   0          5s</span><br><span class="line">alliterating-gopher-nested-6c74d785df-jdqgq       0/1     Running   0          5s</span><br><span class="line">alliterating-gopher-yiran-test-67b5cb5599-nfspm   1/1     Running   0          5s</span><br></pre></td></tr></table></figure>
<p>我们可以看到 nested 已经安装完成了，同时 nested 依赖得 yiran-test 及 yiran-test 依赖得 mysql &amp; apache 也已经安装了。</p>
<p>具体得依赖关系直接引用官方文档示例：</p>
<p>假设名为 “A” 的 chart 创建以下 Kubernetes 对象</p>
<blockquote>
<p>namespace “A-Namespace”<br>statefulset “A-StatefulSet”<br>service “A-Service”  </p>
</blockquote>
<p>此外，A 依赖于创建对象的 chart B.</p>
<blockquote>
<p>namespace “B-Namespace”<br>replicaset “B-ReplicaSet”<br>service “B-Service”  </p>
</blockquote>
<p>安装/升级 chart A 后，会创建/修改单个 Helm 版本。该版本将按以下顺序创建/更新所有上述 Kubernetes 对象：</p>
<blockquote>
<p>A-Namespace<br>B-Namespace<br>A-StatefulSet<br>B-ReplicaSet<br>A-Service<br>B-Service  </p>
</blockquote>
<p>这是因为当 Helm 安装 / 升级 charts 时，charts 中的 Kubernetes 对象及其所有依赖项都是如下</p>
<ol>
<li>聚合成一个单一的集合; </li>
<li>按类型排序，然后按名称排序; </li>
<li>按该顺序创建/更新。</li>
</ol>
<p>因此，单个 release 是使用 charts 及其依赖关系创建的所有对象，这意味着 Helm 不管理依赖服务的相关启动顺序，要由上层应用自己控制（比如创建响应探针）。</p>
<p>具体的资源创建顺序可以看 Tiller <a href="https://github.com/helm/helm/blob/master/pkg/tiller/kind_sorter.go#L29" target="_blank" rel="noopener">相关代码</a>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说 Helm 基础功能使用还是很方便的，关键在于我们如何划分我们应用的粒度，比如 openstack 是以组件为粒度划分的：nova Chart 包含 api、scheduler、conductor 等服务；比如 TiDB Operator 项目是以具体功能来划分的：tidb-backup、tidb-cluster、tidb-operator 。我们应该根据自己的业务需求，合理划分。</p>
<p>还有一点需要注意的是，Helm 项目还处于快速发展阶段（貌似涉及 k8s 的都变化太快），尤其是最近发布了 Helm3 alpha，如果是生产系统，需要考虑后续是否能够平滑升级的影响。</p>
<p>如果实在不喜欢 Helm Tiller 方式，单纯使用 Helm template 也是可以的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/06/17/调整-arp-参数提高网络稳定性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/17/调整-arp-参数提高网络稳定性/" itemprop="url">调整 arp 参数提高网络稳定性</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-17T21:20:04+08:00">
                2019-06-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/17/调整-arp-参数提高网络稳定性/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/17/调整-arp-参数提高网络稳定性/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在机房网络规划中，地区 A 和地区 B 是通过 OpenVPN 连接的，也就是说每个地区的网关是一台虚拟机，提供 DHCP 服务。<br>今天地区 A 的机器又无法连接地区 B 了，我登陆网关尝试从网关 ping 目标主机，发现直接提示 :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No buffer space available</span><br></pre></td></tr></table></figure>
<p>根据这个提示，感觉像是某些系统参数配置的小了，于是查了一下，发现跟 arp 有关。什么是 arp ？ </p>
<p>相信对网络稍微有些概念的同学都不陌生，这里我直接引用维基百科：</p>
<blockquote>
<p>地址解析协议（英语：Address Resolution Protocol，缩写：ARP）。在以太网协议中规定，同一局域网中的一台主机要和另一台主机进行直接通信，必须要知道目标主机的MAC地址。而在TCP/IP协议中，网络层和传输层只关心目标主机的IP地址。这就导致在以太网中使用IP协议时，数据链路层的以太网协议接到上层IP协议提供的数据中，只包含目的主机的IP地址。于是需要一种方法，根据目的主机的IP地址，获得其MAC地址。这就是ARP协议要做的事情。所谓地址解析（address resolution）就是主机在发送帧前将目标IP地址转换成目标MAC地址的过程。</p>
</blockquote>
<blockquote>
<p>另外，当发送主机和目的主机不在同一个局域网中时，即便知道对方的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为委托ARP或ARP代理（ARP Proxy）。</p>
</blockquote>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>知道了原因，那么我们来调整参数就好：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gc_thresh1 (since Linux 2.2)</span><br><span class="line">The minimum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will not run <span class="keyword">if</span> there are fewer than this number of entries <span class="keyword">in</span> the cache. Defaults to 128.</span><br><span class="line">gc_thresh2 (since Linux 2.2)</span><br><span class="line">The soft maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will allow the number of entries to exceed this <span class="keyword">for</span> 5 seconds before collection will be performed. Defaults to 512.</span><br><span class="line">gc_thresh3 (since Linux 2.2)</span><br><span class="line">The hard maximum number of entries to keep <span class="keyword">in</span> the ARP cache. The garbage collector will always run <span class="keyword">if</span> there are more than this number of entries <span class="keyword">in</span> the cache. Defaults to 1024.</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yiran@<span class="built_in">test</span>:~$ cat /etc/sysctl.conf |grep -v ^<span class="comment"># |grep -v ^$</span></span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">yiran@<span class="built_in">test</span>:~$ sudo sysctl -p</span><br><span class="line">net.ipv4.tcp_congestion_control = bbr</span><br><span class="line">net.core.default_qdisc = fq</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/06/15/Kubernetes-实战-高可用集群部署/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/15/Kubernetes-实战-高可用集群部署/" itemprop="url">Kubernetes 实战-高可用集群部署</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-15T01:44:28+08:00">
                2019-06-15
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/15/Kubernetes-实战-高可用集群部署/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/15/Kubernetes-实战-高可用集群部署/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>本文所有节点 OS 均为 CentOS 7.4 。</p>
<h3 id="1-关闭-selinux"><a href="#1-关闭-selinux" class="headerlink" title="1.关闭 selinux"></a>1.关闭 selinux</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/selinux/config </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of three two values:</span></span><br><span class="line"><span class="comment">#     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="comment">#     minimum - Modification of targeted policy. Only selected processes are protected. </span></span><br><span class="line"><span class="comment">#     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node211 ~]<span class="comment"># getenforce </span></span><br><span class="line">Disabled</span><br></pre></td></tr></table></figure>
<h3 id="2-关于-firewalld"><a href="#2-关于-firewalld" class="headerlink" title="2. 关于 firewalld"></a>2. 关于 firewalld</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br></pre></td></tr></table></figure>
<h3 id="3-安装必要-yum-源：epel-release"><a href="#3-安装必要-yum-源：epel-release" class="headerlink" title="3. 安装必要 yum 源：epel-release"></a>3. 安装必要 yum 源：epel-release</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install epel-release</span></span><br><span class="line">[root@node211 ~]<span class="comment"># ls /etc/yum.repos.d/epel.repo </span></span><br><span class="line">/etc/yum.repos.d/epel.repo</span><br></pre></td></tr></table></figure>
<h3 id="4-关闭节点-swap-空间"><a href="#4-关闭节点-swap-空间" class="headerlink" title="4. 关闭节点 swap 空间"></a>4. 关闭节点 swap 空间</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/fstab </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># /etc/fstab</span></span><br><span class="line"><span class="comment"># Created by anaconda on Thu Jun 13 09:45:52 2019</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Accessible filesystems, by reference, are maintained under '/dev/disk'</span></span><br><span class="line"><span class="comment"># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">/dev/mapper/centos-root /                       xfs     defaults        0 0</span><br><span class="line">UUID=c0f0a31a-0c36-42cf-b52a-8f3b027ef948 /boot                   xfs     defaults        0 0</span><br><span class="line"><span class="comment">#/dev/mapper/centos-swap swap                    swap    defaults        0 0</span></span><br><span class="line">[root@node211 ~]<span class="comment"># free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.7G        102M        3.3G        8.3M        230M        3.3G</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>
<h3 id="5-安装-docker-ce"><a href="#5-安装-docker-ce" class="headerlink" title="5. 安装 docker-ce"></a>5. 安装 docker-ce</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># </span></span><br><span class="line">[root@node211 ~]<span class="comment"># head -n 6 /etc/yum.repos.d/docker-ce.repo</span></span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line">[root@node211 ~]<span class="comment"># rpm -q docker</span></span><br><span class="line">docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64</span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl start docker</span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl status docker</span></span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Fri 2019-06-14 19:48:40 CST; 3s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line"> Main PID: 11488 (dockerd-current)</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           ├─11488 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --<span class="built_in">exec</span>-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr...</span><br><span class="line">           └─11495 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-r...</span><br><span class="line"></span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.282909889+08:00"</span> level=info msg=<span class="string">"Docker daemon"</span> commit=<span class="string">"b2f74b2/1.13.1"</span> graphdriver=overlay2 version=1.13.1</span><br><span class="line">Jun 14 19:48:40 node211 dockerd-current[11488]: time=<span class="string">"2019-06-14T19:48:40.293315055+08:00"</span> level=info msg=<span class="string">"API listen on /var/run/docker.sock"</span></span><br><span class="line">Jun 14 19:48:40 node211 systemd[1]: Started Docker Application Container Engine.</span><br></pre></td></tr></table></figure>
<h3 id="6-开启必要系统参数-sysctl"><a href="#6-开启必要系统参数-sysctl" class="headerlink" title="6. 开启必要系统参数 sysctl"></a>6. 开启必要系统参数 sysctl</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># sysctl -p</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br></pre></td></tr></table></figure>
<h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>因为 kubeadm 官方文档中没有详细步骤，因此相关描述尽量具体到命令行。</p>
<h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><table>
<thead>
<tr>
<th>ip</th>
<th>role</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.77.211</td>
<td>master</td>
</tr>
<tr>
<td>192.168.77.212</td>
<td>master</td>
</tr>
<tr>
<td>192.168.77.213</td>
<td>master </td>
</tr>
<tr>
<td>192.168.77.214</td>
<td>node</td>
</tr>
</tbody>
</table>
<h3 id="1-安装-kubeadm"><a href="#1-安装-kubeadm" class="headerlink" title="1. 安装 kubeadm"></a>1. 安装 kubeadm</h3><p>所有节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/yum.repos.d/kubernetes.repo </span></span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">proxy=socks5://127.0.0.1:1080</span><br><span class="line">[root@node211 ~]<span class="comment"># yum install kubeadm kubelet</span></span><br><span class="line">[root@node211 ~]<span class="comment"># which kubeadm </span></span><br><span class="line">/usr/bin/kubeadm</span><br></pre></td></tr></table></figure>
<h3 id="2-安装-keepalived"><a href="#2-安装-keepalived" class="headerlink" title="2. 安装 keepalived"></a>2. 安装 keepalived</h3><p>所有 master 节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install keepalived </span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install keepalived</span></span><br></pre></td></tr></table></figure>
<p>编辑 node211 配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 100         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编辑 node212 配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 90         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编辑 node213 配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># cat /etc/keepalived/keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">        feng110498@163.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">   smtp_server 127.0.0.1</span><br><span class="line">   smtp_connect_timeout 30</span><br><span class="line">   router_id LVS_1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER          </span><br><span class="line">    interface eth0</span><br><span class="line">    lvs_sync_daemon_inteface eth0</span><br><span class="line">    virtual_router_id 79</span><br><span class="line">    advert_int 1</span><br><span class="line">    priority 70         </span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      192.168.77.219/20</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>node211, node212, node213 重启 keepalived：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl restart keepalived</span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl restart keepalived</span></span><br></pre></td></tr></table></figure>
<p>因为 node211 优先级最高，此时 VIP 192.168.77.219 应该在 node211 节点，查看 node211 节点 IP：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># ip ad </span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 52:54:00:42:fd:a6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.77.211/20 brd 192.168.79.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.77.219/20 scope global secondary eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::5554:b212:7895:c8ad/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::3e97:25b9:cc1a:809c/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::7a4f:3726:af17:18bf/64 scope link tentative dadfailed </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </span><br><span class="line">    link/ether 02:42:6f:0e:81:59 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>配置无异常，node211,node212,node213 设置开机自启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable keepalived</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</span><br></pre></td></tr></table></figure>
<h3 id="3-安装-haproxy"><a href="#3-安装-haproxy" class="headerlink" title="3. 安装 haproxy"></a>3. 安装 haproxy</h3><p>所有 master 节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node212 ~]<span class="comment"># yum install haproxy</span></span><br><span class="line">[root@node213 ~]<span class="comment"># yum install haproxy</span></span><br></pre></td></tr></table></figure>
<p>编辑所有 master 节点配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat /etc/haproxy/haproxy.cfg </span></span><br><span class="line">global</span><br><span class="line">        chroot  /var/lib/haproxy</span><br><span class="line">        daemon</span><br><span class="line">        group haproxy</span><br><span class="line">        user haproxy</span><br><span class="line">        <span class="built_in">log</span> 127.0.0.1:514 local0 warning</span><br><span class="line">        pidfile /var/lib/haproxy.pid</span><br><span class="line">        maxconn 20000</span><br><span class="line">        spread-checks 3</span><br><span class="line">        nbproc 8</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">        <span class="built_in">log</span>     global</span><br><span class="line">        mode    tcp</span><br><span class="line">        retries 3</span><br><span class="line">        option redispatch</span><br><span class="line"></span><br><span class="line">listen https-apiserver</span><br><span class="line">        <span class="built_in">bind</span> *:8443</span><br><span class="line">        mode tcp</span><br><span class="line">        balance roundrobin</span><br><span class="line">        timeout server 900s</span><br><span class="line">        timeout connect 15s</span><br><span class="line"></span><br><span class="line">        server m1 192.168.77.211:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m2 192.168.77.212:6443 check port 6443 inter 5000 fall 5</span><br><span class="line">        server m3 192.168.77.213:6443 check port 6443 inter 5000 fall 5</span><br></pre></td></tr></table></figure>
<p>所有 master 节点启动 haproxy，并设置 开机自启动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node211 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node212 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl start haproxy </span></span><br><span class="line">[root@node213 ~]<span class="comment"># systemctl enable haproxy</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.</span><br></pre></td></tr></table></figure>
<h3 id="4-编写-kubeadm-配置文件"><a href="#4-编写-kubeadm-配置文件" class="headerlink" title="4. 编写 kubeadm 配置文件"></a>4. 编写 kubeadm 配置文件</h3><p>在 node211 节点编写 kubeadm 配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># cat kubeadm-init.yaml</span></span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">"192.168.77.219:8443"</span></span><br><span class="line">dns:</span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:</span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">kubernetesVersion: v1.14.3</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  podSubnet: <span class="string">"10.123.0.0/16"</span></span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: <span class="string">"ipvs"</span></span><br></pre></td></tr></table></figure>
<h3 id="5-初始化"><a href="#5-初始化" class="headerlink" title="5. 初始化"></a>5. 初始化</h3><p>在 node211 节点执行初始化操作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubeadm init --config=kubeadm-init.yaml --experimental-upload-certs</span></span><br><span class="line">[init] Using Kubernetes version: v1.14.3</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node211"</span> could not be reached</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node211"</span>: lookup node211 on 192.168.64.215:53: no such host</span><br><span class="line">	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node211 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.211 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/ca"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node211 localhost] and IPs [192.168.77.211 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"sa"</span> key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"kubelet.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[etcd] Creating static Pod manifest <span class="keyword">for</span> <span class="built_in">local</span> etcd <span class="keyword">in</span> <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[<span class="built_in">wait</span>-control-plane] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="string">"/etc/kubernetes/manifests"</span>. This can take up to 4m0s</span><br><span class="line">[kubelet-check] Initial timeout of 40s passed.</span><br><span class="line">[apiclient] All control plane components are healthy after 107.014141 seconds</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap <span class="string">"kubelet-config-1.14"</span> <span class="keyword">in</span> namespace kube-system with the configuration <span class="keyword">for</span> the kubelets <span class="keyword">in</span> the cluster</span><br><span class="line">[upload-certs] Storing the certificates <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node211 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: ptuvy5.hl4rzxugpxpgkgkh</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstrap-token] creating the <span class="string">"cluster-info"</span> ConfigMap <span class="keyword">in</span> the <span class="string">"kube-public"</span> namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following <span class="built_in">command</span> on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">    --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted <span class="keyword">in</span> two hours; If necessary, you can use </span><br><span class="line"><span class="string">"kubeadm init phase upload-certs --experimental-upload-certs"</span> to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4</span><br></pre></td></tr></table></figure>
<p>按照说明，拷贝 kubectl 配置文件并验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS     ROLES    AGE   VERSION</span><br><span class="line">node211   NotReady   master   82s   v1.14.3</span><br></pre></td></tr></table></figure>
<h3 id="6-部署-flannel-网络插件"><a href="#6-部署-flannel-网络插件" class="headerlink" title="6. 部署 flannel 网络插件"></a>6. 部署 flannel 网络插件</h3><p>在 node211 节点部署 flannel 插件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure>
<p>查看部署状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            0/1     Pending   0          3m17s</span><br><span class="line">coredns-d5947d4b-zdptx            0/1     Pending   0          3m17s</span><br><span class="line">etcd-node211                      1/1     Running   0          2m48s</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          2m28s</span><br><span class="line">kube-controller-manager-node211   1/1     Running   0          2m59s</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          36s</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          3m16s</span><br><span class="line">kube-scheduler-node211            1/1     Running   0          2m41s</span><br></pre></td></tr></table></figure>
<h3 id="7-添加其他-master-节点"><a href="#7-添加其他-master-节点" class="headerlink" title="7. 添加其他 master 节点"></a>7. 添加其他 master 节点</h3><p>按照 node211 初始化提示，在 node212 节点及 node213 节点添加到集群，角色为 master：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node212"</span> could not be reached</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node212"</span>: lookup node212 on 192.168.64.215:53: no such host</span><br><span class="line">	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_sh ip_vs_rr ip_vs_wrr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node212 localhost] and IPs [192.168.77.212 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node212 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.212 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node212 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure>
<p>按照说明，拷贝 kubectl 配置文件并验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node212 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node212 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   7m48s   v1.14.3</span><br><span class="line">node212   Ready    master   66s     v1.14.3</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 \</span><br><span class="line">&gt;     --experimental-control-plane --certificate-key 1436c652cc6bb7e91386b4f744e3376df7b3b6ffd5e9a1a2930f9b6241daac4a</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node213"</span> could not be reached</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node213"</span>: lookup node213 on 192.168.64.215:53: no such host</span><br><span class="line">	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[preflight] Running pre-flight checks before initializing the new control plane instance</span><br><span class="line">[preflight] Pulling images required <span class="keyword">for</span> setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action <span class="keyword">in</span> beforehand using <span class="string">'kubeadm config images pull'</span></span><br><span class="line">[download-certs] Downloading the certificates <span class="keyword">in</span> Secret <span class="string">"kubeadm-certs"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[certs] Using certificateDir folder <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Generating <span class="string">"front-proxy-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"etcd/server"</span> certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/peer"</span> certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed <span class="keyword">for</span> DNS names [node213 localhost] and IPs [192.168.77.213 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating <span class="string">"etcd/healthcheck-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver-etcd-client"</span> certificate and key</span><br><span class="line">[certs] Generating <span class="string">"apiserver"</span> certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed <span class="keyword">for</span> DNS names [node213 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.77.213 192.168.77.219]</span><br><span class="line">[certs] Generating <span class="string">"apiserver-kubelet-client"</span> certificate and key</span><br><span class="line">[certs] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[certs] Using the existing <span class="string">"sa"</span> key</span><br><span class="line">[kubeconfig] Generating kubeconfig files</span><br><span class="line">[kubeconfig] Using kubeconfig folder <span class="string">"/etc/kubernetes"</span></span><br><span class="line">[endpoint] WARNING: port specified <span class="keyword">in</span> controlPlaneEndpoint overrides bindPort <span class="keyword">in</span> the controlplane address</span><br><span class="line">[kubeconfig] Writing <span class="string">"admin.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"controller-manager.conf"</span> kubeconfig file</span><br><span class="line">[kubeconfig] Writing <span class="string">"scheduler.conf"</span> kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-apiserver"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-controller-manager"</span></span><br><span class="line">[control-plane] Creating static Pod manifest <span class="keyword">for</span> <span class="string">"kube-scheduler"</span></span><br><span class="line">[check-etcd] Checking that the etcd cluster is healthy</span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line">[etcd] Announced new etcd member joining to the existing etcd cluster</span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd member to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[etcd] Waiting <span class="keyword">for</span> the new etcd member to join the cluster. This can take up to 40s</span><br><span class="line">[upload-config] storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the label <span class="string">"node-role.kubernetes.io/master=''"</span></span><br><span class="line">[mark-control-plane] Marking the node node213 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line"></span><br><span class="line">This node has joined the cluster and a new control plane instance was created:</span><br><span class="line"></span><br><span class="line">* Certificate signing request was sent to apiserver and approval was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line">* Control plane (master) label and taint were applied to the new node.</span><br><span class="line">* The Kubernetes control plane instances scaled up.</span><br><span class="line">* A new etcd member was added to the <span class="built_in">local</span>/stacked etcd cluster.</span><br><span class="line"></span><br><span class="line">To start administering your cluster from this node, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">	mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">	sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">	sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> to see this node join the cluster.</span><br></pre></td></tr></table></figure>
<p>按照说明，拷贝 kubectl 配置文件并验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node213 ~]<span class="comment"># mkdir -p $HOME/.kube</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span><br><span class="line">[root@node213 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   11m     v1.14.3</span><br><span class="line">node212   Ready    master   4m39s   v1.14.3</span><br><span class="line">node213   Ready    master   72s     v1.14.3</span><br></pre></td></tr></table></figure>
<h3 id="8-添加其他-node-节点"><a href="#8-添加其他-node-节点" class="headerlink" title="8. 添加其他 node 节点"></a>8. 添加其他 node 节点</h3><p>按照 node211 初始化提示，添加 node214 节点到集群，角色为 node：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@node214 ~]<span class="comment"># kubeadm join 192.168.77.219:8443 --token ptuvy5.hl4rzxugpxpgkgkh \</span></span><br><span class="line">&gt;     --discovery-token-ca-cert-hash sha256:2a190612b1e3a68a549d02b42335bfba4dd4af3bb1361124ad46862e1ab418d4 </span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node214"</span> could not be reached</span><br><span class="line">	[WARNING Hostname]: hostname <span class="string">"node214"</span>: lookup node214 on 192.168.64.215:53: no such host</span><br><span class="line">	[WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="string">'systemctl enable kubelet.service'</span></span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with <span class="string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span></span><br><span class="line">	[WARNING RequiredIPVSKernelModulesAvailable]: </span><br><span class="line"></span><br><span class="line">The IPVS proxier may not be used because the following required kernel modules are not loaded: [ip_vs_wrr ip_vs_sh ip_vs_rr]</span><br><span class="line">or no <span class="built_in">builtin</span> kernel IPVS support was found: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_sh:&#123;&#125; ip_vs_wrr:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;].</span><br><span class="line">However, these modules may be loaded automatically by kube-proxy <span class="keyword">if</span> they are available on your system.</span><br><span class="line">To verify IPVS support:</span><br><span class="line"></span><br><span class="line">   Run <span class="string">"lsmod | grep 'ip_vs|nf_conntrack'"</span> and verify each of the above modules are listed.</span><br><span class="line"></span><br><span class="line">If they are not listed, you can use the following methods to load them:</span><br><span class="line"></span><br><span class="line">1. For each missing module run <span class="string">'modprobe $modulename'</span> (e.g., <span class="string">'modprobe ip_vs'</span>, <span class="string">'modprobe ip_vs_rr'</span>, ...)</span><br><span class="line">2. If <span class="string">'modprobe $modulename'</span> returns an error, you will need to install the missing module support <span class="keyword">for</span> your kernel.</span><br><span class="line"></span><br><span class="line">[kubelet-start] Downloading configuration <span class="keyword">for</span> the kubelet from the <span class="string">"kubelet-config-1.14"</span> ConfigMap <span class="keyword">in</span> the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file <span class="string">"/var/lib/kubelet/config.yaml"</span></span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file <span class="string">"/var/lib/kubelet/kubeadm-flags.env"</span></span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[kubelet-start] Waiting <span class="keyword">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run <span class="string">'kubectl get nodes'</span> on the control-plane to see this node join the cluster.</span><br></pre></td></tr></table></figure>
<p>至此 kubeadm 配合 keepalived &amp; haproxy 搭建高可用集群就完成了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@node211 ~]<span class="comment"># kubectl get node</span></span><br><span class="line">NAME      STATUS   ROLES    AGE     VERSION</span><br><span class="line">node211   Ready    master   4h10m   v1.14.3</span><br><span class="line">node212   Ready    master   4h3m    v1.14.3</span><br><span class="line">node213   Ready    master   4h      v1.14.3</span><br><span class="line">node214   Ready    &lt;none&gt;   3h57m   v1.14.3</span><br><span class="line">[root@node211 ~]<span class="comment"># kubectl get pod -n kube-system</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-d5947d4b-rn2wl            1/1     Running   0          4h10m</span><br><span class="line">coredns-d5947d4b-zdptx            1/1     Running   0          4h10m</span><br><span class="line">etcd-node211                      1/1     Running   0          4h9m</span><br><span class="line">etcd-node212                      1/1     Running   0          4h3m</span><br><span class="line">etcd-node213                      1/1     Running   0          4h</span><br><span class="line">kube-apiserver-node211            1/1     Running   0          4h9m</span><br><span class="line">kube-apiserver-node212            1/1     Running   1          4h3m</span><br><span class="line">kube-apiserver-node213            1/1     Running   0          3h59m</span><br><span class="line">kube-controller-manager-node211   1/1     Running   1          4h9m</span><br><span class="line">kube-controller-manager-node212   1/1     Running   0          4h2m</span><br><span class="line">kube-controller-manager-node213   1/1     Running   0          3h59m</span><br><span class="line">kube-flannel-ds-amd64-gchpj       1/1     Running   0          4h</span><br><span class="line">kube-flannel-ds-amd64-mx44p       1/1     Running   0          3h57m</span><br><span class="line">kube-flannel-ds-amd64-vzk7c       1/1     Running   0          4h7m</span><br><span class="line">kube-flannel-ds-amd64-x9rm7       1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-fj448                  1/1     Running   0          4h</span><br><span class="line">kube-proxy-jmhm7                  1/1     Running   0          4h3m</span><br><span class="line">kube-proxy-s7jdf                  1/1     Running   0          3h57m</span><br><span class="line">kube-proxy-w5gsg                  1/1     Running   0          4h10m</span><br><span class="line">kube-scheduler-node211            1/1     Running   1          4h9m</span><br><span class="line">kube-scheduler-node212            1/1     Running   0          4h2m</span><br><span class="line">kube-scheduler-node213            1/1     Running   0          3h59m</span><br></pre></td></tr></table></figure>
<h3 id="HA-机制"><a href="#HA-机制" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>由集群节点上运行的 keepalived &amp; haproxy 提供 VIP &amp; LB，集群中所有节点的 kubelet 连接至 VIP:<haproxy port> EndPoints。</haproxy></p>
<p>当 VIP 所在节点发生故障，VIP 切换到集群中其他 master 节点，即可正常提供服务。</p>
<h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><ol>
<li>kubeadm 需要正常网络支持，需要确保自己处于正常网络环境下；</li>
<li>kubeadm 在添加节点时，有可能会 hang 住，未查明原因；</li>
<li>kubeadm 默认生成证书有效期为 1年，若想要修改，则需要手动生成证书替换；</li>
<li>…</li>
</ol>
<h2 id="kubespray"><a href="#kubespray" class="headerlink" title="kubespray"></a>kubespray</h2><p>因为 kubespray 项目主要使用 ansible 配合 kubeadm 部署，具体内容可以直接查看 github 文档，因此不详细记录具体步骤。</p>
<h3 id="环境信息-1"><a href="#环境信息-1" class="headerlink" title="环境信息"></a>环境信息</h3><table>
<thead>
<tr>
<th>ip</th>
<th>role</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.77.201</td>
<td>master</td>
</tr>
<tr>
<td>192.168.77.202</td>
<td>master</td>
</tr>
<tr>
<td>192.168.77.203</td>
<td>master </td>
</tr>
<tr>
<td>192.168.77.204</td>
<td>node</td>
</tr>
</tbody>
</table>
<h3 id="1-安装-kubespray"><a href="#1-安装-kubespray" class="headerlink" title="1. 安装 kubespray"></a>1. 安装 kubespray</h3><p>在 GitHub <a href="https://github.com/kubernetes-sigs/kubespray/releases" target="_blank" rel="noopener">项目链接</a>上下载最新 Release 版本代码。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 ~]<span class="comment"># wget https://github.com/kubernetes-sigs/kubespray/archive/v2.10.3.tar.gz</span></span><br></pre></td></tr></table></figure>
<h3 id="2-安装必要依赖"><a href="#2-安装必要依赖" class="headerlink" title="2. 安装必要依赖"></a>2. 安装必要依赖</h3><p>项目依赖于 Python3，所以这里采用 Python3.6 版本进行安装。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># yum install python36-pip</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># pip3 install -r requirements.txt</span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>生成 ansible inventory</li>
</ol>
<p>项目默认提供了一个 Python 脚本用于自动生成 inventory，该脚本生成 inventory 通常需要根据实际情况自己调整。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cp -rfp inventory/sample inventory/mycluster</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># declare -a IPS=(192.168.77.201 192.168.77.202 192.168.77.203 192.168.77.203)</span></span><br><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py $&#123;IPS[@]&#125;</span></span><br><span class="line">DEBUG: Adding group all</span><br><span class="line">DEBUG: Adding group kube-master</span><br><span class="line">DEBUG: Adding group kube-node</span><br><span class="line">DEBUG: Adding group etcd</span><br><span class="line">DEBUG: Adding group k8s-cluster</span><br><span class="line">DEBUG: Adding group calico-rr</span><br><span class="line">DEBUG: Skipping existing host 192.168.77.203.</span><br><span class="line">DEBUG: adding host node1 to group all</span><br><span class="line">DEBUG: adding host node2 to group all</span><br><span class="line">DEBUG: adding host node3 to group all</span><br><span class="line">DEBUG: adding host node1 to group etcd</span><br><span class="line">DEBUG: adding host node2 to group etcd</span><br><span class="line">DEBUG: adding host node3 to group etcd</span><br><span class="line">DEBUG: adding host node1 to group kube-master</span><br><span class="line">DEBUG: adding host node2 to group kube-master</span><br><span class="line">DEBUG: adding host node1 to group kube-node</span><br><span class="line">DEBUG: adding host node2 to group kube-node</span><br><span class="line">DEBUG: adding host node3 to group kube-node</span><br></pre></td></tr></table></figure>
<p>查看生成 inventory 结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@node201 kubespray-2.10.3]<span class="comment"># cat inventory/mycluster/hosts.yml </span></span><br><span class="line">all:</span><br><span class="line">  hosts:</span><br><span class="line">    node1:</span><br><span class="line">      ansible_host: 192.168.77.201</span><br><span class="line">      ip: 192.168.77.201</span><br><span class="line">      access_ip: 192.168.77.201</span><br><span class="line">    node2:</span><br><span class="line">      ansible_host: 192.168.77.202</span><br><span class="line">      ip: 192.168.77.202</span><br><span class="line">      access_ip: 192.168.77.202</span><br><span class="line">    node3:</span><br><span class="line">      ansible_host: 192.168.77.203</span><br><span class="line">      ip: 192.168.77.203</span><br><span class="line">      access_ip: 192.168.77.203</span><br><span class="line">  children:</span><br><span class="line">    kube-master:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">    kube-node:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    etcd:</span><br><span class="line">      hosts:</span><br><span class="line">        node1:</span><br><span class="line">        node2:</span><br><span class="line">        node3:</span><br><span class="line">    k8s-cluster:</span><br><span class="line">      children:</span><br><span class="line">        kube-master:</span><br><span class="line">        kube-node:</span><br><span class="line">    calico-rr:</span><br><span class="line">      hosts: &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到跟我们计划中的有所差别，根据实际情况调整 kube-master 数量即可。</p>
<h3 id="4-编写部署配置参数"><a href="#4-编写部署配置参数" class="headerlink" title="4. 编写部署配置参数"></a>4. 编写部署配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/all/all.yml</code> 路径下包含了一些全局配置，比如 proxy 之类的，可以手动调整。</p>
<h3 id="5-编写-k8s-配置参数"><a href="#5-编写-k8s-配置参数" class="headerlink" title="5. 编写 k8s 配置参数"></a>5. 编写 k8s 配置参数</h3><p>在 <code>[root@node201 kubespray-2.10.3]# ls inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml</code> 路径下包含了 k8s 所有配置项，根据实际情况编辑修改。</p>
<h3 id="6-部署"><a href="#6-部署" class="headerlink" title="6. 部署"></a>6. 部署</h3><p>在所有准备工作完成后，执行部署操作。</p>
<p>注意， Kubespray 部署的前提条件是你的网络是一个正常的网络，可以正常访问所有网站，若无法访问，则根据自身实际情况，调整配置，配置路径为： <code>roles/download/defaults/main.yml</code> 。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml</span><br></pre></td></tr></table></figure>
<p>等待部署完成即可。</p>
<h3 id="HA-机制-1"><a href="#HA-机制-1" class="headerlink" title="HA 机制"></a>HA 机制</h3><p>集群中所有的 Node 节点自己启动一个 Nginx Static Pod，用于代理转发，将所有指定 <code>127.0.0.1:6443</code> 的请求转发至所有 master 节点真实 apiserver ，这样所有的 kubelet 只需要自己节点即可，无需其他节点参与。</p>
<h3 id="坑-1"><a href="#坑-1" class="headerlink" title="坑"></a>坑</h3><ol>
<li>CentOS 默认 Python2.7，需要单独安装 Python3.6</li>
<li>通过 pip 安装依赖，部分软件包需要 gcc,python36-devel,openssl-devel 等依赖包，需要根据错误提示自行安装，文档中没有提到</li>
<li>默认会安装 docker &amp; containerd 服务，但是 containerd 服务未设置开机自启动，会导致 docker 无法自动运行</li>
<li>在安装过程中，会安装 selinux 相应 Python 库，但是该依赖未在 <code>requirements.txt</code> 声明</li>
<li>…</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>无论是直接只用 kubeadm + vip 方式部署 HA 集群，还是通过 Kubespray 部署，在网络正常情况下，是很快可以完成的。</p>
<p>在使用 kubeadm 过程中，因为无需引入第三方依赖库，导致整体流程顺畅，体验极佳。</p>
<p>在 Kubespray 过程中，因为采用 Python3 方式，但相关依赖又未显示声明，导致部署过程繁琐。但是也比较好理解，Kubespray 作为一个致力于部署企业级 k8s 集群的项目，需要处理大量的边界条件了，这个项目中 YAML 就写了 15k 行，可见一斑。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/06/05/查看磁盘扇区大小/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/05/查看磁盘扇区大小/" itemprop="url">如何查看磁盘扇区大小</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-05T20:45:06+08:00">
                2019-06-05
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/05/查看磁盘扇区大小/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/05/查看磁盘扇区大小/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="磁盘扇区"><a href="#磁盘扇区" class="headerlink" title="磁盘扇区"></a>磁盘扇区</h2><p>引用维基百科：</p>
<blockquote>
<p>In computer disk storage, a sector is a subdivision of a track on a magnetic disk or optical disc. Each sector stores a fixed amount of user-accessible data, traditionally 512 bytes for hard disk drives (HDDs) and 2048 bytes for CD-ROMs and DVD-ROMs. Newer HDDs use 4096-byte (4 KiB) sectors, which are known as the Advanced Format (AF).</p>
</blockquote>
<p>扇区大小常见的可以分为 512 bytes, 2048 bytes 和 4096 bytes。</p>
<h2 id="查看扇区大小"><a href="#查看扇区大小" class="headerlink" title="查看扇区大小"></a>查看扇区大小</h2><p>通过 lsblk 命令可以查看。</p>
<p>物理扇区 512 byte：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:51:59 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdg</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdg           512     512</span><br><span class="line">├─sdg1        512     512</span><br><span class="line">├─sdg2        512     512</span><br><span class="line">├─sdg3        512     512</span><br><span class="line">└─sdg4        512     512</span><br></pre></td></tr></table></figure>
<p>物理扇区 4096 byte：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 20:52:03 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sdb</span><br><span class="line">NAME      PHY-SEC LOG-SEC</span><br><span class="line">sdb          4096     512</span><br><span class="line">├─sdb1       4096     512</span><br><span class="line">├─sdb2       4096     512</span><br><span class="line">├─sdb3       4096     512</span><br><span class="line">└─sdb4       4096     512</span><br></pre></td></tr></table></figure>
<p>CDROM 比较特殊，是 2048 byte：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yiran 21:07:35 ~]<span class="variable">$lsblk</span> -o NAME,PHY-SEC,LOG-SEC /dev/sr0</span><br><span class="line">NAME PHY-SEC LOG-SEC</span><br><span class="line">sr0     2048    2048</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/05/31/Kubernetes-实战-集群部署/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/31/Kubernetes-实战-集群部署/" itemprop="url">Kubernetes 实战-集群部署</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-31T21:22:51+08:00">
                2019-05-31
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/31/Kubernetes-实战-集群部署/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/31/Kubernetes-实战-集群部署/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。</p>
<p>说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。</p>
<p>本文并不会介绍具体的部署步骤，望周知。</p>
<h2 id="Kubernetes-主要组件"><a href="#Kubernetes-主要组件" class="headerlink" title="Kubernetes 主要组件"></a>Kubernetes 主要组件</h2><p>因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：</p>
<h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><ul>
<li><p>apiserver： 集群中所有其他组件通过 apiserver 进行交互</p>
</li>
<li><p>scheduler： 按照 Pod 配置来对 Pod 进行节点调度</p>
</li>
<li><p>controller-manager：负责节点管理，资源的具体创建动作， <code>desired state management</code> 具体实行者</p>
</li>
<li><p>etcd：用于存储集群中数据的键值存储</p>
</li>
</ul>
<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><ul>
<li><p>kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器</p>
</li>
<li><p>kube-proxy：负责维护 node 的网络规则，还负责处理 Pod,Node和外部之间的通信</p>
</li>
<li><p>容器运行时：在节点上运行容器的具体实现，常见的有 Docker/rkt/CRI-O</p>
</li>
</ul>
<h2 id="Kubernetes-集群准备"><a href="#Kubernetes-集群准备" class="headerlink" title="Kubernetes 集群准备"></a>Kubernetes 集群准备</h2><h3 id="所需资源"><a href="#所需资源" class="headerlink" title="所需资源"></a>所需资源</h3><p>大部分的安装文档中，都会先写明 os 要求，计算 &amp; 存储资源需求，k8s 自身对资源消耗很低，通常的 2c4g + 30GiB 足够运行起来。</p>
<p>上面说完了硬件资源，那么我们来说下软件资源， k8s 作为一个容器编排系统，它需要的软件资源也是很重要的一部分，这里我们来说一下网络部分。</p>
<p>假设我们集群中存在 5 个节点，使用 <code>kubeadm init</code> 方式部署集群，那么最基本的，需要 5 个节点的 IP。那如果是高可用的集群呢？我们需要加一个 VIP，也就是 6 个 IP 地址。</p>
<p>在考虑了 IP 地址之后，我们来说下网段划分，以 flannel 为例，在创建网络时，每个 k8s 节点都会分配一段子网用于 Pod 分配 IP，这里的网段是不可以跟宿主机的网络重叠的，所以这里的网络划分也是一个很重要的资源。</p>
<p>具体其他网络类型，我会在之后的网络部分详细的写一下。</p>
<h3 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h3><p>好了，现在我们已经有了资源了（无论是硬件资源还是软件资源），那么我们可以部署了，那此时采用什么方式部署，或者说怎么部署成了问题。</p>
<hr>
<p>首先说下最特殊的服务，kubelet，它作为节点的实际管控者，它如果运行在容器中，那么谁来控制kubelet 容器的启停呢？当节点故障恢复后，又如何自启动呢？最开始我使用 Rancher 部署的时候发现他们是将 kubelet 直接部署在容器中的，那是因为他们在节点上还有其他 Agent 用于管理节点，Rancher 相当于 k8s 集群之外的上帝，管控着一切。</p>
<p>当我们没有 Rancher 这类管理工具时，还是老老实实将 kubelet 以服务的形式部署在宿主机上吧。</p>
<hr>
<p>官方推荐使用 <code>kubeadm</code> 进行集群部署，简单快捷，只是还在快速迭代中，存在较多不确定性，那么现在那些大厂是如何部署的呢？</p>
<p>我花了点时间阅读了下 Github 上面一些关于 k8s 部署项目，简单的罗列一下：</p>
<table>
<thead>
<tr>
<th>项目名称</th>
<th>项目地址</th>
<th>星</th>
<th>服务运行方式</th>
<th>ha </th>
</tr>
</thead>
<tbody>
<tr>
<td>ansible-kubeadm</td>
<td><a href="https://github.com/4admin2root/ansible-kubeadm" target="_blank" rel="noopener">https://github.com/4admin2root/ansible-kubeadm</a></td>
<td>3</td>
<td>Static Pod</td>
<td>- </td>
</tr>
<tr>
<td>ansible-kubeadm-ha-cluster</td>
<td><a href="https://github.com/sv01a/ansible-kubeadm-ha-cluster" target="_blank" rel="noopener">https://github.com/sv01a/ansible-kubeadm-ha-cluster</a></td>
<td>5</td>
<td>Docker</td>
<td>keepvalied </td>
</tr>
<tr>
<td>kubeadm-playbook</td>
<td><a href="https://github.com/ReSearchITEng/kubeadm-playbook" target="_blank" rel="noopener">https://github.com/ReSearchITEng/kubeadm-playbook</a></td>
<td>117</td>
<td>Static Pod</td>
<td>keepalived </td>
</tr>
<tr>
<td>Kubernetes-ansible</td>
<td><a href="https://github.com/zhangguanzhang/Kubernetes-ansible" target="_blank" rel="noopener">https://github.com/zhangguanzhang/Kubernetes-ansible</a></td>
<td>208</td>
<td>Service</td>
<td>keepalived &amp; Haproxy</td>
</tr>
<tr>
<td>kubeadm-ansible</td>
<td><a href="https://github.com/kairen/kubeadm-ansible" target="_blank" rel="noopener">https://github.com/kairen/kubeadm-ansible</a></td>
<td>281</td>
<td>Static Pod</td>
<td>- </td>
</tr>
<tr>
<td>kubeadm-ha</td>
<td><a href="https://github.com/cookeem/kubeadm-ha" target="_blank" rel="noopener">https://github.com/cookeem/kubeadm-ha</a></td>
<td>502</td>
<td>Service</td>
<td>keepalived &amp; nginx </td>
</tr>
<tr>
<td>kubeasz</td>
<td><a href="https://github.com/easzlab/kubeasz" target="_blank" rel="noopener">https://github.com/easzlab/kubeasz</a></td>
<td>2987</td>
<td>Service</td>
<td>keepalived &amp; Haproxy </td>
</tr>
</tbody>
</table>
<p>可以看到虽然有些细微的差别，但是大家做的都围绕着一个目的，就是把上一节提到的 k8s 所有必要组件部署到集群中，我们根据服务运行方式和 HA 方式来说一下。</p>
<h2 id="服务运行方式"><a href="#服务运行方式" class="headerlink" title="服务运行方式"></a>服务运行方式</h2><p>根据我上面总结的各个项目，大体分为 3 类，分别是：Host Service、Docker、Static Pod。我们一个一个的过一下。</p>
<h3 id="Host-Service"><a href="#Host-Service" class="headerlink" title="Host Service"></a>Host Service</h3><p>在没有容器的时代，我们要部署一个服务，都是采用 Host Service 方式，我们在宿主机的 OS 上配置一个服务，管理方式可能是 init.d ，也可能是 systemd 。k8s 所有的服务都可以通过 Host Service 方式运行。</p>
<p>优点：</p>
<p>在 systemd 大法加持下，所有服务均可通过 systemd 统一管理，可以配置随着系统进行启停。</p>
<p>缺点：</p>
<p>如果通过 systemd 方式部署，那么服务配置修改、服务升级是一个大麻烦。</p>
<h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>既然我们觉得 Host Service 的方式服务配置修改、升级都比较麻烦，那我们直接通过 Docker 启动就好了，升级直接更新 image 重新启动，一切问题放佛都解决了。</p>
<p>但是考虑一个问题，当集群全部掉电，系统开机后，k8s 如何自启动？以 Docker 作为容器运行时的基础上，Docker 比较让人诟病的一点就是它是一个 Daemon，在这里反而是优点，我们可以设置 Docker 随开机自启动，并将 k8s 所有服务对应镜像设置为 <code>--restart=always</code>，就可以解决这个问题。</p>
<p>优点：</p>
<p>服务配置、升级方便。</p>
<p>缺点：</p>
<p>依赖于 Docker，若更换其他 CRI，无法处理极端情况。</p>
<h3 id="Static-Pod"><a href="#Static-Pod" class="headerlink" title="Static Pod"></a>Static Pod</h3><p>最后我们来说说 Static Pod，这种方式是 kubeadm 目前所采用的方式，也是 k8s 所推荐的方式。</p>
<p>什么是 Static Pod？其实就是字面意义， <code>静态 Pod</code>，k8s 不去调度的 Pod，而是由 kubelet 直接管理。前面我们在讨论 kubelet 提到，kubelet 是以服务的形式运行在宿主机上的，那么也就是 kubelet 的启停是通过 systemd 控制的，不受 k8s 控制。</p>
<p>Static Pod 由 kubelet 控制，当 kubelet 启动后，会自动拉起 Static Pod，且 Static Pod 不受 k8s 控制，无论是通过 kubectl 进行删除，还是通过 docker 对该 Pod 进行 stop 动作，kubelet 都会保证 Static Pod 正常运行。</p>
<p>这个方式很适合我们来处理 k8s 自身的服务，比如 apiserver/scheduler ，每个 master 节点上通过 Static Pod 配置，当 kubelet 启动后，自动拉起 apiserver/scheduler 等服务，那么 k8s 集群也就自启动了，也就解决了我们上面提到的集群全部掉电的情况。</p>
<p>优点：</p>
<p>跟随 kubelet 启停，完美适配 k8s 升级场景。</p>
<p>缺点：</p>
<p>因为随着 kubelet 启停，所以导致更新 kubelet 配置时需要指定 manifest 文件路径。</p>
<h2 id="HA-配置"><a href="#HA-配置" class="headerlink" title="HA 配置"></a>HA 配置</h2><p>k8s 作为一个基础架构服务，它的可用性关乎着我们整个业务的稳定，所以一定要保证不会出现单点故障。</p>
<p>在公有云场景下，由公有云提供 LB 的支持，只要我们部署了多个 master 节点，就不用担心了。</p>
<p>那在裸机场景下怎么办呢？</p>
<p>目前通用解决方案是 VIP 配合 LB（也就是 keepalived &amp; haproxy/nginx）。</p>
<p> keepalived 提供了 VIP。在 k8s 集群部署过程中，所有节点都指定 <code>controlPlaneEndpoint</code> 为 VIP，而 VIP 在集群中的一个 master 节点上。当 VIP 所在节点故障时， VIP 自动漂浮到集群中其他 master 节点上，保证高可用。</p>
<p> haproxy/nginx 作为 LB，当我们 k8s 集群中所有节点都连接一个 apiserver 时，通过 LB 按照既定策略将请求分发到集群中多个 master 节点上，保证性能。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于 k8s 集群的部署大概就是上述这些内容，具体的操作步骤都可以通过官网或者 github 找到相关资料。目前如果个人学习的话，直接通过 kubeadm 部署就好；如果想要在生产环境中部署，那么需要根据自身业务类型，仔细考虑自己是否需要 HA，如何配置 HA。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zdyxry.github.io/2019/05/26/BIOS-vs-UEFI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yiran">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/yiran.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yiran's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/26/BIOS-vs-UEFI/" itemprop="url">BIOS vs UEFI</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-26T08:46:23+08:00">
                2019-05-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/26/BIOS-vs-UEFI/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/26/BIOS-vs-UEFI/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>大家应该都安装过操作系统，PC 或者服务器上。那么我们在安装操作系统时通常需要进入到 BIOS 或 UEFI 界面去安装。之前维护的一个 ISO 版本只支持 BIOS，最近有了支持 UEFI 安装的需求，今天来了解一下其中的差异，之后尝试编写一个支持 UEFI 的 KickStart 配置。</p>
<h2 id="Legacy-BIOS"><a href="#Legacy-BIOS" class="headerlink" title="Legacy BIOS"></a>Legacy BIOS</h2><p>按照惯例，引用维基百科中（不同语言对于名词解释信息可能是完全不同的，最好直接看英文）的解释：</p>
<blockquote>
<p>BIOS (/ˈbaɪɒs/ BY-oss; an acronym for Basic Input/Output System and also known as the System BIOS, ROM BIOS or PC BIOS) is non-volatile firmware used to perform hardware initialization during the booting process (power-on startup), and to provide runtime services for operating systems and programs.[1] The BIOS firmware comes pre-installed on a personal computer’s system board, and it is the first software to run when powered on. The name originates from the Basic Input/Output System used in the CP/M operating system in 1975.[2][3] The BIOS originally proprietary to the IBM PC has been reverse engineered by companies looking to create compatible systems. The interface of that original system serves as a de facto standard.</p>
</blockquote>
<p>主要功能有：</p>
<ul>
<li>POST - 在加载操作系统之前， 检测硬件确保没有错误</li>
<li>Bootstrap Loader - 寻找引导加载程序，并将控制权转</li>
<li>BIOS 驱动程序 - 低级驱动程序，使计算机可以对计算机硬件进行基本操作控制</li>
<li>BIOS/CMOS 设置 - 允许配置硬件设置，包括系统设置，如计算机密码，硬件时钟等</li>
</ul>
<h3 id="BIOS-vs-CMOS"><a href="#BIOS-vs-CMOS" class="headerlink" title="BIOS vs CMOS"></a>BIOS vs CMOS</h3><p>更改BIOS配置时，设置不会存储在BIOS芯片本身。相反，它们存储在一个特殊的存储芯片上，称为<code>CMOS</code>。</p>
<p>与大多数RAM芯片一样，存储BIOS设置的芯片使用CMOS工艺制造。它包含少量数据，通常为256 个字节。CMOS 芯片上的信息包括计算机上安装的磁盘驱动器类型，系统时钟的当前日期和时间以及计算机的引导顺序。</p>
<p>BIOS是非易失性的：即使计算机没电也会保留其信息，因为即使计算机已关闭，计算机也需要记住其BIOS设置。这就是为什么CMOS有自己的专用电源，即CMOS电池。 通常我们在使用电脑的时候，如果忘记了 BIOS 密码，无法更改 BIOS 设置时， 那么可以通过拔掉 CMOS 电池，再安装即可恢复。</p>
<h2 id="U-EFI"><a href="#U-EFI" class="headerlink" title="(U)EFI"></a>(U)EFI</h2><blockquote>
<p>The Unified Extensible Firmware Interface (UEFI) is a specification that defines a software interface between an operating system and platform firmware. UEFI replaces the Basic Input/Output System (BIOS) firmware interface originally present in all IBM PC-compatible personal computers,[1][2] with most UEFI firmware implementations providing legacy support for BIOS services. UEFI can support remote diagnostics and repair of computers, even with no operating system installed.[3]</p>
</blockquote>
<p>Intel 为了解决 BIOS 的一些缺点，提出了 EFI ，后来由于各种历史原因，EFI 转变为了 UEFI，其中的 U 是 <code>Unified</code> 。</p>
<p>那么 BIOS 有啥缺点呢？ 对于服务器级别来说，最大的缺点可能就是不支持 2TiB 以上空间的磁盘引导，关于为什么不支持大家可以自己查阅下 MBR vs GPT 相关资料，这里不详细解释。</p>
<p>那么老大哥提出了 UEFI，除了解决了引导磁盘的容量限制，还有如下优点（这几点看上去就跟普通用户没啥关系）：</p>
<ul>
<li>独立于CPU的架构</li>
<li>独立于CPU的驱动程序</li>
<li>灵活的pre-OS环境，包括网络功能</li>
<li>模块化设计</li>
<li>向后和向前兼容性</li>
</ul>
<h2 id="Linux-安装识别"><a href="#Linux-安装识别" class="headerlink" title="Linux 安装识别"></a>Linux 安装识别</h2><p>了解了大概的概念，那么我们来实际看看 Linux 分别从 BIOS 启动和 UEFI 启动安装有什么不同吧，接下来示例以 CentOS 为例。</p>
<p>首先我们看下 CentOS ISO 中的目录结构：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@dell-r720xd-1 CentOS-7.4]<span class="comment"># tree . -d 1</span></span><br><span class="line">.</span><br><span class="line">├── EFI <span class="comment"># EFI 模式下引导程序路径</span></span><br><span class="line">│   └── BOOT</span><br><span class="line">│       └── fonts</span><br><span class="line">├── images <span class="comment"># 系统启动镜像</span></span><br><span class="line">│   └── pxeboot</span><br><span class="line">├── isolinux   <span class="comment"># 默认引导程序路径，包括引导选项配置，引导背景图片，引导内核镜像等</span></span><br><span class="line">├── LiveOS    <span class="comment"># 临时加载镜像</span></span><br><span class="line">├── Packages   <span class="comment"># ISO 附带所有软件包，以 RPM 形式存放</span></span><br><span class="line">└── repodata    <span class="comment"># ISO 中 YUM Repo 配置文件，保存了在只做 YUM Repo 时指定的软件组，支持语言等信息</span></span><br><span class="line">1 [error opening dir]</span><br><span class="line"></span><br><span class="line">9 directories</span><br></pre></td></tr></table></figure>
<h3 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h3><p>我们来看下在 BIOS 下如何指定安装 KickStart 配置：</p>
<p>在 <code>isolinux/isolinux.cfg</code> 中指定即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label linux</span><br><span class="line">  menu label ^Install yiran&apos;OS</span><br><span class="line">  menu default</span><br><span class="line">  kernel vmlinuz</span><br><span class="line">  append initrd=initrd.img inst.stage2=hd:LABEL=YIRANOS-2 ks=hd:LABEL=YIRANOS-2:/ks_yiranos.cfg quiet</span><br></pre></td></tr></table></figure></p>
<h3 id="UEFI"><a href="#UEFI" class="headerlink" title="UEFI"></a>UEFI</h3><p>跟 BIOS 一样，只是换了一个配置位置，只需要在 <code>EFI/BOOT/grub.cfg</code> 中指定 KS 配置即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">### BEGIN /etc/grub.d/10_linux ###</span><br><span class="line">menuentry &apos;Install CentOS 7&apos; --class fedora --class gnu-linux --class gnu --class os &#123;</span><br><span class="line">        linuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 inst.ks=hd:LABEL=YIRANOS-2:/ks_yiranos.cfg quiet</span><br><span class="line">        initrdefi /images/pxeboot/initrd.img</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随着越来越多服务器厂商将 UEFI 设置为默认模式，哪怕我们没有用到 UEFI 的高级功能，也最好将自己的 ISO 支持到 UEFI，避免因为 BIOS 的一些历史遗留问题导致后续技术支持出现困难。后续找时间写一下关于 UEFI 的 KickStart 配置文件。主要变化应该是在 <code>/boot</code> 分区部分有些变化，之后再说啦。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/yiran.png" alt="yiran">
            
              <p class="site-author-name" itemprop="name">yiran</p>
              <p class="site-description motion-element" itemprop="description">Normal is boring</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">71</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zdyxry" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zdyxry@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/zhouyiran1994" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://instagram.com/zdyxry" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://winkidney.com/" title="amao" target="_blank">amao</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://jiajunhuang.com/" title="jiajun" target="_blank">jiajun</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://liuliqiang.info/" title="liqiang" target="_blank">liqiang</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yiran</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://zdyxry.disqus.com/count.js" async></script>
    

    

  

















  





  

  

  

  
  

  

  

  

</body>
</html>

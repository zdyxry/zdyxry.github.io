<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linux on Yiran's Blog</title><link>https://zdyxry.github.io/tags/Linux/</link><description>Recent content in Linux on Yiran's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 28 Aug 2022 13:00:00 +0000</lastBuildDate><atom:link href="https://zdyxry.github.io/tags/Linux/atom.xml" rel="self" type="application/rss+xml"/><item><title>如何平(优）滑（雅）的抛弃 CentOS7</title><link>https://zdyxry.github.io/2022/08/28/%E5%A6%82%E4%BD%95%E5%B9%B3%E4%BC%98%E6%BB%91%E9%9B%85%E7%9A%84%E6%8A%9B%E5%BC%83-CentOS7/</link><pubDate>Sun, 28 Aug 2022 13:00:00 +0000</pubDate><guid>https://zdyxry.github.io/2022/08/28/%E5%A6%82%E4%BD%95%E5%B9%B3%E4%BC%98%E6%BB%91%E9%9B%85%E7%9A%84%E6%8A%9B%E5%BC%83-CentOS7/</guid><description>背景 Link to heading 2023/03/01 更新：目前已正式使用，符合预期。
CentOS 7 自身的生命周期截止到 2024年6月30日。在2020年底，CentOS 社区宣布修改现有的发布模式，将 CentOS 从作为 RHEL 的下游改为 CentOS Stream，即 RHEL 的上游，更导致 CentOS8的生命周期短的可怜，这让社区中原本就对 CentOS 不满的开发者/使用者不满，从而出现了抛弃 CentOS 转投其他发行版的情况。
大家选择使用 CentOS ，虽然都在说稳定，但是我理解更看重的是 RedHat 在身后背书，CentOS 作为 RHEL 的下游，所有的软件版本都是经过 RedHat 测试验证的，且后期维护也是有 RedHat 的身影在，不担心维护的问题。
CentOS 原有的模式也是有问题的，用户很难参与到 RHEL 的研发周期。用户发现了 CentOS 某个版本存在问题，想要给 CentOS 进行贡献，让 CentOS 下一个版本修复该问题。此时只有一条路，就是贡献给开源组件自身，但是这样也只是存在修复的可能，最终是否可能修复还是看 RedHat 开发人员的决定（毕竟 RHEL/CentOS 中存在大量开源组件自身不包含，但是 RHEL/CentOS 通过 rpm spec 中进行 Patch 的方式包含的 Patch）。在引入了 CentOS Stream 之后，用户就可以通过贡献给 CentOS 社区，来保证 CentOS 下一个版本包含该 Patch，至于 RHEL 是否包含，用户并不关心，那是 RedHat 关心的问题。</description></item><item><title>Yum 寻找 Best Package 评分机制</title><link>https://zdyxry.github.io/2022/04/29/Yum-%E5%AF%BB%E6%89%BE-Best-Package-%E8%AF%84%E5%88%86%E6%9C%BA%E5%88%B6/</link><pubDate>Fri, 29 Apr 2022 23:04:16 +0000</pubDate><guid>https://zdyxry.github.io/2022/04/29/Yum-%E5%AF%BB%E6%89%BE-Best-Package-%E8%AF%84%E5%88%86%E6%9C%BA%E5%88%B6/</guid><description>背景 Link to heading 公司产品最终交付形态是 ISO，在涉及一个产品的多个 OEM 场景时，会选择在标准版本的基础上，删除某些软件包，新增某些软件包的形式来减少构建时间。产品的 BaseOS 是 CentOS，包管理器是 RPM 系，也就需要使用 rpm / yum 等命令来实现。 其中新增某些软件包是使用 yumdownloader 来完成的。在 Yum Repository 中会包含同一软件包的多个版本，预期 yumdownloader 会下载 Yum Repository 中某个软件最新版本的包，比如 yumdownloader zbs-5.1.2* ，则会下载 zbs-5.1.2 大版本的最新 release 版本。
但是最近发现，从某个版本开始 yumdownloader 没有下载最新的软件包，反而停在了一个两个月之前构建的版本，于是开始调查原因。
Yumdownloader Link to heading yumdownloader 工具集是由 yum-utils 提供，同时还提供了 repotrack，repoquery, reposync 等有用的工具。yumdownloader 使用方式是 yumdownloader $pkg 即可。在 yum-utils 中会大量引用 yum module，因此需要同时查找两个 Git repo。
yum-utils 代码仓库地址： https://github.com/rpm-software-management/yum-utils/blob/master/yumdownloader.py
yum 代码仓库地址：https://github.com/rpm-software-management/yum
下载逻辑 Link to heading def main(self): # Add command line option specific to yumdownloader self.</description></item><item><title>使用 init 进程运行 Container</title><link>https://zdyxry.github.io/2022/03/05/%E4%BD%BF%E7%94%A8-init-%E8%BF%9B%E7%A8%8B%E8%BF%90%E8%A1%8C-Container/</link><pubDate>Sat, 05 Mar 2022 15:10:39 +0000</pubDate><guid>https://zdyxry.github.io/2022/03/05/%E4%BD%BF%E7%94%A8-init-%E8%BF%9B%E7%A8%8B%E8%BF%90%E8%A1%8C-Container/</guid><description>背景 Link to heading 关注过 Bare Metal 相关项目的同学应该都了解过系统的启动流程、如何快速的置备一台物理服务器等之类的实现方式，通常都需要运行一个 LiveOS 来实现某些动作。 在 Tinkerbell 项目中，使用 Linuxkit 来作为 LiveOS，Plunder 项目中使用 BOOTy 来作为 LiveOS。前几天 @thebsdbox 将 BOOTy 中的一部分抽离了出来，作为 ginit 展示主要的实现方式，可以更好的让我们理解安装环节中的具体细节。 今天来看一下这个项目。
如果安装一个 CentOS，那么通常是会通过 kernel + initramfs.img 启动，initramfs.img 中会包含 systemd 、anaconda、dracut 等一些列组件，然后通过 systemd 指定不同的 Target 所属/依赖/顺序来完成最终 Anaconda 调用。Anaconda 通过解析 /proc/cmdline 中的 KickStart 参数来决定自己的安装方式。
ginit 项目展示了以下内容：
制作 initramfs.img 通过 Container image 制作一个 RAW image 通过 QEMU 使用 RAW image 和 Linux Kernel 来运行一个虚拟机 ginit 自动运行 Container 中 entrypoint 指令 流程演示 Link to heading 通过 Container image 制作一个 RAW image Link to heading RAW image 中最终不会包含 Kernel 部分，以 Nginx Container 为例。提取 nginx:latest image 中的 Entrypoint ，通过 dd 置备一个 RAW image，并格式化为 ext4 ，raw image 作为 loop 设备挂载到本地，通过 docker export 将 Nginx Image 拷贝到挂载点下，卸载挂载点，最终 RAW image 包含了 Nginx Container 的所有内容。这里的 RAW image 因为不包含 kernel，所以无法直接启动，只是作为后续动作的依赖。</description></item><item><title>DHCP lease 生命周期</title><link>https://zdyxry.github.io/2021/12/18/DHCP-lease-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link><pubDate>Sat, 18 Dec 2021 18:42:08 +0000</pubDate><guid>https://zdyxry.github.io/2021/12/18/DHCP-lease-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid><description>背景 Link to heading 昨天配合一个同事排查虚拟机 IP 发生了变化的问题，正好整理一下 DHCP lease 生命周期以及变化流程。
DHCP lease 生命周期 Link to heading Allocation：一个客户端开始时没有有效的租约，因此也没有 DHCP 分配的地址。它通过一个分配过程获得一个租约。 Reallocation：如果一个客户端已经有了一个来自现有租约的地址，那么当它重启或关闭后启动时，它将与授予它租约的 DHCP 服务器联系，以确认租约并获得操作参数。这有时被称为重新分配；它与完全分配过程相似，但时间更短。 Normal Operation：一旦租约被激活，客户端就会正常工作，在租约周期内使用其分配的IP地址和其他参数。客户端被称为与租约和地址绑定。 Renewal：在租约时间的某一部分过期后，客户端将试图联系最初授予租约的服务器，以更新租约，这样它就可以继续使用其 IP 地址。 Rebinding：如果与最初的租约服务器续约失败（例如，因为该服务器已经下线），那么客户端将尝试重新绑定到任何活跃的 DHCP 服务器，试图在任何允许它这样做的服务器上延长其当前租约。 Release：客户端可以在任何时候决定它不再希望使用它被分配的IP地址，并可以终止租约，释放 IP 地址。 Allocation 流程 Link to heading - 1.客户端创建 DHCPDISCOVER 消息 客户端开始处于INIT（初始化）状态。它没有IP地址，甚至不知道网络上是否有 DHCP 服务器或在哪里。为了找到一个，它创建了一个 DHCPDISCOVER 消息，包括以下信息。 - 在消息的 CHAddr 字段中包含自己的硬件地址，用来识别自身。 - 一个随机的交易标识符，放在 XID 字段中，这被用来识别以后的消息是同一事务的一部分。 - 另外，客户可以使用 `Requested IP Address` DHCP 选项请求一个特定的IP地址，使用IP地址 `Lease Time` 选项请求一个特定的租约长度，或通过在报文中加入`Parameter Request List`选项请求特定的配置参数。 - 2.客户端发送 DHCPDISCOVER 消息 客户端在本地网络上广播 DHCPDISCOVER 消息。客户端过渡到 SELECTING 状态，在那里等待对其消息的回复。 - 3.</description></item><item><title>CentOS 执行 grub2-mkconfig 导致磁盘只读</title><link>https://zdyxry.github.io/2021/10/02/CentOS-%E6%89%A7%E8%A1%8C-grub2-mkconfig-%E5%AF%BC%E8%87%B4%E7%A3%81%E7%9B%98%E5%8F%AA%E8%AF%BB/</link><pubDate>Sat, 02 Oct 2021 07:30:09 +0000</pubDate><guid>https://zdyxry.github.io/2021/10/02/CentOS-%E6%89%A7%E8%A1%8C-grub2-mkconfig-%E5%AF%BC%E8%87%B4%E7%A3%81%E7%9B%98%E5%8F%AA%E8%AF%BB/</guid><description>背景 Link to heading 最近遇到了一个故障，在集群软件升级过程中，发现某一个磁盘分区变为只读，导致存储应用识别该磁盘不可用。调查发现集群升级过程中，会重新生成每个节点的 GRUB 配置文件，在执行 grub2-mkconfig 过程中导致的磁盘分区只读。记录下 grub2-mkconfig 命令执行到真正磁盘只读指令下发的流程。
调查 Link to heading grub2-mkconfig Link to heading 在执行 grub2-mkconfig 命令时，如果没有指定配置 GRUB_DISABLE_OS_PROBER=true 时，则 GRUB 会调用 os-prober （/etc/grub.d/30_os-prober）用于扫描其他操作系统进行后续配置。
os-prober Link to heading os-prober 是用来探测其他磁盘中存在操作系统的情况。通常由各个发行版本提供，源码地址：https://salsa.debian.org/installer-team/os-prober 。
CentOS 默认包含的 os-prober 与源码版本并不相同，包含了一些额外的配置，通过 RPM changelog 可以查看：
Name : os-prober Version : 1.58 Release : 9.el7 Architecture: x86_64 Install Date: Wed 18 Aug 2021 03:44:43 PM CST Group : System Environment/Base Size : 97946 License : GPLv2+ and GPL+ Signature : RSA/SHA256, Mon 21 Nov 2016 03:50:19 AM CST, Key ID 24c6a8a7f4a80eb5 Source RPM : os-prober-1.</description></item><item><title>OSTree 背景介绍</title><link>https://zdyxry.github.io/2021/05/22/OSTree-%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D/</link><pubDate>Sat, 22 May 2021 09:00:15 +0000</pubDate><guid>https://zdyxry.github.io/2021/05/22/OSTree-%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D/</guid><description>背景 Link to heading 前段时间对 OSTree 做了一些简单的了解，进行了内部分享，好久没更新博客，把 PPT 整理出来水一篇。
仓库地址： https://github.com/zdyxry/ostree-share/ PPT： https://github.com/zdyxry/ostree-share/blob/master/ostree-intro.md
（Marp 真的好用，用 Markdown 写草稿，然后短时间就可以转换成 PPT</description></item><item><title>cgroups notification API demo</title><link>https://zdyxry.github.io/2021/01/01/cgroups-notification-API-demo/</link><pubDate>Fri, 01 Jan 2021 18:28:00 +0000</pubDate><guid>https://zdyxry.github.io/2021/01/01/cgroups-notification-API-demo/</guid><description>cgroups notification API demo Link to heading package main import ( &amp;#34;flag&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;os&amp;#34; &amp;#34;path/filepath&amp;#34; &amp;#34;golang.org/x/sys/unix&amp;#34; ) func main() { cgName := flag.String(&amp;#34;cgName&amp;#34;, &amp;#34;yiran&amp;#34;, &amp;#34;cgroup path&amp;#34;) flag.Parse() level := &amp;#34;critical&amp;#34; cgDir := filepath.Join(&amp;#34;/sys/fs/cgroup/memory/&amp;#34;, *cgName) evName := &amp;#34;memory.pressure_level&amp;#34; fmt.Printf(&amp;#34;cgroup name is: %s\n&amp;#34;, *cgName) fmt.Printf(&amp;#34;cgroup path is: %s\n&amp;#34;, cgDir) fmt.Printf(&amp;#34;cgroup event name is: %s\n&amp;#34;, evName) evFile, err := os.Open(filepath.Join(cgDir, evName)) if err != nil { panic(err) } fd, err := unix.</description></item><item><title>ARM 服务器适配总结</title><link>https://zdyxry.github.io/2020/08/22/ARM-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%82%E9%85%8D%E6%80%BB%E7%BB%93/</link><pubDate>Sat, 22 Aug 2020 15:09:04 +0000</pubDate><guid>https://zdyxry.github.io/2020/08/22/ARM-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%82%E9%85%8D%E6%80%BB%E7%BB%93/</guid><description>背景 Link to heading 上一次写 ARM 服务器相关的是在大半年以前了，当时适配工作做得神烦，最近在折腾 ARM 上 KVM 虚拟化相关的事情，目标是虚拟化功能最小代码改动同时兼容 x86 和 ARM，记录一下目前的一点经验总结。
硬件配置：
华为泰山服务器 Kunpeng 920 软件配置：
CentOS 7.6 kernel-4.18.0-193.1.2.el7.aarch64 libvirt-4.5.0 qemu-2.12.0 信息获取 Link to heading Architecture Link to heading 如果产品之前都是 x86 架构下的，在适配 ARM 时往往需要进行架构判断，此时可以通过 arch 来获取，在 ARM 架构显示 aarch64 ，在 x86 架构下显示 x86_64 。
aarch64 等价于 arm64。
AArch64 or ARM64 is the 64-bit extension of the ARM architecture.
KVM module Link to heading 在 x86 上我们可以直接通过 lsmod |grep kvm 查看到 KVM module 情况:</description></item><item><title>通过 grub 修复系统无法正确引导问题</title><link>https://zdyxry.github.io/2020/08/02/%E9%80%9A%E8%BF%87-grub-%E4%BF%AE%E5%A4%8D%E7%B3%BB%E7%BB%9F%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E5%BC%95%E5%AF%BC%E9%97%AE%E9%A2%98/</link><pubDate>Sun, 02 Aug 2020 09:10:39 +0000</pubDate><guid>https://zdyxry.github.io/2020/08/02/%E9%80%9A%E8%BF%87-grub-%E4%BF%AE%E5%A4%8D%E7%B3%BB%E7%BB%9F%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E5%BC%95%E5%AF%BC%E9%97%AE%E9%A2%98/</guid><description>背景 Link to heading 最近在将一个节点的 kernel 从 4.18 版本降级到 4.14 后，发现系统无法启动，直接进入到了 GRUB 提示符界面，记录下修复过程。
现象 Link to heading 因为 kernel 4.18 版本和 4.14 的 打包方式发生了比较大的变化，4.18 版本多出了 kernel-core 和 kernel-modules 两个 rpm：
4.14
[root@sh-workstation Packages]# ls |grep ^kernel kernel-4.14.0-115.el7a.0.1.aarch64.rpm kernel-headers-4.14.0-115.el7a.0.1.aarch64.rpm kernel-tools-4.14.0-115.el7a.0.1.aarch64.rpm kernel-tools-libs-4.14.0-115.el7a.0.1.aarch64.rpm 4.18
kernel-4.18.0-147.8.1.el7.aarch64.rpm kernel-core-4.18.0-147.8.1.el7.aarch64.rpm kernel-modules-4.18.0-147.8.1.el7.aarch64.rpm kernel-tools-4.18.0-147.8.1.el7.aarch64.rpm kernel-tools-libs-4.18.0-147.8.1.el7.aarch64.rpm 在没有官方 yum repo 的情况下，降级就比较麻烦，我直接尝试 rpm -Uvh kernel-4.14*.rpm ，然后将 4.18 的 kernel-core 和 kernel-modules 卸载掉，然后重启后，发现系统直接进入到了 GRUB 提示符界面，无法正常启动，只能寻求修复办法。
修复方式 Link to heading 当时系统启动后，显示的是 grub&amp;gt; 提示符，说明此时已经加载了 grub 程序，但是没有找到 grub.</description></item><item><title>PyInstaller 与 RPM 配合使用踩坑</title><link>https://zdyxry.github.io/2020/05/13/PyInstaller-%E4%B8%8E-RPM-%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8%E8%B8%A9%E5%9D%91/</link><pubDate>Wed, 13 May 2020 20:35:50 +0000</pubDate><guid>https://zdyxry.github.io/2020/05/13/PyInstaller-%E4%B8%8E-RPM-%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8%E8%B8%A9%E5%9D%91/</guid><description>背景 Link to heading 最近公司一个项目用到了 pyinstaller 打包 Python 环境，又因为公司内部发布的最小粒度是 rpm，发现这俩工具配合起来默认配置会有坑，然后搜索下来几乎没看到有人提到，可能用 pyinstaller 和用 rpm 的完全是两类人吧 - -。
准备 Link to heading 先列一下官方默认例子的输出结果：
root@localhost:/tmp/demo $ pwd /tmp/demo root@localhost:/tmp/demo $ ls demo.py root@localhost:/tmp/demo $ pyinstaller demo.py 63 INFO: PyInstaller: 3.6 63 INFO: Python: 3.6.8 65 INFO: Platform: Linux-3.10.0-862.el7.x86_64-x86_64-with-centos-7.5.1804-Core 65 INFO: wrote /tmp/demo/demo.spec 67 INFO: UPX is not available. 70 INFO: Extending PYTHONPATH with paths [&amp;#39;/tmp/demo&amp;#39;, &amp;#39;/tmp/demo&amp;#39;] ... 来看一下生成文件的目录结构：
. ├── build │ └── demo ├── demo.</description></item><item><title>在终端输入命令后系统做了什么</title><link>https://zdyxry.github.io/2020/04/25/%E5%9C%A8%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5%E5%91%BD%E4%BB%A4%E5%90%8E%E7%B3%BB%E7%BB%9F%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88/</link><pubDate>Sat, 25 Apr 2020 15:18:03 +0000</pubDate><guid>https://zdyxry.github.io/2020/04/25/%E5%9C%A8%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5%E5%91%BD%E4%BB%A4%E5%90%8E%E7%B3%BB%E7%BB%9F%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88/</guid><description>背景 Link to heading 无论是使用 VNC 连接还是 SSH 连接，每天都在用终端去执行命令，今天来了解下在执行命令后，系统具体做了什么，如何做的。
Shell Link to heading shell 是一个程序，也是一种编程语言，一个管理进程和运行程序的程序，在 Linux 中有很多 shell 可选，比如 bash、zsh、fish 等等，shell 主要有 3 个功能：
运行程序 管理输入和输出 可编程 运行程序很容易理解，在终端上输入的每个命令都是一个可执行程序，我们在 shell 中输入并执行程序；管理输入和输出，在 shell 中可以使用 &amp;lt; &amp;gt; | 符合控制输入、输出重定向，可以告诉 shell 将进程的输入和输出连接到一个文件或者其他的进程；编程，shell 是一种编程语言，可以进行变量赋值、循环、条件判断等操作。
如何运行程序 Link to heading shell 永远在等待用户输入，输入完成按下回车键后，开始执行相应命令（程序），然后等待程序执行完成后打印相应输出，伪代码：
while (! end_of_input) get command execute command wait for command to finish 在 shell 中因为需要执行其他的程序，需要用到 execvp ，execvp 会将指定的程序复制到调用它的进程，将指定的字符串组作为参数传递给程序，然后运行程序。这里存在一个问题， execvp 的执行过程是内核将程序加载到当前进程，替换当前进程的代码和数据，然后执行，那么原有进程的状态都被替换掉，在执行完程序就直接退出，不会再回到原程序等待下次输入。
为了保证我们在执行程序后回到 shell 中，需要每次创建新的进程来执行程序，调用 fork 指令，进程调用 fork 后，内核分配新的内存块和内核数据结构，复制原进程到新的进程，向运行进程添加新的进程，将控制返回给两个进程。通过 fork 返回值来判断当前进程是否为父进程或子进程。</description></item><item><title>文件系统基本概念及常用操作解释</title><link>https://zdyxry.github.io/2020/04/18/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E9%87%8A/</link><pubDate>Sat, 18 Apr 2020 14:24:23 +0000</pubDate><guid>https://zdyxry.github.io/2020/04/18/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%A7%A3%E9%87%8A/</guid><description>背景 Link to heading 前段时间一个朋友问我删除文件和格式化有什么区别，最近正好在读《Unix Linux 编程实践教程》这本书，其中第4章是来讲文件系统相关的，本文是对其中内容进行部分摘要来解释。
Unix 文件系统的内部结构 Link to heading 从用户角度看，Unix 系统中的硬盘上的文件组成一棵目录树，每个目录能包含文件或其他目录，目录树的深度几乎没有限制，子目录下可以包含其他文件和子目录（套娃）。文件内容放置在对应的目录中，对应的目录内容放置在上层目录中。
文件系统是对硬盘设备的一种多层次的抽象，主要包含以下三层。
第一层：从硬盘到分区 Link to heading 一个硬盘能够存储大量的数据，硬盘可以被划分为多个区域，也就是硬盘分区，每个分区在系统中都可以看作是独立的硬盘。
第二层：从磁盘到块序列 Link to heading 一个硬盘由许多磁性盘片组成，每个盘片的表面都被划分为很多同心圆，这些同心圆被成为磁道，每个磁道有进一步被划分为扇区，每个扇区可以存储一定字节数的数据，例如每个扇区有 512字节空间。扇区是磁盘上的基本存储单元，磁盘都包含大量的扇区。
为磁盘块编号是一种很重要的方法，给每个磁盘块分配连续的编号使得操作系统能够计算磁盘上的每个块，可以一个磁盘接一个磁盘的从上到下给所有的块编号，还可以一个磁道接一个磁道的从外向里给所有的块编号，一个将磁盘扇区编号的系统使得我们可以把磁盘视为一系列块的组合。
第三层：从块序列到三个区域的划分 Link to heading 文件系统可以用来存储文件内容、文件属性（文件所有者、日期等）和目录，这些不同类型的数据是如何存储在编号的磁盘块上的呢?
文件系统区域划分 Link to heading 一部分成为数据区，用来存放文件真正的内容。另一部分成为 i-node 表，用来存放文件属性。第三部分成为超级块（superblock），用来存放文件系统自身的信息。文件系统由这 3 部分组合而成，其中任一部分都是由很多有序磁盘块组成的。
超级块 Link to heading 文件系统的第一个块被成为超级块。这个块中存放文件系统自身的结构信息，比如记录了每个区域的大小。超级块也存放未被使用的磁盘块信息，不同的文件系统的超级块信息不同，可以通过 debugfs 等类似命令查看。
i-node 表 Link to heading 文件系统的下一个部分被成为 i-node 表，每个文件都有一些属性，如大小、文件所有者和最近修改时间等等，这些属性被记录在一个被成为 i-node 的结构中，所有的 i-node 都有相同的大小，并且 i-node 表是这些 i-node 结构组成的一个列表。文件系统中每个文件都有一个 i-node。
数据区 Link to heading 文件系统的第 3 个部分是数据区。文件的真正内容保存在这个区域。磁盘上所有块的大小都是一样的。如果文件包含了超过一个块的内容，则文件内容会存放在多个磁盘块中。一个较大的文件很容易分布在上千个独立的磁盘块中。</description></item><item><title>fsck 是如何工作的</title><link>https://zdyxry.github.io/2020/04/05/fsck-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</link><pubDate>Sun, 05 Apr 2020 09:08:33 +0000</pubDate><guid>https://zdyxry.github.io/2020/04/05/fsck-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84/</guid><description>背景 Link to heading 在平时会遇到不少系统崩溃之后文件系统异常的情况，通常我们会通过 fsck 工具进行修复，今天来了解下 fsck 做了什么，是怎么做的。
工作负载示例 Link to heading 假设现在存在一种工作负载，将单个数据块附加到原有文件。通过打开文件，调用 lseek() 将文件偏移量移动到文件末尾，然后在关闭文件之前，向文件发出单个 4KB 写入来完成追加。
假定磁盘上使用标准的简单文件系统结构，包括一个 inode 位图（inode bitmap，只有 8 位，每个 inode 一个），一个数据位图（databitmap，也是 8 位，每个数据块一个），inode（总共 8 个，编号为 0 到 7，分布在 4 个块上），以及数据块（总共 8 个，编号为 0～7）。以下是该文件系统的示意图：
查看图中的结构，可以看到分配了一个 inode（inode 号为 2），它在 inode 位图中标记，单个分配的数据块（数据块 4）也在数据中标记位图。inode 表示为 I [v1]，因为它是此 inode 的第一个版本。它将很快更新（由于上述工作负载）。再来看看这个简化的 inode。在 I[v1]中，可以看到：
owner : remzi permissions : read-write size : 1 pointer : 4 pointer : null pointer : null pointer : null 在这个简化的 inode 中，文件的大小为 1（它有一个块位于其中），第一个直接指针指向块 4（文件的第一个数据块，Da），并且所有其他 3 个直接指针都被设置为 null（表示它们未被使用）。当然，真正的 inode 有更多的字段。</description></item><item><title>理解网卡混杂模式</title><link>https://zdyxry.github.io/2020/03/18/%E7%90%86%E8%A7%A3%E7%BD%91%E5%8D%A1%E6%B7%B7%E6%9D%82%E6%A8%A1%E5%BC%8F/</link><pubDate>Wed, 18 Mar 2020 19:49:04 +0000</pubDate><guid>https://zdyxry.github.io/2020/03/18/%E7%90%86%E8%A7%A3%E7%BD%91%E5%8D%A1%E6%B7%B7%E6%9D%82%E6%A8%A1%E5%BC%8F/</guid><description>背景 Link to heading 我自己会在虚拟机验证一些网络配置，比如在虚拟机内部创建 OVS Bridge 之类的操作。当 Hypervisor 是 vSphere ESXi 时，通常需要在 ESXi 的虚拟交换机上开启混杂模式，今天来说一说网络的混杂模式。
网卡工作模式 Link to heading 网卡有以下几种工作模式，通常网卡会配置广播和多播模式：
广播模式（Broad Cast Model）:它的物理地址地址是 0Xffffff 的帧为广播帧，工作在广播模式的网卡接收广播帧。它将会接收所有目的地址为广播地址的数据包，一般所有的网卡都会设置为这个模式 多播传送（MultiCast Model）：多播传送地址作为目的物理地址的帧可以被组内的其它主机同时接收，而组外主机却接收不到。但是，如果将网卡设置为多播传送模式，它可以接收所有的多播传送帧，而不论它是不是组内成员。当数据包的目的地址为多播地址，而且网卡地址是属于那个多播地址所代表的多播组时，网卡将接纳此数据包，即使一个网卡并不是一个多播组的成员，程序也可以将网卡设置为多播模式而接收那些多播的数据包。 直接模式（Direct Model）:工作在直接模式下的网卡只接收目地址是自己 Mac 地址的帧。只有当数据包的目的地址为网卡自己的地址时，网卡才接收它。 混杂模式（Promiscuous Model）:工作在混杂模式下的网卡接收所有的流过网卡的帧，抓包程序就是在这种模式下运行的。网卡的缺省工作模式包含广播模式和直接模式，即它只接收广播帧和发给自己的帧。如果采用混杂模式，网卡将接受同一网络内所有所发送的数据包，这样就可以到达对于网络信息监视捕获的目的。它将接收所有经过的数据包，这个特性是编写网络监听程序的关键。 混杂模式 Link to heading 混杂模式（promiscuous mode）是计算机网络中的术语。是指一台机器的网卡能够接收所有经过它的数据流，而不论其目的地址是否是它。
一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据。当网卡工作在混杂模式下时，网卡将来自接口的所有数据都捕获并交给相应的驱动程序。网卡的混杂模式一般在网络管理员分析网络数据作为网络故障诊断手段时用到，同时这个模式也被网络黑客利用来作为网络数据窃听的入口。在Linux操作系统中设置网卡混杂模式时需要管理员权限。在Windows操作系统和Linux操作系统中都有使用混杂模式的抓包工具，比如著名的开源软件Wireshark。
查看网卡是否开启混杂模式 Link to heading [root@node1 21:06:51 ~]$ifconfig ens192 ens192: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt; mtu 1500 inet 192.168.17.88 netmask 255.255.240.0 broadcast 192.168.31.255 inet6 fe80::20c:29ff:fe99:4eb2 prefixlen 64 scopeid 0x20&amp;lt;link&amp;gt; ether 00:0c:29:99:4e:b2 txqueuelen 1000 (Ethernet) RX packets 163692 bytes 10031607 (9.</description></item><item><title>SR-IOV 基本概念</title><link>https://zdyxry.github.io/2020/03/12/SR-IOV-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link><pubDate>Thu, 12 Mar 2020 23:04:27 +0000</pubDate><guid>https://zdyxry.github.io/2020/03/12/SR-IOV-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid><description>背景 Link to heading 上周了解了 RDMA 的基础概念之后，发现在 KVM 平台无需其他配置就可使用，但是在 vSphere 场景下，需要面临一个取舍，其中的一个选择就是 SR-IOV ，今天来了解下 SR-IOV 。
SR-IOV Link to heading SR-IOV 全称 Single Root I/O Virtualization，是 Intel 在 2007年提出的一种基于硬件的虚拟化解决方案。
在虚拟化场景中，CPU 与内存是最先解决的，但是 I/O 设备一直没有很好的解决办法，Intel 有 VT-d（Virtualization Technology for Directed I/O）可以将物理服务器的 PCIe 设备直接提供给虚拟机使用，也就是我们常说的“直通”（passthrough），但是直通面临一个问题是 PCIe 设备只能给一个虚拟机使用，其他虚拟机就只能干瞪眼，这肯定是不行的，所以有了 SR-IOV，一个物理设备可以虚拟出多个虚拟设备给虚拟机使用。
SR-IOV 是一种规范，使得单根端口下的单个快速外围组件互连 (PCIe) 物理设备显示为管理程序或客户机操作系统的多个单独的物理设备，既有直通设备的性能优势，又可以支持多个虚拟机，一举两得。
SR-IOV 使用 physical functions (PF) 和 virtual functions (VF) 为 SR-IOV 设备管理全局功能。
PF 包含SR-IOV 功能的完整PCIe设备，PF 作为普通的PCIe 设备被发现、管理和配置 。PF 通过分配VF 来配置和管理 SR-IOV 功能。禁用SR-IOV后，主机将在一个物理网卡上创建一个 PF。 VF 是轻量级 PCIe 功能（I/O 处理）的 PCIe 设备，每个 VF 都是通过 PF 来生成管理的，VF 的具体数量限制受限于 PCIe 设备自身配置及驱动程序的支持，启用S​​R-IOV后，主机将在一个物理NIC上创建单个PF和多个VF。 VF的数量取决于配置和驱动程序支持。 每个 SR-IOV 设备都可有一个 PF(Physical Functions)，并且每个 PF 最多可有64,000个与其关联的 VF(Virtual Function)。PF 可以通过寄存器创建 VF，这些寄存器设计有专用于此目的的属性。一旦在 PF 中启用了 SR-IOV，就可以通过 PF 的总线、设备和功能编号（路由 ID）访问各个 VF 的 PCI 配置空间。</description></item><item><title>systemd 常用操作及配置</title><link>https://zdyxry.github.io/2020/03/06/systemd-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%8F%8A%E9%85%8D%E7%BD%AE/</link><pubDate>Fri, 06 Mar 2020 21:02:49 +0000</pubDate><guid>https://zdyxry.github.io/2020/03/06/systemd-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%8F%8A%E9%85%8D%E7%BD%AE/</guid><description>背景 Link to heading 现在大多数 Linux 发行版是由 systemd 来进行系统管理了， systemd 也是越来越复杂，但是常用的操作就那么多，今天来说说自己常用的操作及配置。
操作 Link to heading daemon-reload Link to heading 在给系统新增服务的时候，通常要不断的改 test.service 配置文件，在改配置文件后，通常需要执行 systemctl daemon-reload 来重新加在 systemd 配置。
no-page Link to heading 通常使用 journalctl 查看服务日志，但是当日志超过当前行可显示最大字符数时，默认会将日志截断，此时可以使用 journalctl -u &amp;lt;service&amp;gt; --no-page 来让日志自动折行。
disk-usage Link to heading systemd 日志配置文件在 /etc/systemd/journald.conf ，那么如果我想查看一个服务的日志占用空间，可以用 journalctl -u &amp;lt;servie&amp;gt; --disk-usage 命令查看。
poweroff Link to heading Linux 下关机命令有很多， init,shutdown,poweroff ，但是如果你注意过会发现以下事实：
root@localhost:~ $ ll `which init` lrwxrwxrwx. 1 root root 22 8月 22 2019 /usr/sbin/init -&amp;gt; .</description></item><item><title>DMA &amp; RDMA 基本概念</title><link>https://zdyxry.github.io/2020/03/02/DMA-RDMA-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link><pubDate>Mon, 02 Mar 2020 19:56:16 +0000</pubDate><guid>https://zdyxry.github.io/2020/03/02/DMA-RDMA-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid><description>背景 Link to heading 最近公司的存储系统要支持 RDMA 了，但是我连 RDMA 具体是啥还不清楚，今天花点时间来学习下相关知识。
在了解 RDMA 之前，需要先知道 DMA 是什么，所以会一点点说。
P.S. 本文部分内容截取自 《OSTEP》。
DMA Link to heading 系统架构 Link to heading 先来看一个典型的系统架构，其中，CPU 通过某种内存总线（memory bus）或互连电缆连接到系统内存。图像或者其他高性能 I/O 设备通过常规的I/O 总线（I/O bus）连接到系统，在许多现代系统中会是 PCI 或它的衍生形式。最后，更下面是外围总（peripheral bus），比如 SCSI、SATA 或者 USB。它们将最慢的设备连接到系统，包括磁盘、鼠标及其他类似设备。
为什么会用这样的分层架构？因为物理布局及造价成本。越快的总线长度越短，因此高性能的内存总线没有足够的空间来接太多设备。另外，在工程上高性能总线的造价非常高。所以，系统的设计采用了这种分层的方式，这样可以让要求高性能的设备（比如显卡）离 CPU 更近一些，低性能的设备离 CPU 远一些。将磁盘和其他低速设备连到外围总线的好处很多，其中较为突出的好处就是你可以在外围总线上连接大量的设备。
标准设备 Link to heading 现在来看一个标准设备（不是真实存在的），通过它来帮助我们更好地理解设备交互的制。从图 中，可以看到一个包含两部分重要组件的设备。 第一部分是向系统其他部分展现的硬件接口（interface）。同软件一样，硬件也需要一些接口，让系统软件来控它的操作。因此，所有设备都有自己的特定接口以及典型交互的协议。 第二部分是它的内部结构（internal structure）。这部分包含设备相关的特定实现，负责具体实现设备展示给系统的抽象接口。非常简单的设备通常用一个或几个芯片来实现它们的功能。更复杂的设备会包含简单的 CPU、一些通用内存、设备相关的特定芯片，来完成它们的工作。例如，现代 RAID 控制器通常包含成百上千行固件（firmware，即硬件设备中的软件），以实现其功能。
标准协议 Link to heading 在图 36.2 中，一个（简化的）设备接口包含 3 个寄存器：一个状态（status）寄存器，可以读取并查看设备的当前状态；一个命令（command）寄存器，用于通知设备执行某个具体任务；一个数据（data）寄存器，将数据传给设备或从设备接收数据。通过读写这些寄存器，操作系统可以控制设备的行为。
我们现在来描述操作系统与该设备的典型交互，以便让设备为它做某事。协议如下：
While (STATUS == BUSY) ; // wait until device is not busy Write data to DATA register Write command to COMMAND register (Doing so starts the device and executes the command) While (STATUS == BUSY) ; // wait until device is done with your request 该协议包含 4 步：</description></item><item><title>使用 Raft 实现 VIP 功能</title><link>https://zdyxry.github.io/2020/01/17/%E4%BD%BF%E7%94%A8-Raft-%E5%AE%9E%E7%8E%B0-VIP-%E5%8A%9F%E8%83%BD/</link><pubDate>Fri, 17 Jan 2020 22:48:28 +0000</pubDate><guid>https://zdyxry.github.io/2020/01/17/%E4%BD%BF%E7%94%A8-Raft-%E5%AE%9E%E7%8E%B0-VIP-%E5%8A%9F%E8%83%BD/</guid><description>背景 Link to heading 在部署应用时想要应用是高可用，通常会在应用前放置一个 HAProxy，当任何一个 Server 故障，HAProxy 会自动切换，但是 HAProxy 也存在单点故障，因此需要多个 HAProxy 来保证业务不中断，这时候我们需要另一个软件配合：Keepalived。通常我用 Keepalived 仅用来提供 VIP，保证当一个 Keepalived 故障，VIP 自动在其他 Keepalived 节点配置。
Keepalived 有一个问题是 virtual route ID 必须是同一网段内唯一的，当我们想要在一个网段内部署多个集群时，就需要人为的介入去分配 virtual route ID，不方便。这次来使用 Raft 自己实现 VIP 逻辑。
hashicorp/raft Link to heading Raft 有很多开源实现，其中 Hashicorp 实现的 Raft 库 已经被 Consul 等软件使用，且接口友善，选择使用它来实现。在 Github 上有很多 Raft 的使用示例，比较简单且完整的是 otoolep/hraftd，我们来看看他是怎么使用的。
otoolep/hraftd Link to heading main.go Link to heading 在 main.go 中主要做了 4 件事情：store.New, store.Open, http.New, http.Start，先来看看程序是如何启动的：
func init() { // 设置命令行参数 flag.</description></item><item><title>记一次 libcgroup 配置失败（二）</title><link>https://zdyxry.github.io/2019/12/25/%E8%AE%B0%E4%B8%80%E6%AC%A1-libcgroup-%E9%85%8D%E7%BD%AE%E5%A4%B1%E8%B4%A5%E4%BA%8C/</link><pubDate>Wed, 25 Dec 2019 21:49:06 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/25/%E8%AE%B0%E4%B8%80%E6%AC%A1-libcgroup-%E9%85%8D%E7%BD%AE%E5%A4%B1%E8%B4%A5%E4%BA%8C/</guid><description>背景 Link to heading 前几天同事找到我，说有一台服务器上的 cgroup 没有创建出来，导致其他程序出现了问题，记录一下。
现象 Link to heading 在我们的服务器上，通常会通过 libcgconfig 来进行 cgroup 的配置，供其他服务使用。结果发现对应的 cgroup 没有创建出来，于是查看 cgconfig.service 的状态，发现是异常退出的：
报错信息比较重要的是这一条：
failed to set /sys/fs/cgroup/cpuset/zbs/cpuset.mems: Invalid argument。
调查 Link to heading 检查下 /etc/cgconfig.conf 中的配置是否正确：
group . { cpuset { cpuset.memory_pressure_enabled = &amp;#34;1&amp;#34;; } } group zbs { cpu { cpu.rt_runtime_us = &amp;#34;950000&amp;#34;; cpu.rt_period_us = &amp;#34;1000000&amp;#34;; } cpuset { cpuset.cpus = &amp;#34;0,1,2,3,4,5&amp;#34;; cpuset.mems = &amp;#34;0-1&amp;#34;; cpuset.cpu_exclusive = &amp;#34;1&amp;#34;; cpuset.mem_hardwall = &amp;#34;1&amp;#34;; } } .</description></item><item><title>邻居发现协议（NDP）简易实现</title><link>https://zdyxry.github.io/2019/12/11/%E9%82%BB%E5%B1%85%E5%8F%91%E7%8E%B0%E5%8D%8F%E8%AE%AENDP%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</link><pubDate>Wed, 11 Dec 2019 21:29:59 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/11/%E9%82%BB%E5%B1%85%E5%8F%91%E7%8E%B0%E5%8D%8F%E8%AE%AENDP%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</guid><description>背景 Link to heading 在做节点管理时，经常面临节点自动扫描，自动关联等功能，这时候需要 NDP 来帮助我们来完成，关于 NDP 的实现有几种方式，今天来聊一下这个。
邻居发现协议（NDP） Link to heading 引用维基百科的介绍：
The Neighbor Discovery Protocol (NDP, ND)[1] is a protocol in the Internet protocol suite used with Internet Protocol Version 6 (IPv6). It operates at the link layer of the Internet model (RFC 1122), and is responsible for gathering various information required for internet communication, including the configuration of local connections and the domain name servers and gateways used to communicate with more distant systems.</description></item><item><title>SSH known_hosts 显示 IP 地址</title><link>https://zdyxry.github.io/2019/12/06/SSH-known_hosts-%E6%98%BE%E7%A4%BA-IP-%E5%9C%B0%E5%9D%80/</link><pubDate>Fri, 06 Dec 2019 21:54:25 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/06/SSH-known_hosts-%E6%98%BE%E7%A4%BA-IP-%E5%9C%B0%E5%9D%80/</guid><description>背景 Link to heading 工作上使用的电脑因为各种各样的原因，被我安装为 Ubuntu 19.04，平时使用上没什么问题，但是最近发现它默认的 SSH 配置随着版本升级发生了变化，known_hosts 文件中记录的不再是 IP 地址，而是一串字符，这导致了当我想要删除某个主机的 key 时，无法准确的找到，因此想办法解决这个事情。
SSH Link to heading 我们使用 Linux 系统的一个最基本的服务应该就是 SSH 了，除了偶尔我们通过 VNC 或者 KVM(Keyboard Virtual Manager)连接控制主机外，都是通过 SSH 到 Linux 主机上进行某些操作。那么 SSH 就是 Secure Shell，安全外壳协议。可在不安全的网络中为网络服务提供安全的传输环境。SSH 通过在网络中建立安全隧道来实现SSH客户端与服务器之间的连接。
SSH 最重要的就是安全，采用的是非对称加密，关于加密相关的部分，可以看我之前写的一篇《图解密码技术》读书笔记 里面有比较完整的相关知识。
这里主要说一下使用密码登陆和密钥登陆的流程。
密码登陆 Link to heading 客户端向服务端发起连接请求 root@test-hostname-ubuntu-s-1804:~# ssh 192.168.67.90 The authenticity of host &amp;#39;192.168.67.90 (192.168.67.90)&amp;#39; can&amp;#39;t be established. ECDSA key fingerprint is SHA256:ca9Zk/7pR4f6rrNP3wi1WK+CQMtG4Ka+kkouwQYU0nY. Are you sure you want to continue connecting (yes/no)?</description></item><item><title>Linux 引导那些事儿</title><link>https://zdyxry.github.io/2019/12/01/Linux-%E5%BC%95%E5%AF%BC%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</link><pubDate>Sun, 01 Dec 2019 00:32:26 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/01/Linux-%E5%BC%95%E5%AF%BC%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</guid><description>背景 Link to heading 在平时工作中我自己可能80% 的时间都在使用 Linux 进行工作，无论是软件开发还是环境维护，那么对于 Linux 发行版自然也是有比较熟悉的，比如 RHEL 系列。平时大家可能多多少少都会听到或接触到一些命令（术语），比如：grub,chroot,isolinux,bios 等等。今天打算就用 CentOS 发行版的 ISO 来谈谈自己对于 Linux 系统引导，安装，启动的理解。
CentOS ISO 目录树如下：
├── CentOS_BuildTag ├── EFI ├── EULA ├── GPL ├── images ├── isolinux ├── LiveOS ├── Packages ├── repodata ├── RPM-GPG-KEY-CentOS-7 ├── RPM-GPG-KEY-CentOS-Testing-7 └── TRANS.TBL isolinux Link to heading ISO 9660 Link to heading ISO 9660 是一种文件系统，也是一种规范，它规定了 ISO 文件应该是什么样子，常见的 Linux 发行版 ISO 都符合该规范。
引导 Link to heading 有了 ISO，有了计算机，那么我们需要使用 ISO 安装想要的操作系统，通常我们会将 ISO 通过光盘/USB/网络等方式挂载到计算机上，通过设置 BIOS/UEFI 选项，将启动项设置为 ISO，然后启动进行安装。接下来一个一个来说。</description></item><item><title>Cloud-init 无需重启执行配置</title><link>https://zdyxry.github.io/2019/11/29/Cloud-init-%E6%97%A0%E9%9C%80%E9%87%8D%E5%90%AF%E6%89%A7%E8%A1%8C%E9%85%8D%E7%BD%AE/</link><pubDate>Fri, 29 Nov 2019 21:35:26 +0000</pubDate><guid>https://zdyxry.github.io/2019/11/29/Cloud-init-%E6%97%A0%E9%9C%80%E9%87%8D%E5%90%AF%E6%89%A7%E8%A1%8C%E9%85%8D%E7%BD%AE/</guid><description>背景 Link to heading 最近在折腾 Cluster API 的时候，因为目前社区中比较成熟的方案是通过 Cloud-init 执行 kubeadm 命令部署 k8s ，因此需要使用 Cloud-init 进行功能验证，但是 Cloud-init 通常执行的前提条件是系统初次启动时，自动执行配置，这点对于调试很不友好，因此需要找到一个无需重启即可执行 Cloud-init 配置的方式。
Cloud-init Link to heading 使用过公有云或者私有云的同学应该都知道在创建虚拟机的时候可以传递一个脚本用于在机器置备的时候执行某些动作，尤其在批量执行的时候，通常会很方便。这个其实就是 Cloud-init 所做的工作，就跟它的名字一样，针对 Cloud 场景执行 init 动作。
引用官网介绍：
Cloud-init is the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major public cloud providers, provisioning systems for private cloud infrastructure, and bare-metal installations.
NoCloud Link to heading Cloud-init 官方支持云平台种类很多常见的公有云如 Aliyun、AWS、Azure，常见的私有云解决方案如 OpenStack、ZStack、OVF 等都有支持。</description></item><item><title>使用 Ansible 传输文件的几种方式</title><link>https://zdyxry.github.io/2019/11/22/%E4%BD%BF%E7%94%A8-Ansible-%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</link><pubDate>Fri, 22 Nov 2019 20:15:17 +0000</pubDate><guid>https://zdyxry.github.io/2019/11/22/%E4%BD%BF%E7%94%A8-Ansible-%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</guid><description>背景 Link to heading Ansible 作为一款配置管理和应用部署的软件，日常使用的场景很多，我自己也是重度用户。最近在使用 Ansible 进行文件传输的时候踩了个坑，借此机会整理下 Ansible 传输文件的几种方式。
实验环境：
[root@yiran ~]# ansible --version ansible 2.7.12 config file = None configured module search path = [u&amp;#39;/root/.ansible/plugins/modules&amp;#39;, u&amp;#39;/usr/share/ansible/plugins/modules&amp;#39;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Aug 7 2019, 00:51:29) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] Template Link to heading https://docs.ansible.com/ansible/latest/modules/template_module.html
Ansible 一个常用的功能就是配置管理，通过 Ansible 批量分发配置文件，达到统一版本管理的效果，如果我们想要进行少量的文件传输，从控制节点传输到被管理节点，那么可以采用这种方式来完成。
具体的使用官方文档 已经讲的很详细了，这里不再啰嗦。
因为版本是通过 Jinja2 模板传输，支持模板渲染 虽然我们可以在模版中不制定任何参数，直接将其拷贝，但是相比 Copy/Fetch 模块还是需要有模版渲染的一步，速度要慢一些</description></item><item><title>为什么你的 mdadm 同步这么慢</title><link>https://zdyxry.github.io/2019/11/15/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84-mdadm-%E5%90%8C%E6%AD%A5%E8%BF%99%E4%B9%88%E6%85%A2/</link><pubDate>Fri, 15 Nov 2019 20:20:03 +0000</pubDate><guid>https://zdyxry.github.io/2019/11/15/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84-mdadm-%E5%90%8C%E6%AD%A5%E8%BF%99%E4%B9%88%E6%85%A2/</guid><description>背景 Link to heading 自己一直通过 mdadm 在软件层面对多块磁盘进行 RAID1 配置，一个主要的原因是 mdadm 是 KickStart 默认软件。因为只是 RAID1，所以使用起来也是比较方便，虽然有些小坑，但总体来说还好。
最近遇到一个问题， mdadm 在配置 RAID1 时，磁盘同步很慢。
现象 Link to heading 先说下磁盘构成，一般情况下是这样：
[root@yiran 20:23:48 ~]$lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 59.6G 0 disk ├─sda1 8:1 0 256M 0 part /boot/efi └─sda2 8:2 0 512M 0 part /boot sdb 8:16 0 223.6G 0 disk └─sdb1 8:17 0 45G 0 part └─md127 9:127 0 45G 0 raid1 / sdc 8:32 0 223.</description></item><item><title>ARM 服务器适配记录</title><link>https://zdyxry.github.io/2019/11/01/ARM-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%82%E9%85%8D%E8%AE%B0%E5%BD%95/</link><pubDate>Fri, 01 Nov 2019 22:12:00 +0000</pubDate><guid>https://zdyxry.github.io/2019/11/01/ARM-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%82%E9%85%8D%E8%AE%B0%E5%BD%95/</guid><description>背景 Link to heading 最近各大 2B 厂家都在搞国产化，我司也不例外，花费了些时间折腾了下 ARM 服务器，记录下踩坑和使用感受。
本文所使用的开发环境为 CentOS7.6。
编译 Link to heading 使用的第一步是编译自己的软件，我日常使用的软件发布的最小粒度是 RPM，所以我们需要把自己在 X86 上的软件都重新编译一份 ARM64v8 的。
Python Link to heading 在预期中，Python 应该是最简单的，直接编译 RPM 就好，我在之前博客《RPM 常用构建方式》 中提到过，对于 Python 来说，最简单的是通过 FPM 来构建 RPM，但是这里有两个坑。
Python 软件是否依赖了 C/C++ ，如果依赖了，那么需要在 ARM 机器上使用 FPM 的时候编译构建 RPM 从网上下载的 noarch.rpm ，是否真的是 noarch 的？需要仔细检查（别问我怎么知道的 在我以为一切搞定的时候，想起来还有个服务是需要机器学习等第三方库的，是使用 conda 安装的。 好嘛，完蛋了，折腾几个小时下来结论是 conda 对于 ARM64 几乎处于不支持的状态。最后也没搞定。
Golang Link to heading 最简单的莫过于 Golang 了，在 Golang 1.5 及之后版本，只需要设置 GOOS 和 GOARCH 这两个环境变量就能编译出目标平台的 Go binary 文件。</description></item><item><title>Skopeo 初次体验</title><link>https://zdyxry.github.io/2019/10/26/Skopeo-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</link><pubDate>Sat, 26 Oct 2019 22:52:06 +0000</pubDate><guid>https://zdyxry.github.io/2019/10/26/Skopeo-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</guid><description>背景 Link to heading 新一代容器工具体验系列已经完成了 Podman 和 Buildah 的介绍，今天来体验下三剑客中的 Skopeo。
容器工具体验系列：
Podman 初次体验 Buildah 初次体验 Skopeo 初次体验 What Link to heading Skopeo 的功能很简单，一句话描述就是：提供远程仓库的镜像管理能力。
功能列表：
复制镜像，无需特殊权限即可从不通仓库复制镜像 无需拉取镜像即可获取远程镜像仓库中的镜像属性（包括 layer） 删除镜像仓库中的镜像 &amp;hellip; 支持镜像仓库类型：
container-storage 本地路径 docker registry 仓库 docker 打包镜像文件 本地 docker 拉取的镜像文件 OCI 镜像 &amp;hellip; 吐槽：Podman 和 Buildah 好歹都有自己的域名： podman.io 和 buildah.io ，Skopeo 虽然用的少但是也得搞个官网吧。。。
How Link to heading 知道了 Skopeo 主要是对镜像仓库及镜像信息的获取，那么我们来看几个具体的例子，了解下 Skopeo 的使用。
镜像详情 Link to heading root@yiran-workstation:~ $ skopeo inspect docker://docker.</description></item><item><title>记一次系统被入侵分析过程</title><link>https://zdyxry.github.io/2019/10/25/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%B3%BB%E7%BB%9F%E8%A2%AB%E5%85%A5%E4%BE%B5%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B/</link><pubDate>Fri, 25 Oct 2019 22:33:06 +0000</pubDate><guid>https://zdyxry.github.io/2019/10/25/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%B3%BB%E7%BB%9F%E8%A2%AB%E5%85%A5%E4%BE%B5%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B/</guid><description>背景 Link to heading 今天早上接到同事报警，环境中两个节点出现了 CPU 使用率告警，通过 top 查看发现是一个叫 iSdqkI 的进程，但是这明显不是常规进程，初步怀疑是系统被入侵了，在同事的协助下最终解决了。这次主要记录遇到这种问题的排查思路，也算是对过程的复述。
分析过程 Link to heading 首先我们得到的信息是 CPU 使用率告警，第一时间是通过 top 来看看是哪个进程在作怪：
可以看到 FnrgiY 这个进程 CPU 使用率为 556%，且这个进程不是我们系统中存在的进程，这里判断是入侵后被植入的软件，通过 ps 命令查看进程的具体执行内容：
可以看到， FnrgiY 应该是一个可执行的程序（可能是脚本，也可能是一个 binary 文件），我非常年轻的想通过 find 查看这个文件在哪，然后 kill 掉进程删除文件就好了：
嗯，果然年轻了，系统下不存在该文件，那么我们尝试在 /proc/15582 下来看看有什么线索，先看看 cmdline ，跟进程名相同，没啥信息
同样 stack 文件也没什么有用的信息
来看看这个进程的 cwd 是啥，可以看到 cwd 是 /usr/bin 路径的软链接，但是我刚刚已经检查过了，在 /usr/bin/ 下没有这个可执行文件
通过 lsof -p 命令，来看看这个进程打开了哪些文件，可以看到它启动的进程文件是 /usr/bin/e6bb0f* ，但是被删掉了，然后它还有一个 TCP 连接（先不管），看到一个存在的文件 /tmp/.X11-unix/1
来看看这个文件是啥，cat 一下发现这个文件其实是 pid 文件，并没有其他信息
既然它用到了这个文件，那么我们来看下这个文件所在路径，引用 StackExchange 里面的回答：</description></item><item><title>Buildah 初次体验</title><link>https://zdyxry.github.io/2019/10/19/Buildah-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</link><pubDate>Sat, 19 Oct 2019 10:15:39 +0000</pubDate><guid>https://zdyxry.github.io/2019/10/19/Buildah-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</guid><description>背景 Link to heading 上周体验了 Podman 来管理容器的构建、生命周期管理等。Podman 自身是可以通过 Dockerfile 来进行容器镜像的构建，并且也支持容器镜像的 pull/push/login 等操作，Buildah 能够带来什么好处，我们为什么要使用它？
容器工具体验系列：
Podman 初次体验 Buildah 初次体验 Skopeo 初次体验 注意：本文章所采用环境为 CentOS7，需要除了 Buildah 工具外，还需要安装 containers-common 用于配置容器。
What Link to heading 我们现在使用的容器管理工具无论是 Podman 还是 Docker，都是符合 OCI 规范的，他们操作的镜像也需要符合 OCI 规范，Buildah 介绍很简单： A tool that facilitates building OCI images。
Buildah 功能列表：
创建容器 通过 Dockerfile 或者一个处于运行状态的容器（指 Buildah 自身创建的容器，Podman 不可见 挂载/卸载镜像文件系统 使用更新后挂载的镜像文件系统作为文件系统层创建新的镜像 &amp;hellip; Buildah 与 Podman 的关系 Link to heading 在官方说法中，Buildah 与 Podman 是相辅相成的关系，有很多共同点：它们都不需要 root 权限；都可以通过 Dockerfile 来构建容器镜像；都采用 fork-exec 模型；都不需要守护进程等等。 Buildah 主要的优势在于可以在没有 Doclerfiles 的情况下创建容器镜像，这也造成了从 Docker 切换到 Buildah 的用户使用成本会稍微高一些，因为部分概念发生了改变，主要有以下这些对比：</description></item><item><title>Podman 初次体验</title><link>https://zdyxry.github.io/2019/10/12/Podman-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</link><pubDate>Sat, 12 Oct 2019 21:40:30 +0000</pubDate><guid>https://zdyxry.github.io/2019/10/12/Podman-%E5%88%9D%E6%AC%A1%E4%BD%93%E9%AA%8C/</guid><description>背景 Link to heading CentOS8 在9月24号正式 Release 了，比 RHEL8 要推迟了4个月。这次的更新感觉比 CentOS7 的更新要来的重要，内核更新到了4.x，网络管理彻底替换了 network.service，防火墙管理等等，还包括去除了 Docker 作为默认的容器化管理工具，使用 Podman、Buildah、Skopeo 进行了替换，这里来体验下 Podman。
容器工具体验系列：
Podman 初次体验 Buildah 初次体验 Skopeo 初次体验 本篇文章所有环境基于 CentOS8。
Podman Link to heading 为啥不用 Docker 了？我个人觉得 Docker 目前使用上最大的问题就是需要运行一个守护进程，虽然需要 root 用户也是一个问题，但是对于我个人来说还好。随着 K8S 定义 CRI 标准，且 Docker 的稳定性一直是个问题（虽然最近有在往好的趋势发展），但越来越多人使用 CRI-O 来替代 Docker，Docker 在被大家所抛弃（- -
Podman 创建的容器不需要守护进程，且可以用普通用户创建容器。Podman 中的大部分命令的使用方式与 Docker 相同，可以看左 alias docker=podman 。
Podman 的缺点：
仅在 Linux 下支持，无法像 Docker 一样支持 Windows 和 MacOS 缺少 docker-compose 工具替代品，哪怕有 k8s Pod 概念（虽然有 podman-compose，但是他还没有release 1.</description></item><item><title>Linux 下常用故障模拟方法</title><link>https://zdyxry.github.io/2019/08/31/Linux-%E4%B8%8B%E5%B8%B8%E7%94%A8%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/</link><pubDate>Sat, 31 Aug 2019 07:47:00 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/31/Linux-%E4%B8%8B%E5%B8%B8%E7%94%A8%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/</guid><description>背景 Link to heading 在日常开发时，有时候需要保证自己代码的健壮性，需要模拟各种故障测试，比如：磁盘、网络、端口等，今天来汇总一下平时使用最多的几种故障模拟方法
磁盘 Link to heading 插入拔出 Link to heading 服务器的存储控制器如果是直通模式，那么在 OS 中能够直接获取到磁盘插入与拔出事件，有时候我们需要检测到相应的事件来自动化的做某些动作，具体的实现方式见之前的文章 Linux 下磁盘设备自动发现方式 。
那么我们写完了代码想要测试，不想去机房物理操作，怎么模拟呢？
Hypervisor Link to heading 如果你的代码部署的机器是一台虚拟机，那么在 Hypervisor 层面一般都会有对应的接口来完成相应的操作。
比如 Vsphere ESXi 中可以直接编辑虚拟机，在磁盘选项中有一个“移除”按钮，可以直接移除磁盘：
再比如 KVM 下，可以通过 Libvirt 接口来 detach 磁盘。
当然对于插入动作，Hypervisor 也会提供对应的功能。
物理服务器 Link to heading 如果 OS 不是在 Hypervisor 上，而是直接安装在了物理服务器上，我们怎么做呢？
通常我们服务器上的磁盘都是 SCSI 设备，会实现完整的 SCSI（接口），可以通过修改相应设备的标置文件来达到目的。
示例： 节点存在设备 /dev/sda ，修改标置文件，在系统中会发现磁盘已经被移除了。
[root@yiran ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10G 0 disk sr0 11:0 1 4.</description></item><item><title>provider vs provisioner</title><link>https://zdyxry.github.io/2019/08/17/provider-vs-provisioner/</link><pubDate>Sat, 17 Aug 2019 21:08:46 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/17/provider-vs-provisioner/</guid><description>背景 Link to heading 之前 liqiang 同学写了一篇博客：Status 还是 State 用于总结日常工作中遇到的相似词的区别。这两天看代码，经常能够看到两个词：provider 和 provisioner，作为一个英语渣渣，很难准确的理解两个词的区别。
字典解释 Link to heading provider 字典中的解释为：
供应者（商） 提供者（商） 供养人 &amp;hellip; provisioner 字典中的解释为：
粮食供应者 看上去从字面上也是一个意思，那么我们来找个实际场景看看。
实际场景 Link to heading Terraform Link to heading 在 Terraform 概念中，同时存在 provider 和 provisioner 两个概念：
A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.</description></item><item><title>Linux 下磁盘设备自动发现方式</title><link>https://zdyxry.github.io/2019/08/02/Linux-%E4%B8%8B%E7%A3%81%E7%9B%98%E8%AE%BE%E5%A4%87%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E6%96%B9%E5%BC%8F/</link><pubDate>Fri, 02 Aug 2019 23:10:04 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/02/Linux-%E4%B8%8B%E7%A3%81%E7%9B%98%E8%AE%BE%E5%A4%87%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E6%96%B9%E5%BC%8F/</guid><description>背景 Link to heading 如果在 PC 上安装过 Linux，那么通常会遇到过硬件设备无法发现的问题，这类问题最终都可以通过 google 来解决掉。那么当我们在服务器场景下，如何做到设备自动发现且在设备发现后执行某些动作呢？
最近看了几个关于存储系统的 Operator 部分实现，记录一下。
命令行 Link to heading 最简单的肯定是我们写一个循环，永远检测我们要发现的设备，比如 lsblk 可以列举当前服务器所有 block 设备，那么我们就在循环内部执行 lsblk，diff 每次执行的结果，如果有新的设备，那么执行某些操作。
lsblk 是通过读取 /sys/block 下的具体目录判断的，那么我么也可以直接读取该路径下的目录来实现。
如果是网络设备也是一样，我们可以在循环内部执行 ip link list 来获取所有网络设备。
UDEV Link to heading 照常先引用维基百科的解释：
udev 是Linux kernel 2.6系列的设备管理器。它主要的功能是管理/dev目錄底下的设备节点。它同时也是用来接替devfs及hotplug的功能，这意味着它要在添加/删除硬件时处理/dev目录以及所有用户空间的行为，包括加载firmware时。
如果你的 OS 是通过 systemd 来管理所有进程的话，那么可以发现一个服务叫做 systemd-udevd ，这个是 udev 的守护进程:
[root@node90 19:58:10 ~]$systemctl status systemd-udevd ● systemd-udevd.service - udev Kernel Device Manager Loaded: loaded (/usr/lib/systemd/system/systemd-udevd.service; static; vendor preset: disabled) Active: active (running) since Fri 2019-06-14 15:25:55 CST; 1 months 18 days ago Docs: man:systemd-udevd.</description></item><item><title>CentOS定制-软件源错误</title><link>https://zdyxry.github.io/2019/07/21/CentOS%E5%AE%9A%E5%88%B6-%E8%BD%AF%E4%BB%B6%E6%BA%90%E9%94%99%E8%AF%AF/</link><pubDate>Sun, 21 Jul 2019 19:58:43 +0000</pubDate><guid>https://zdyxry.github.io/2019/07/21/CentOS%E5%AE%9A%E5%88%B6-%E8%BD%AF%E4%BB%B6%E6%BA%90%E9%94%99%E8%AF%AF/</guid><description>背景 Link to heading 我一直在维护一个公司内部的 OS 发行版，是基于 CentOS 的，最近接到了一个需求，是需要更新 Kernel 及一些软件包，但是遇到了无法安装 OS 的问题，记录一下解决方式。
定制 OS Link to heading 关于定制 OS，在之前的博客中已经提到过几次了，CentOS 是比较容易改动的一个发行版，因为有着 RHEL 红（爸）帽（爸），有着完善的文档可以参考。
主要需要注意的是两点：
分区方式 软件包选择 今天遇到的问题是第二点。
先说下前提，由于是 2B 产品，所以对于每次的 BaseOS 版本升级都非常谨慎，每次 BaseOS 版本都会进行各种测试。但是如果仅仅是升级部分所需要的软件包，就不用这么麻烦了，我们可以定制自己所需要的软件组（group），来进行安装/升级。
问题 Link to heading 这次接到的需要是升级 Kernel、libiscsi、qemu 三个软件，后两个是虚拟化相关的，相关依赖较少；kernel 是跟 BaseOS 版本关联性很大的。
比如 CentOS 7.6 中，kernel 版本为：kernel-3.10.0-957.el7.x86_64.rpm，这个版本对 selinux 等相关软件是有依赖要求的，我在这里翻车了。
解决 Link to heading 像往常一样，将对应的 rpm 放置到了对应的 yum 源中，更新 yum 源，制作 ISO，在安装过程中报错：
报错显示是软件源出了问题，但是没有更多的信息了，这时候我们可以通过 console 连接到其他的 pty 中，查看对应的日志，比如 CentOS 默认的日志在： /tmp/packaging.</description></item><item><title>调整 arp 参数提高网络稳定性</title><link>https://zdyxry.github.io/2019/06/17/%E8%B0%83%E6%95%B4-arp-%E5%8F%82%E6%95%B0%E6%8F%90%E9%AB%98%E7%BD%91%E7%BB%9C%E7%A8%B3%E5%AE%9A%E6%80%A7/</link><pubDate>Mon, 17 Jun 2019 21:20:04 +0000</pubDate><guid>https://zdyxry.github.io/2019/06/17/%E8%B0%83%E6%95%B4-arp-%E5%8F%82%E6%95%B0%E6%8F%90%E9%AB%98%E7%BD%91%E7%BB%9C%E7%A8%B3%E5%AE%9A%E6%80%A7/</guid><description>背景 Link to heading 最近发现一直使用的机房网络不稳定，时常出现网络无法联通，过一会又可以联通的情况，今天又遇到了，要彻底解决它。
问题 Link to heading 在机房网络规划中，地区 A 和地区 B 是通过 OpenVPN 连接的，也就是说每个地区的网关是一台虚拟机，提供 DHCP 服务。 今天地区 A 的机器又无法连接地区 B 了，我登陆网关尝试从网关 ping 目标主机，发现直接提示 :
No buffer space available 根据这个提示，感觉像是某些系统参数配置的小了，于是查了一下，发现跟 arp 有关。什么是 arp ？
相信对网络稍微有些概念的同学都不陌生，这里我直接引用维基百科：
地址解析协议（英语：Address Resolution Protocol，缩写：ARP）。在以太网协议中规定，同一局域网中的一台主机要和另一台主机进行直接通信，必须要知道目标主机的MAC地址。而在TCP/IP协议中，网络层和传输层只关心目标主机的IP地址。这就导致在以太网中使用IP协议时，数据链路层的以太网协议接到上层IP协议提供的数据中，只包含目的主机的IP地址。于是需要一种方法，根据目的主机的IP地址，获得其MAC地址。这就是ARP协议要做的事情。所谓地址解析（address resolution）就是主机在发送帧前将目标IP地址转换成目标MAC地址的过程。
另外，当发送主机和目的主机不在同一个局域网中时，即便知道对方的MAC地址，两者也不能直接通信，必须经过路由转发才可以。所以此时，发送主机通过ARP协议获得的将不是目的主机的真实MAC地址，而是一台可以通往局域网外的路由器的MAC地址。于是此后发送主机发往目的主机的所有帧，都将发往该路由器，通过它向外发送。这种情况称为委托ARP或ARP代理（ARP Proxy）。
解决 Link to heading 知道了原因，那么我们来调整参数就好：
gc_thresh1 (since Linux 2.2) The minimum number of entries to keep in the ARP cache. The garbage collector will not run if there are fewer than this number of entries in the cache.</description></item><item><title>LVM 使用总结</title><link>https://zdyxry.github.io/2019/03/29/LVM-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</link><pubDate>Fri, 29 Mar 2019 22:42:27 +0000</pubDate><guid>https://zdyxry.github.io/2019/03/29/LVM-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</guid><description>全称 Logical Volume Manager，逻辑卷管理，简单的说就是能将物理磁盘统一管理，在现在这个满大街都在谈论分布式存储的年代，已经很少有人关注和使用它了，毕竟如果买个公有云上的虚拟机，挂载磁盘都已经得到 TP999+ 的稳定性加持，扩容什么的也只是一句话的事，完全没必要使用 LVM 了。
那么我今天为啥要写 LVM 呢，首先肯定是最近要使用它（清理它），其次，现在 RedHat 系列发行版仍将 LVM 作为系统安装的默认磁盘处理方式，对于我这种安装系统家常便饭的人，还是要了解下的。
本文不会包含 LVM 命令的使用。
概念 Link to heading LVM 中有 3 个最重要的概念，分别是 PV，VG，LV，下面我分别来说一下：
PV Link to heading Physical Volumes，物理卷，属于 LVM 中最底层的单元，通常介质为磁盘，或者磁盘上的某个分区，多个 PV 可以组成 VG（卷组）。
VG Link to heading Volume Group，卷组，也是我们通常说的“池化”具体表现形式，VG 可以存在多个，我们可以根据具体的用途来划分 VG，来提供相应的性能保证，比如我们可以对 IO 密集型应用使用全 SSD 组成的 VG。
LV Link to heading Logical Volume，逻辑卷，也就是 LVM 中的 LV，LVM 暴露出来供我们使用的逻辑（虚拟）卷。我们可以将其当做一个普通的磁盘使用，可以在其上进行分区，格式化，磁盘读写等操作。
优缺点 Link to heading 简单的介绍了 LVM 的概念，那么我们来说下 LVM 的优缺点：</description></item><item><title>Python 调用 systemd watchdog 方法</title><link>https://zdyxry.github.io/2019/03/10/Python-%E8%B0%83%E7%94%A8-systemd-watchdog-%E6%96%B9%E6%B3%95/</link><pubDate>Sun, 10 Mar 2019 10:21:19 +0000</pubDate><guid>https://zdyxry.github.io/2019/03/10/Python-%E8%B0%83%E7%94%A8-systemd-watchdog-%E6%96%B9%E6%B3%95/</guid><description>systemd Link to heading 在之前的博客中介绍过 systemd 的基本使用及通过 timer 来替换 crontab 的方法，今天来说一下如何调用 watchdog。
在 systemd 中，提供 watchdog 来检测服务状态状态，官方文档中描述这个功能为 &amp;ldquo;keep-alive ping&amp;rdquo;，我们可以在服务的启动配置中，添加 WatchdogSec 来指定 timeout 时间，在服务程序中通过发送 WATCHDOG=1 来不断的通知 systemd，服务处于正常状态，当超过 timeout 时间未收到 WATCHDOG=1 信号后，systemd 会根据 Restart 配置，决定是否自动重启服务。
示例 Link to heading 服务程序：
root@yiran-30-250:/usr/lib/systemd/system $ cat /root/project/watchdog/test.py #!/usr/bin/python # coding:utf-8 import os import time import socket import logging print(&amp;#34;Test starting up...&amp;#34;) time.sleep(1) # 模拟执行真实业务 print(&amp;#34;Test startup finished&amp;#34;) try: sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) addr = os.</description></item><item><title>聊一聊 ISO 9660</title><link>https://zdyxry.github.io/2019/01/12/%E8%81%8A%E4%B8%80%E8%81%8A-ISO-9660/</link><pubDate>Sat, 12 Jan 2019 22:06:45 +0000</pubDate><guid>https://zdyxry.github.io/2019/01/12/%E8%81%8A%E4%B8%80%E8%81%8A-ISO-9660/</guid><description>ISO 9660 Link to heading ISO 9660，也被一些硬件和软件供应商称作CDFS（光盘文件系统），是一个由国际标准化组织（ISO）为光盘介质发布的文件系统。其目标是能够在不同的操作系统如Windows、Mac OS以及类Unix系统上交换数据。
我们平时接触到的 ISO 格式文件均为 ISO 9660，我们可以通过 file 命令进行查看，以 RedHat 发行版为例：
[root@node redhat]# file rhel-server-7.6-x86_64-dvd.iso rhel-server-7.6-x86_64-dvd.iso: ISO 9660 CD-ROM filesystem data &amp;#39;RHEL-7.6 Server.x86_64 &amp;#39; (bootable) 用途 Link to heading 安装操作系统，我们可以通过将 ISO 挂载到物理服务器/PC 上，从 ISO 启动进行操作系统的安装； 作为软件安装源，比如红帽系列的版本，在 ISO 中，通常会有 Packages 路径，下面包含了当前版本完整的软件源，我们可以通过本地挂载的方式使用； 作为软件发行方式，常见的有 VMware vmtools, KVM Virtio 等统一打包方式； &amp;hellip; 制作方式 Link to heading genisoimage mkisofs 第一种方式不常用，主要使用第二种方式进行使用，之前的文章中提到过 LiveCD 制作方式也是通过 mkisofs 命令继续构建的。
这里我们可以参考 VMware 官方文档了解如何定制自己的 ISO。
说一下何时需要定制我们自己的 ISO。当我们获取到各个发行版本的 ISO 后，我们可以正常使用进行安装，但是都是交互式安装，需要我们通过图形化界面/命令行交互的方式进行配置确认及参数设置。如果我们想要 ISO 支持静默安装，那么就需要我们自己定制属于自己的 KickStart 脚本，然后放置到 ISO 中并设置为默认执行项。</description></item><item><title>cgroup 资源预留</title><link>https://zdyxry.github.io/2018/12/18/cgroup-%E8%B5%84%E6%BA%90%E9%A2%84%E7%95%99/</link><pubDate>Tue, 18 Dec 2018 21:15:33 +0000</pubDate><guid>https://zdyxry.github.io/2018/12/18/cgroup-%E8%B5%84%E6%BA%90%E9%A2%84%E7%95%99/</guid><description>cgroup 资源预留 Link to heading 背景 Link to heading 在之前的博客中，提到了 cgroup 中如何为自己的服务配置资源限制，比如 CPU，内存等，当时以为在 cgroup.conf 中配置的服务，那么相应绑定的 CPU 就归属于该服务，也就是资源预留，今天发现并不是这样，记录下如何通过 cgroup 做资源预留。
资源预留 Link to heading 在之前提到的博客中关于资源限制是这么配置的：
[root@node 20:56:49 ~]$cat /etc/cgconfig.conf # yiran cgroups configuration group . { cpuset { cpuset.memory_pressure_enabled = &amp;#34;1&amp;#34;; } } group yiran { cpuset { cpuset.cpus = &amp;#34;0,1,2,3,4,5&amp;#34;; cpuset.mems = &amp;#34;0-1&amp;#34;; cpuset.cpu_exclusive = &amp;#34;1&amp;#34;; cpuset.mem_hardwall = &amp;#34;1&amp;#34;; } } group yiran/test { cpuset { cpuset.cpus = &amp;#34;0&amp;#34;; cpuset.</description></item><item><title>Traceroute 简易实现</title><link>https://zdyxry.github.io/2018/11/18/Traceroute-%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</link><pubDate>Sun, 18 Nov 2018 12:33:35 +0000</pubDate><guid>https://zdyxry.github.io/2018/11/18/Traceroute-%E7%AE%80%E6%98%93%E5%AE%9E%E7%8E%B0/</guid><description>背景 Link to heading 在平时遇到网络问题时，我们通常会使用 ping,route,ip 等命令去 debug，当我们确定我们本机的网络配置及服务没有问题后，我通常会使用 traceroute 来判断网络走向。
最近公司搬家之后，整体网络架构进行了改进，随着配置的复杂化，稳定性相较于原来有了很大的下降，导致最近频繁使用 traceroute，一直使用它却不知道是怎么工作的，研究了一下，作为总结。
Traceroute Link to heading 先上维基百科的解释：
traceroute，现代Linux系统称为tracepath，Windows系统称为tracert，是一种计算机网络工具。它可显示数据包在IP网络经过的路由器的IP地址。
我们通常使用无需特殊配置，直接用 traceroute 加上我们的目标地址即可，如：
root@yiran-workstation:~ $ traceroute 192.168.16.1 traceroute to 192.168.16.1 (192.168.16.1), 30 hops max, 60 byte packets 1 gateway (192.168.8.1) 19.469 ms 19.089 ms 18.911 ms 2 192.168.1.201 (192.168.1.201) 11.539 ms 11.423 ms 11.307 ms 3 192.168.16.1 (192.168.16.1) 18.289 ms 18.184 ms 18.064 ms 当我们想设置 TTL 数值时，我们可以使用 -m 参数:
root@yiran-workstation:~ $ traceroute 192.</description></item><item><title>cgroups 常用配置</title><link>https://zdyxry.github.io/2018/09/23/cgroups-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/</link><pubDate>Sun, 23 Sep 2018 11:40:19 +0000</pubDate><guid>https://zdyxry.github.io/2018/09/23/cgroups-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/</guid><description>背景 Link to heading 在软件运行过程中，我们经常需要限制 CPU 、内存、磁盘的使用，方式程序超出了限定边界范围。在 Linux 中，我们可以通过 cgroups 来进行限制。
cgroups Link to heading 中文名称为控制组群，具体功能分类为：
资源限制：组可以被设置不超过设定的内存限制；这也包括虚拟内存。 优先级：一些组可能会得到大量的CPU或磁盘IO吞吐量。 结算：用来衡量系统确实把多少资源用到适合的目的上。 控制：冻结组或检查点和重启动。 下面来说下常见的使用方式
CPU Link to heading RedHat 官方文档中描述 cgroups 在 RHEL7/CentOS7 之后的版本需要通过 systemd 配置，不再使用 libcgconfig 方式。 但是在 systemd 的配置中，CPU 相关的配置项比较简单，或者说 OS 自动配置了很多，没有暴露出来。所以我们这里还是采用 libcgconfig 的配置方式。
在进行 CPU 限制之前，我们需要了解一下 NUMA 结构。什么是 NUMA？ NUMA 是一种为多处理器的计算机设计的内存，内存访问时间取决于内存相对于处理器的位置。在NUMA下，处理器访问它自己的本地内存的速度比非本地内存快一些。 不同的 Thread 在同一个 Core 上也会发生抢占情况，具体可以通过 sysbench 进行测试。
相关概念定义：
Socket：物理服务器上的 CPU 插槽 Core：物理 CPU 核心数 Thread：超线程 简单的说就是如果你的程序是计算密集型，那么尽可能的要让 CPU 限制在同一个 NUMA node 上。 查看 NUMA node 方式：</description></item><item><title>Linux 安全清理 /boot 分区</title><link>https://zdyxry.github.io/2018/08/09/Linux-%E5%AE%89%E5%85%A8%E6%B8%85%E7%90%86-/boot-%E5%88%86%E5%8C%BA/</link><pubDate>Thu, 09 Aug 2018 15:00:25 +0000</pubDate><guid>https://zdyxry.github.io/2018/08/09/Linux-%E5%AE%89%E5%85%A8%E6%B8%85%E7%90%86-/boot-%E5%88%86%E5%8C%BA/</guid><description>背景 Link to heading 众多 Linux 发行版中，很多为了稳定性要求，Kernel 的版本还处于比较低的版本，比如：3.10.0-xxx（说的就是 CentOS）。 在工作中难免因为某些需求要升级 Kernel，最近遇到多次升级 Kernel 后导致 /boot 分区被占满的情况，特此查找资料记录。
升级方式 Link to heading 在之前的博客中提到过多次，CentOS 发行版升级一般采用 yum update ，Kernel 因为没有相关依赖，也可以直接 rpm -Uvh 的方式升级。
yum update Link to heading 举个例子，比如这台 Linux，目前情况如下：
[root@node111 14:25:10 ~]$uname -a Linux node111 3.10.0-693.11.6.el7.smartx.1.x86_64 #1 SMP Tue Jan 16 22:09:10 CST 2018 x86_64 x86_64 x86_64 GNU/Linux [root@node111 14:25:12 ~]$ll /boot/ total 124929 -rw-r--r--. 1 root root 140924 Jan 16 2018 config-3.10.0-693.11.6.el7.smartx.1.x86_64 drwxr-xr-x.</description></item><item><title>CentOS LiveCD 构建及修改方式</title><link>https://zdyxry.github.io/2018/08/04/CentOS-LiveCD-%E6%9E%84%E5%BB%BA%E5%8F%8A%E4%BF%AE%E6%94%B9%E6%96%B9%E5%BC%8F/</link><pubDate>Sat, 04 Aug 2018 06:39:28 +0000</pubDate><guid>https://zdyxry.github.io/2018/08/04/CentOS-LiveCD-%E6%9E%84%E5%BB%BA%E5%8F%8A%E4%BF%AE%E6%94%B9%E6%96%B9%E5%BC%8F/</guid><description>背景 Link to heading Linux 作为目前服务器占比超过 60% 的操作系统，日常使用过程中，都是采用官方发布的 DVD ISO 安装，可以选择安装桌面版本还是 Minimal 版本。但是如果我们想要验证服务器硬件是否满足产品需求，就需要通过 Live CD 的方式先在内存中运行操作系统，然后进行硬件检测。通过这种方式可以在最小成本（无需安装操作系统到磁盘中）下完成硬件检测。目前 OpenStack 厂商 Mirantis 及云计算厂商 Nutanix 均采用该种方式进行产品安装前校验。要进行校验的第一步，就是构建属于我们自己的 Live CD ISO，下面以 CentOS 为例介绍如何构建和定制 Live CD ISO。
流程 Link to heading 准备构建工具 编写 KickStart 脚本 准备软件源 构建 构建工具 Link to heading 核心工具 livecd-creator。 构建环境为 CentOS 7。指定官方 yum repo：
root@yiran-30-250:/etc/yum.repos.d $ ll 总用量 56K -rw-r--r-- 1 root root 1.7K 1月 15 2018 CentOS-Base.repo -rw-r--r--. 1 root root 1.</description></item><item><title>RPM 常用构建方式</title><link>https://zdyxry.github.io/2018/07/28/RPM-%E5%B8%B8%E7%94%A8%E6%9E%84%E5%BB%BA%E6%96%B9%E5%BC%8F/</link><pubDate>Sat, 28 Jul 2018 06:47:40 +0000</pubDate><guid>https://zdyxry.github.io/2018/07/28/RPM-%E5%B8%B8%E7%94%A8%E6%9E%84%E5%BB%BA%E6%96%B9%E5%BC%8F/</guid><description>背景 Link to heading 作为一个标准化的产品，需要提供简单快捷的软件安装方式，比如 Python pip、 Ubuntu apt-get、SUSE zypper 或者是 CentOS/RHEL yum。都可以让用户快速的安装产品并上手使用，极大的节省了软件安装的时间。因为工作中使用的发行版是 CentOS/RHEL 系列，常用的方式是 RPM。 本文介绍下常用的集中构建 RPM 软件包方式，便于快速上手构建属于自己的 RPM。
RPM 介绍 Link to heading 什么是 RPM？ RPM 全称为 RedHat Package Manager，也就是常见的以 .rpm 为后缀的软件包。属于 RedHat 系列发行版的通用软件包解决方式。 在安装 CentOS(RHEL，以下 CentOS 均可适用于 RHEL)操作系统时，我们可以选择要安装的软件包，默认系统安装为 Minimal 模式，只包含系统必要的 RPM。
理论上操作系统上所有的软件都是通过 RPM 的方式安装的，比如常用的 Kernel、GCC、LS 等工具。
RPM 使用 Link to heading 在进行 RPM 安装时，通常会遇到一个问题：依赖。由于某些特定的软件包在使用时，要求系统必须安装其所依赖的软件才可以正常工作，因此我们需要查看要安装的软件包的依赖。
root@yiran-30-250:~/project/Blog master ✔ $ rpm -qR sos-3.2-35.el7.centos.3.noarch /usr/bin/python bzip2 config(sos) = 3.2-35.el7.centos.3 libxml2-python python(abi) = 2.</description></item><item><title>Linux 主机异常信息收集</title><link>https://zdyxry.github.io/2018/07/06/Linux-%E4%B8%BB%E6%9C%BA%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86/</link><pubDate>Fri, 06 Jul 2018 15:28:09 +0000</pubDate><guid>https://zdyxry.github.io/2018/07/06/Linux-%E4%B8%BB%E6%9C%BA%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86/</guid><description>背景 Link to heading 通常情况下，一个公司内部都会有监控报警平台去支撑业务主机的稳定运行，比如 Nagios，Zabbix 或者 Prometheus 等其他工具。 这些工具，无论是 push 模式还是 pull 模式，都基于一个前提：主机是可访问的，或者说 agent/exporter 是可正常工作的。
如果运维人员在面对一台处于异常状态的主机，应该如何收集该主机的当前状态及相应日志呢？
企业级产品通常会内置工具用来采集异常状态信息，比如：
DELL 服务器如果出现了未知错误，拨打 400 询问问题， 一线客服会告诉你，打开 iDRAC 界面，点击“收集日志” 选项，会自动收集并下载节点信息 如果你是 RedHat RHEL 用户，主机出现异常状态，红帽技术支持工程师进行初始分析时，通常会使用 sosreport 工具 简介 Link to heading sosreport 命令是一个工具，用来收集 RHEL 系统上的配置信息、系统信息和诊断信息。比如：正在运行的内核版本、已加载的模块，以及系统和服务的配置文件。同时，sosreport 支持用户自己编写 Plugin ，用于收集自己想要收集的服务日志及命令结果。
使用 Link to heading sosreport 提供了 sosreport 命令行，用户可以直接执行该命令收集信息，默认收集所有 Plugin 信息。
执行流程 Link to heading 通过 which 命令查看提供 sosreport 命令行的文件，可以看到指向 sos.sosreport.main。
root@yiran-30-250:~/project/sos master ✔ $ which sosreport /usr/sbin/sosreport root@yiran-30-250:~/project/sos master ✔ $ cat /usr/sbin/sosreport #!</description></item><item><title>使用 systemd timer 替换 crontab</title><link>https://zdyxry.github.io/2018/06/28/%E4%BD%BF%E7%94%A8-systemd-timer-%E6%9B%BF%E6%8D%A2-crontab/</link><pubDate>Thu, 28 Jun 2018 07:04:39 +0000</pubDate><guid>https://zdyxry.github.io/2018/06/28/%E4%BD%BF%E7%94%A8-systemd-timer-%E6%9B%BF%E6%8D%A2-crontab/</guid><description>在日常工作中，经常会用到定时任务配合脚本自动处理一些重复性工作，通常我会选择 crontab &amp;amp; flock &amp;amp; script 这样的组合进行配置， 最近了解了 systemd timer ，发现虽然配置上比 crontab 要麻烦一点（需要编写两个配置文件）， 但是其他的优势是 crontab 不具备的。
systemd timer 优势 Link to heading systemd 会自动将定时任务事件记录在 systemd 日志中，可以通过 journalctl 轻松查找 可以配置定时器之间的依赖关系 定时器启动/关闭不需要再注释掉 crontab 中的某一行或移除 /etc/cron.d/ 下的某个文件，而是直接 start/stop/enable/disable 控制 自带锁机制，无需通过 flock 或脚本中添加锁文件的方式控制任务执行与否 使用方式 Link to heading 例如，我想要定时备份数据从开发机到服务器上，通常重要的文件采用 inotify &amp;amp; rsync 的方式同步，优先级较低的采用定时同步方式。
crontab 配置方式 Link to heading 编写 crontab 配置文件 Link to heading crontab -e (或者在 /etc/cron.*/ 对应路径下编写配置文件) 0 1 * * * flock /usr/local/bin/system-backup.</description></item><item><title>CentOS 多网卡多网关配置</title><link>https://zdyxry.github.io/2018/05/29/CentOS-%E5%A4%9A%E7%BD%91%E5%8D%A1%E5%A4%9A%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE/</link><pubDate>Tue, 29 May 2018 19:21:36 +0000</pubDate><guid>https://zdyxry.github.io/2018/05/29/CentOS-%E5%A4%9A%E7%BD%91%E5%8D%A1%E5%A4%9A%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE/</guid><description>背景 Link to heading 日常使用中，开发机通常只配置一块网卡，通常配置为 DHCP 自动获取 IP，这样就可以自动从 DHCP Server 获取 IP，网关，DNS Server 等配置，不用手动配置。
最近在工作中存在一种场景，一台服务器，多块网卡，需要对不同的网卡配置各自网关，查找了一些资料，也踩了一些坑，记录一下过程。
配置过程 Link to heading 官方配置 尝试在 /etc/sysconfig/network-scripts/ifcfg-&amp;lt;device&amp;gt; 中增加 GATEWAY 自动使之生效，实际上来看，多张网卡都配置 GATEWAY 字段会产生冲突，只能放弃通过网卡配置文件方式，尝试通过给网卡配置静态路由方式添加。
在 CentOS 官方网站查找配置静态路由方法，文档中提到通过编写网卡路由配置文件/etc/sysconfig/network-scripts/route-_&amp;lt;interface&amp;gt; ，配置完成后重启网络可以自动生效。
官方这种方式我尝试了两种文件编写方式，一种为192.168.64.0/20 via &amp;lt;gateway&amp;gt; dev &amp;lt;device&amp;gt;，一种为 ADDRESS0=xxxxx; NETMASK0=xxxxx; GATEWAY0=xxxxx。两种配置方式均没有生效，只能放弃。
手动配置 可以通过 route add 命令临时给第二章网卡配置网关，通过 route -n 查看是可以生效。但是因为最终目标是让路由随着网络服务（公司内部未使用 NetworkManager）Network 启动而配置，因此在 /etc/rc.local 中假如 route add 类似命令无法走通。
上述方法要么无法生效，要么存在无法持久化问题，均无法完成需求。 只能通过查看 网卡服务启动脚本来查找，查看 /etc/init.d/network （systemd 服务控制也是执行该文件），该文件是一个 Shell 脚本，找到路由配置相关：
2 # Add non interface-specific static-routes. 1 if [ -f /etc/sysconfig/static-routes ]; then 141 if [ -x /sbin/route ]; then 1 grep &amp;#34;^any&amp;#34; /etc/sysconfig/static-routes | while read ignore args ; do 2 /sbin/route add -$args 3 done 4 else 5 net_log $&amp;#34;Legacy static-route support not available: /sbin/route not found&amp;#34; 6 fi 7 fi 可以看到，在网络服务 network 启动过程中，会判断 /etc/sysconfig/static-routes 文件是否存在，如果存在则会通过 route 命令对该文件中记录以 any 开头的路由进行添加，添加命令为 /sbin/route add -$args。</description></item></channel></rss>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Yiran's Blog</title><link>https://zdyxry.github.io/tags/Python/</link><description>Recent content in Python on Yiran's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 09 May 2022 14:44:42 +0000</lastBuildDate><atom:link href="https://zdyxry.github.io/tags/Python/atom.xml" rel="self" type="application/rss+xml"/><item><title>Celery/Kombu MongoDB 连接异常调查记录</title><link>https://zdyxry.github.io/2022/05/09/Celery/Kombu-MongoDB-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E8%B0%83%E6%9F%A5%E8%AE%B0%E5%BD%95/</link><pubDate>Mon, 09 May 2022 14:44:42 +0000</pubDate><guid>https://zdyxry.github.io/2022/05/09/Celery/Kombu-MongoDB-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E8%B0%83%E6%9F%A5%E8%AE%B0%E5%BD%95/</guid><description>背景 链接到标题 产品组件 JobCenter 使用 Celery 实现异步任务中心，同时会运行 job-center-worker （celery worker） 和 job-center-scheduler(celery beat) 两个进程，使用 MongoDB 作为 Backend 存储 message 等信息（Celery 官方已说明不再维护对 MongoDB 的支持）。其中 MongoDB 配置了 ReplicaSet 保证高可用。
近期 Celery/Kombu 中遇到了 No free channel ids 问题，经过排查在这个 PR 中解决了该问题，在考虑 cherry-pick 的工作量和可维护性考虑，最终将产品中的 celery 和 kombu 组件从 3.x 统一升级到了 4.x 版本。
测试同学反馈近期在进行可靠性测试时，发现将 MongoDB 节点的存储网络 ifdown 会导致 JobCenter hang. 针对该问题进行调查。
调查 链接到标题 Celery 链接到标题 先尝试复现该问题，首先尝试 ifdown Primary 节点存储网络，现象复现；尝试 ifdown Secondary 节点存储网络，无法复现； 尝试 stop MongoDB service 替代 ifdown，Primary 或 Secondary 均无法复现。推测与 MongoDB 连接处理有关。</description></item><item><title>Gunicorn 信号处理</title><link>https://zdyxry.github.io/2021/06/27/Gunicorn-%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</link><pubDate>Sun, 27 Jun 2021 18:55:41 +0000</pubDate><guid>https://zdyxry.github.io/2021/06/27/Gunicorn-%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</guid><description>背景 链接到标题 最近在项目中使用到了 Gunicorn 的 Graceful Shutdown 功能，阅读代码学习一下 Gunicorn 的信号处理。
Master 链接到标题 Gunicorn 启动入口：
WSGIApplication(&amp;#34;%(prog)s [OPTIONS] [APP_MODULE]&amp;#34;).run() BaseApplication().run() Arbiter(self).run() Master 主要控制逻辑实现在 Arbiter 中，包括信号处理和主循环逻辑。在调用 Arbiter().run() 会最终调用到 Arbiter.init_signals() ，在该函数中会将 Master 中定义的需要处理的信号函数进行相应的注册：
Arbiter().run(): Arbiter().start() Arbiter().init_signals() # 初始化 Master 信号处理 # initialize all signals for s in self.SIGNALS: # &amp;#34;HUP QUIT INT TERM TTIN TTOU USR1 USR2 WINCH&amp;#34; signal.signal(s, self.signal) signal.signal(signal.SIGCHLD, self.handle_chld) 所有定义的信号处理函数都是 Aribiter().signal() ，该函数将接收到的信号存放在 Arbiter.SIG_QUEUE 中，最多保留 5 个信号，然后触发 Arbiter().wakeup() ，在 Arbiter().wakeup() 中，向 Arbiter.</description></item><item><title>zipstreamer 源码阅读</title><link>https://zdyxry.github.io/2020/02/21/zipstreamer-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Fri, 21 Feb 2020 19:33:08 +0000</pubDate><guid>https://zdyxry.github.io/2020/02/21/zipstreamer-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 链接到标题 在之前的一篇博客《HTTP Content-Length 学习》 中提到自己踩了一个坑，就是 content-length 与实际大小不匹配导致文件下载失败，在解决过程中用到了 zipstreamer ，今天来看看 zipstreamer 是如何工作的。
zipfile 链接到标题 Python 标准库中提供了 zipfile 用来对 Zip 文件进行操作，可以进行 Zip 的创建、写入、读取、解压等动作，但是 zipfile 只能对文件进行操作，没办法传入 stream，所以能做的操作有限。
示例 链接到标题 import zipfile def test(): for i in range(1, 4): f = open(&amp;#34;file&amp;#34; + str(i) + &amp;#34;.txt&amp;#34;, &amp;#39;w&amp;#39;) f.write(str(i)) f.close() f = zipfile.ZipFile(&amp;#39;filename.zip&amp;#39;, &amp;#39;w&amp;#39;, zipfile.ZIP_DEFLATED) f.write(&amp;#39;file1.txt&amp;#39;) f.write(&amp;#39;file2.txt&amp;#39;) f.write(&amp;#39;file3.txt&amp;#39;) f.close() f = zipfile.ZipFile(&amp;#39;filename.zip&amp;#39;) f.extractall() f.close() if __name__ == &amp;#34;__main__&amp;#34;: test() zipstreamer 链接到标题 在提供文件下载接口时，有一个比较常见的需求是传过来一个 stream，然后我们要将 stream 作为 Zip 中的一个文件转发出去，实时下载，这时候就需要 zipstreamer 来实现了。</description></item><item><title>如何捕捉 Ctrl+C 指令</title><link>https://zdyxry.github.io/2019/11/23/%E5%A6%82%E4%BD%95%E6%8D%95%E6%8D%89-Ctrl-C-%E6%8C%87%E4%BB%A4/</link><pubDate>Sat, 23 Nov 2019 21:23:24 +0000</pubDate><guid>https://zdyxry.github.io/2019/11/23/%E5%A6%82%E4%BD%95%E6%8D%95%E6%8D%89-Ctrl-C-%E6%8C%87%E4%BB%A4/</guid><description>背景 链接到标题 有时候在运行代码的时候，想要程序在接收到 Ctrl+C 指令的时候做一些平滑的处理，来写一下在 Python 和 Golang 中如何接收 Ctrl+C 指令。
Python 链接到标题 import signal import time import sys def run_program(): while True: print(&amp;#34;Test code...&amp;#34;) time.sleep(1) def exit_gracefully(signum, frame): signal.signal(signal.SIGINT, original_sigint) print(&amp;#34;Receive Ctrl+C.&amp;#34;) sys.exit(1) if __name__ == &amp;#39;__main__&amp;#39;: original_sigint = signal.getsignal(signal.SIGINT) signal.signal(signal.SIGINT, exit_gracefully) run_program() Golang 链接到标题 package main import ( &amp;#34;fmt&amp;#34; &amp;#34;os&amp;#34; &amp;#34;os/signal&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) func WaitForCtrlC() { var end_waiter sync.WaitGroup end_waiter.Add(1) var signal_channel chan os.Signal signal_channel = make(chan os.</description></item><item><title>httprunner 源码阅读</title><link>https://zdyxry.github.io/2019/09/06/httprunner-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Fri, 06 Sep 2019 19:50:27 +0000</pubDate><guid>https://zdyxry.github.io/2019/09/06/httprunner-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 链接到标题 最近工作上每天疲于应付各种事情，周末实在不想继续做工作相关的事情，想起一直想了解的自动化测试框架 httprunner，就阅读下。之前一直有关注作者 debugtalk 的博客，收获很多。
随着公司的发展，自己也做过很多的工作，其中就包含测试，但是自己当时大部分都是手工测试，虽然会针对其中的部分进行代码编写，但是不成体系。虽然后来就没有继续负责测试工作了，但是对于测试还是很感兴趣，平时开发过程中，最多也就是使用 unittest 或 pytest 来编写单测，这次通过阅读 httprunner 代码来感受下测试框架。
P.S. 在使用及了解 httprunner 之前，最好先了解下 unittest。
httprunner 链接到标题 HttpRunner 是一款面向 HTTP(S) 协议的通用测试框架，只需编写维护一份 YAML/JSON 脚本，即可实现自动化测试、性能测试、线上监控、持续集成等多种测试需求。
一句话总结就是 api 自动化测试，其中用到了以下的开源项目：
requests locust unittest &amp;hellip; requests 和 unittest 可以说是 python 开发者用的比较多的两个项目。locust 是一个 api 压力测试，这个我们公司也有用到。
介绍完了项目，我们来跟着官方文档了解运行流程。
执行流程 链接到标题 文档中的 快速上手 章节与章节名称很配，真心是 快速上手 ，通过一个又一个的示例来了解具体的功能使用，循序渐进，简直完美。不过又一点不好的地方是 demo 示例的代码不再 httprunner 中，而是在 docs 项目中，使用起来不是很方便，如果有 Dockerfile 来支撑，就更好了。
在 httprunner 项目中，项目的包管理是通过 poetry 进行的，比 setuptools 要清晰很多。
首先来看命令，httprunner 随着项目的演进，支持的命令行有 4个，其中 3 个是重复的，1个是压力测试：</description></item><item><title>Heap In Python &amp; Golang</title><link>https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/</link><pubDate>Sun, 28 Jul 2019 12:11:45 +0000</pubDate><guid>https://zdyxry.github.io/2019/07/28/Heap-In-Python-Golang/</guid><description>背景 链接到标题 最近使用到了 heap 这个数据结构，记录一下在 Python 和 Golang 中最基本的使用方法～
堆（英语：Heap）是计算机科学中的一種特別的樹狀数据结构。若是滿足以下特性，即可稱為堆積：「給定堆積中任意節點P和C，若P是C的母節點，那麼P的值會小於等於（或大於等於）C的值」。若母節點的值恆小於等於子節點的值，此堆積稱為最小堆積（min heap）；反之，若母節點的值恆大於等於子節點的值，此堆積稱為最大堆積（max heap）。在堆積中最頂端的那一個節點，稱作根節點（root node），根節點本身沒有母節點（parent node）。
Python 链接到标题 create 链接到标题 In [1]: import heapq In [2]: a = [1,3,2,5,4] In [3]: heapq.heapify(a) In [4]: a Out[4]: [1, 3, 2, 5, 4] In [5]: b = [] In [6]: heapq.heappu heapq.heappush heapq.heappushpop In [6]: heapq.heappush(b, 1) In [7]: heapq.heappush(b, 3) In [8]: heapq.heappush(b, 2) In [9]: heapq.heappush(b, 5) In [10]: heapq.heappush(b, 4) In [11]: b Out[11]: [1, 3, 2, 5, 4] read 链接到标题 In [28]: b Out[28]: [1, 3, 2, 5, 4] In [29]: heapq.</description></item><item><title>PySnooper 源码阅读</title><link>https://zdyxry.github.io/2019/04/27/PySnooper-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Sat, 27 Apr 2019 20:14:28 +0000</pubDate><guid>https://zdyxry.github.io/2019/04/27/PySnooper-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 链接到标题 在 18 年的时候 jiajun 同学发过一篇博客，讲如何调试相关的总结。结合最近自己的经验，紧靠 logging 和 print 就能解决日常的 80%问题，剩下的 20% 也都可以通过review 代码来解决，我只有当确实没什么思路的时候，才会采用 pdb 的方式去调试。之所以先 review 代码再采用 pdb 的方式是想确认自己已经理清了相关代码的上下文和逻辑，不至于在单步调试的时候出现 恍然大悟 （贬义） 的状况。
最近两天 Github 上关于 Python 的项目最火的就是 PySnooper，这个项目的 Slogan 就是 Never use print for debugging again ，这里的 print 替换为 logging 也没啥差。整个代码在初步可用阶段代码量很少，也确实能够给平时写些小脚本带来便利，便抽时间看了看具体的实现。
PySnooper 链接到标题 先来看下目录结构：
yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/PySnooper 3d0d051 ✗ $ tree . . ├── LICENSE ├── MANIFEST.in ├── README.md ├── make_release.sh ├── misc │ └── IDE\ files │ └── PySnooper.</description></item><item><title>apscheduler 源码阅读</title><link>https://zdyxry.github.io/2019/04/06/apscheduler-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Sat, 06 Apr 2019 10:58:49 +0000</pubDate><guid>https://zdyxry.github.io/2019/04/06/apscheduler-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>简介 链接到标题 apscheduler 全称 Advanced Python Scheduler，调度器，主要功能如下：
动态添加、删除任务 暂停、恢复任务 周期性调度：cron,date,interval &amp;hellip; 那么接下来我们根据官方示例，看看 apscheduler 是如何进行处理任务的。
示例版本为 2.1，因为在 2.1 版本包含目前 master 分支上的主要功能，简单易懂。
代码结构如下：
yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/apscheduler 2.1 ✔ $ tree apscheduler apscheduler ├── __init__.py ├── events.py ├── job.py ├── jobstores │ ├── __init__.py │ ├── base.py │ ├── mongodb_store.py │ ├── ram_store.py │ ├── redis_store.py │ ├── shelve_store.py │ └── sqlalchemy_store.py ├── scheduler.py ├── threadpool.py ├── triggers │ ├── __init__.py │ ├── cron │ │ ├── __init__.</description></item><item><title>记一次 Python 编码踩坑</title><link>https://zdyxry.github.io/2019/04/02/%E8%AE%B0%E4%B8%80%E6%AC%A1-Python-%E7%BC%96%E7%A0%81%E8%B8%A9%E5%9D%91/</link><pubDate>Tue, 02 Apr 2019 07:37:59 +0000</pubDate><guid>https://zdyxry.github.io/2019/04/02/%E8%AE%B0%E4%B8%80%E6%AC%A1-Python-%E7%BC%96%E7%A0%81%E8%B8%A9%E5%9D%91/</guid><description>背景 链接到标题 一直知道 Python 容易踩编码的坑，尤其是 Python2，昨天第一次遇到，记一下。
起因 链接到标题 产品中有一个账号关联的功能，需要的参数大概有 host,port,user,passwd 这么几个参数，昨天发现一个环境中账号关联失败，看请求应该还没到账号认证那里就失败了，查看 rest-server 日志，并没有发现错误异常，api 也是正常返回的，通过其他方式验证账号是有效的，当时觉得很奇怪，没什么想法。
调查 链接到标题 既然 rest-server 中日志没有报错，那么看看服务是否有什么异常。 这里特意说名下，如果服务使用的是 gunicorn 或者 celery 等第三方库作为守护进程，有一些系统报错是不会记录到你的服务中的，而是会直接打印到系统中（messages or systemd）。
发现 systemctl status 和 journal -u 有报错，报错内容如下：
4月 02 06:20:00 SCVM70 gunicorn[31225]: Traceback (most recent call last): 4月 02 06:20:00 SCVM70 gunicorn[31225]: File &amp;#34;/usr/lib64/python2.7/site-packages/gevent/threadpool.py&amp;#34;, line 207, in _worker 4月 02 06:20:00 SCVM70 gunicorn[31225]: value = func(*args, **kwargs) 4月 02 06:20:00 SCVM70 gunicorn[31225]: error: getaddrinfo() argument 2 must be integer or string 4月 02 06:20:00 SCVM70 gunicorn[31225]: (&amp;lt;ThreadPool at 0x120ae50 0/5/10&amp;gt;, &amp;lt;built-in function getaddrinfo&amp;gt;) failed with error 字面意思是传递参数的类型不对，必须为 int 或者 string。</description></item><item><title>huey 源码阅读</title><link>https://zdyxry.github.io/2019/03/31/huey-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Sun, 31 Mar 2019 11:25:48 +0000</pubDate><guid>https://zdyxry.github.io/2019/03/31/huey-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 链接到标题 最近读完了 《Redis 实战》对 Redis 有了一些了解，但是没有在实际项目中应用过，就想找一个使用 Redis 的项目来看看，找到 Huey 是因为之前使用过，趁机了解下具体实现。
简介 链接到标题 Huey 的定位是一个轻量级的任务队列，仅依赖于 Redis 作为任务相关信息存储，支持的功能有：
多种 worker 执行方式：thread，process，greenlet 支持多种任务类型：特定时间运行，周期性运行 包含重试机制，可以指定重试次数及重试间隔 支持任务锁 &amp;hellip; 我们根据官方的示例，来看看 Huey 是如何处理任务的，目录结构如下：
master ✔ $ pwd /Users/yiran/Documents/git-repo/huey/examples/simple yiran@zhouyirandeMacBook-Pro:~/Documents/git-repo/huey/examples/simple master ✔ $ tree . . ├── README ├── __init__.py ├── config.py ├── cons.sh ├── main.py └── tasks.py 注意，这个目录结构是 Huey 官方建议的，具体原因为：
Behind-the-scenes when you decorate a function with task() or periodic_task(), the function registers itself with a centralized in-memory registry.</description></item><item><title>Python 调用 systemd watchdog 方法</title><link>https://zdyxry.github.io/2019/03/10/Python-%E8%B0%83%E7%94%A8-systemd-watchdog-%E6%96%B9%E6%B3%95/</link><pubDate>Sun, 10 Mar 2019 10:21:19 +0000</pubDate><guid>https://zdyxry.github.io/2019/03/10/Python-%E8%B0%83%E7%94%A8-systemd-watchdog-%E6%96%B9%E6%B3%95/</guid><description>systemd 链接到标题 在之前的博客中介绍过 systemd 的基本使用及通过 timer 来替换 crontab 的方法，今天来说一下如何调用 watchdog。
在 systemd 中，提供 watchdog 来检测服务状态状态，官方文档中描述这个功能为 &amp;ldquo;keep-alive ping&amp;rdquo;，我们可以在服务的启动配置中，添加 WatchdogSec 来指定 timeout 时间，在服务程序中通过发送 WATCHDOG=1 来不断的通知 systemd，服务处于正常状态，当超过 timeout 时间未收到 WATCHDOG=1 信号后，systemd 会根据 Restart 配置，决定是否自动重启服务。
示例 链接到标题 服务程序：
root@yiran-30-250:/usr/lib/systemd/system $ cat /root/project/watchdog/test.py #!/usr/bin/python # coding:utf-8 import os import time import socket import logging print(&amp;#34;Test starting up...&amp;#34;) time.sleep(1) # 模拟执行真实业务 print(&amp;#34;Test startup finished&amp;#34;) try: sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) addr = os.getenv(&amp;#34;NOTIFY_SOCKET&amp;#34;) # systemd default addr = &amp;#34;/run/systemd/notify&amp;#34; if addr and addr[0] == &amp;#34;@&amp;#34;: addr = &amp;#34;\0&amp;#34; + addr[1:] except Exception: logging.</description></item><item><title>Python 生成器使用</title><link>https://zdyxry.github.io/2019/03/09/Python-%E7%94%9F%E6%88%90%E5%99%A8%E4%BD%BF%E7%94%A8/</link><pubDate>Sat, 09 Mar 2019 21:47:35 +0000</pubDate><guid>https://zdyxry.github.io/2019/03/09/Python-%E7%94%9F%E6%88%90%E5%99%A8%E4%BD%BF%E7%94%A8/</guid><description>背景 链接到标题 在清理 Pocket 列表的时候，发现自己很早之前收藏过 dabeaz 在 2008 年 PyCon 关于生成器的 PPT 讲解，今天读完，有所收获。
在 PPT 中， dabeaz 通过一个具体的文件处理的例子，一步一步的讲解了程序的演进，具体代码可以在 Github 查看。
生成器 链接到标题 使用 yield 关键字的函数就是生成器。生成器在运行时生成值，所以只能迭代一次。生成器可以通过 next 关键字执行，通常我们通过 for 循环来迭代生成器，可以自动处理 StopIteration 情况。
一个简单的生成器例子：
def countdown(n): while n &amp;gt; 0: yield n n -= 1 &amp;gt;&amp;gt;&amp;gt; for i in countdown(5): ... print(i, end=&amp;#39; &amp;#39;) ... 5 4 3 2 1 当我们调用生成器时，仅返回一个生成器对象，不会执行函数内容，只有当执行 __next__() 时函数才会真正执行。yield 会返回给调用者当前值，同时暂停执行，等待下一次调用 __next__() 继续执行。
协程 链接到标题 在 python 中通过生成器的方式来实现协程：</description></item><item><title>Python socket 编程</title><link>https://zdyxry.github.io/2018/09/30/Python-socket-%E7%BC%96%E7%A8%8B/</link><pubDate>Sun, 30 Sep 2018 20:33:00 +0000</pubDate><guid>https://zdyxry.github.io/2018/09/30/Python-socket-%E7%BC%96%E7%A8%8B/</guid><description>背景 链接到标题 平时工作很少涉及到 Socket 相关，基本上都是 HTTP 之上的业务，最近看到 Real Python 的一篇博客，非常详细的讲解了 Python 下的 socket 编程，其中有两个示例觉得很好，帮助我理解了一些要点，记录一下。
多连接情况 链接到标题 Server 链接到标题 multiconn-server.py
#!/usr/bin/env python3 import sys import socket import selectors import types sel = selectors.DefaultSelector() def accept_wrapper(sock): conn, addr = sock.accept() # 前提条件：可读状态 print(&amp;#39;accepted connection from&amp;#39;, addr) conn.setblocking(False) # 置为非阻塞 data = types.SimpleNamespace(addr=addr, inb=b&amp;#39;&amp;#39;, outb=b&amp;#39;&amp;#39;) events = selectors.EVENT_READ | selectors.EVENT_WRITE sel.register(conn, events, data=data) # 注册事件到 select def service_connection(key, mask): sock = key.fileobj data = key.</description></item><item><title>Python logging 最佳实践-译</title><link>https://zdyxry.github.io/2018/07/22/Python-logging-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-%E8%AF%91/</link><pubDate>Sun, 22 Jul 2018 09:50:47 +0000</pubDate><guid>https://zdyxry.github.io/2018/07/22/Python-logging-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-%E8%AF%91/</guid><description>Python Logging 最佳实践
背景 链接到标题 最近在工作中要新增一个服务，需要处理服务日志，之前每次使用 logging 都是很简单的配置，没有 了解过最佳实践， google 到了 fangpenlin 写的一篇博客（https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/）， 很受用，因此翻译过来便于自己以后回顾使用，也希望大家能够正确配置日志，对运维同学友好些~
Good Logging practice in Python 链接到标题 在显示中，日志是重要的。当你在转账时，它们是转账记录。当飞机起飞时，黑匣子会记录所有事情。 如果某些事情发生了错误，人们可以通过阅读日志有机会弄清楚发生了什么。同样，在系统开发、排查 和运行中日志也是重要的。当你编写了一个服务，日志就是必须品。如果没有日志，我几乎不可能发现 错误如果服务已经挂掉。不仅仅是服务，日志对于桌面应用也是重要的。举个例子，当你的程序在你 客户电脑崩溃了，你可以让他们发送日志文件给你，你才有可能去查找为什么崩溃。相信我，你永远 不知道在不同的电脑环境中会发生什么奇怪的问题.
Print 不是个好主意 链接到标题 尽管日志很重要，但是不是所有的开发者都知道如何正确的使用。我见过一些开发者在开发过程中插入 一些 print ，在开发结束时删除这些 print。向下面这样：
print &amp;#39;Start reading database&amp;#39; records = model.read_recrods() print &amp;#39;# records&amp;#39;, records print &amp;#39;Updating record ...&amp;#39; model.update_records(records) print &amp;#39;done&amp;#39; 当程序是一个简单的脚本时这是工作的，但是对于一个复杂的系统，你最好不要通过这种途径记录日志。 第一，你在日志中无法记录重要的信息，你可能看到很多垃圾信息在日志中，但是没有找到任何有用的。 你也不能通过不修改代码的方式控制日志的输出，还有可能在不使用这些 print 的时候忘记删除掉。 所有的 print 信息都将输出到标准输出中，这个是要禁止的。当然你可以输出到标准错误输出中，但是 这仍不是记录日志的最佳实践。
使用 Python 标准库 Logging 链接到标题 所以，如何正确的记录日志？ 很简单，使用标准库中的 logging 模块。感谢 Python 社区，logging 在标准库中，它被设计的灵活且易用。你可以使用 logging 像下面这样：</description></item><item><title>Flask 流式响应</title><link>https://zdyxry.github.io/2018/07/08/Flask-%E6%B5%81%E5%BC%8F%E5%93%8D%E5%BA%94/</link><pubDate>Sun, 08 Jul 2018 09:44:24 +0000</pubDate><guid>https://zdyxry.github.io/2018/07/08/Flask-%E6%B5%81%E5%BC%8F%E5%93%8D%E5%BA%94/</guid><description>背景 链接到标题 在 web 场景下，经常会碰到下载文件的需求，通常小文件我们会采用 Flask send_file 或者 send_from_directory的方式，下载，但是当下载的文件是一个大压缩文件（&amp;gt;1GiB）时，这种方式就显得不友好了，我们需要采用流式下载的方式返回给客户端。
流式下载 链接到标题 简单实现：
from flask import Response @app.route(&amp;#34;/download/&amp;lt;file_path&amp;gt;&amp;#34;, methods=[&amp;#34;GET&amp;#34;]) def download(file_path): def generate(): if not os.path.exists(file_path): raise &amp;#34;File not found.&amp;#34; with open(file_path, &amp;#34;rb&amp;#34;) as f: while True: chunk = f.read(chunk_size=10 * 1024 * 1024) if not chunk: break yield chunk return Response(generate(), content_type=&amp;#34;application/octet-stream&amp;#34;) 运行 Flask app，可以正确下载文件，但是下载只有实时速度，没有文件总大小，导致无法知道下载进度，也没有文件类型，这些我们都可以通过增加 header 字段实现：
response = Response(generate(), mimetype=&amp;#39;application/gzip&amp;#39;) response.headers[&amp;#39;Content-Disposition&amp;#39;] = &amp;#39;attachment; filename={}.tar.gz&amp;#39;.format(&amp;#34;download_file&amp;#34;) response.headers[&amp;#39;content-length&amp;#39;] = os.stat(str(file_path)).st_size return response 这样，我们下载文件就可以看到文件类型、文件总大小及已下载大小了，其中 mimetype 根据实际压缩文件类型修改匹配即可。</description></item><item><title>将 CSV 文件转换为嵌套 Json 文件</title><link>https://zdyxry.github.io/2018/07/07/%E5%B0%86-CSV-%E6%96%87%E4%BB%B6%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%B5%8C%E5%A5%97-Json-%E6%96%87%E4%BB%B6/</link><pubDate>Sat, 07 Jul 2018 15:40:01 +0000</pubDate><guid>https://zdyxry.github.io/2018/07/07/%E5%B0%86-CSV-%E6%96%87%E4%BB%B6%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%B5%8C%E5%A5%97-Json-%E6%96%87%E4%BB%B6/</guid><description>使用方法 链接到标题 master ✗ $ python csv2json.py --help Usage: csv2json.py [OPTIONS] Convert csv file to json file. Options: --csv_file TEXT Input csv file abspath --json_file TEXT Output json file abspath --help Show this message and exit. 如果执行成功，命令行输出会像下面示例一样。
master ✗ $ python csv2json.py --csv_file ./test.csv --json_file ./yiran-test.json Convert csv file success, json file path is ./yiran-test.json 如果执行失败，则会提示具体失败原因。如：csv 文件无法找到。
master ✗ $ python csv2json.py --csv_file ./tes.csv --json_file ./yiran-test.json Failed to convert csv file with error [Errno 2] No such file or directory: u&amp;#39;.</description></item><item><title>将 Json 转换为 Python Object(一)</title><link>https://zdyxry.github.io/2018/06/09/%E5%B0%86-Json-%E8%BD%AC%E6%8D%A2%E4%B8%BA-Python-Object%E4%B8%80/</link><pubDate>Sat, 09 Jun 2018 11:43:49 +0000</pubDate><guid>https://zdyxry.github.io/2018/06/09/%E5%B0%86-Json-%E8%BD%AC%E6%8D%A2%E4%B8%BA-Python-Object%E4%B8%80/</guid><description>$ cat example.py import json class JSONObject: def __init__(self, dict): vars(self).update(dict) # this is valid json string data = &amp;#39;{&amp;#34;channel&amp;#34;:{&amp;#34;lastBuild&amp;#34;:&amp;#34;2013-11-12&amp;#34;, &amp;#34;component&amp;#34;:[&amp;#34;test1&amp;#34;, &amp;#34;test2&amp;#34;]}}&amp;#39; jsonobject = json.loads(data, object_hook=JSONObject) print(jsonobject.channel.component[0]) print(jsonobject.channel.lastBuild) $ python example.py test1 2013-11-12</description></item></channel></rss>
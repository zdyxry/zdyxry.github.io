<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on Yiran's Blog</title><link>https://zdyxry.github.io/tags/Kubernetes/</link><description>Recent content in Kubernetes on Yiran's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 15 Feb 2020 11:22:13 +0000</lastBuildDate><atom:link href="https://zdyxry.github.io/tags/Kubernetes/atom.xml" rel="self" type="application/rss+xml"/><item><title>K8s drain 命令源码阅读</title><link>https://zdyxry.github.io/2020/02/15/K8s-drain-%E5%91%BD%E4%BB%A4%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Sat, 15 Feb 2020 11:22:13 +0000</pubDate><guid>https://zdyxry.github.io/2020/02/15/K8s-drain-%E5%91%BD%E4%BB%A4%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 Link to heading 之前写过一篇 《Kubernetes 实战-平滑移除节点》 讲如何从 K8s 集群中移除节点的，今天来看看 kubectl drain 命令具体做了什么，怎么实现的。
kubectl Link to heading drain 相关命令都属于 kubectl 的自命令，因此需要先看下 kubectl 的入口，K8s 使用 cobra 作为命令行构建组建（我自己使用 cobra 觉得不怎么好用，而且文档也不清晰。。），统一入口在 cmd/kubectl/kubectl.go ，实际的处理逻辑在 pkg/kubectl/cmd/cmd.go 中
... groups := templates.CommandGroups{ { Message: &amp;#34;Basic Commands (Beginner):&amp;#34;, ... }, { Message: &amp;#34;Deploy Commands:&amp;#34;, ... }, { Message: &amp;#34;Cluster Management Commands:&amp;#34;, Commands: []*cobra.Command{ certificates.NewCmdCertificate(f, ioStreams), clusterinfo.NewCmdClusterInfo(f, ioStreams), top.NewCmdTop(f, ioStreams), drain.NewCmdCordon(f, ioStreams), drain.NewCmdUncordon(f, ioStreams), drain.NewCmdDrain(f, ioStreams), taint.NewCmdTaint(f, ioStreams), }, }, .</description></item><item><title>为什么 flannel.1 丢失后不会自动重建</title><link>https://zdyxry.github.io/2020/01/03/%E4%B8%BA%E4%BB%80%E4%B9%88-flannel.1-%E4%B8%A2%E5%A4%B1%E5%90%8E%E4%B8%8D%E4%BC%9A%E8%87%AA%E5%8A%A8%E9%87%8D%E5%BB%BA/</link><pubDate>Fri, 03 Jan 2020 22:20:37 +0000</pubDate><guid>https://zdyxry.github.io/2020/01/03/%E4%B8%BA%E4%BB%80%E4%B9%88-flannel.1-%E4%B8%A2%E5%A4%B1%E5%90%8E%E4%B8%8D%E4%BC%9A%E8%87%AA%E5%8A%A8%E9%87%8D%E5%BB%BA/</guid><description>背景 Link to heading 在用 K8s 的同学应该多少都使用过 Flannel 作为自己的网络插件，不讨论性能稳定性，在复杂的网络环境配置中 Flannel 的要求应该是最低的，所以我通常使用 Flannel 作为 让 K8s Ready 的最后一步。
在使用过程中，遇到过多次 flannel.1 这个 link 消失的情况，查看官方 Issue 中有人提到过： flannel.1 is deleted by service network restart, and never recreated again. ，但是这个 Issue 从 2017年创建一直到现在都处于 Open 状态，看上去社区也不打算去解决，其实不只是重启网络，如果没有特殊指定的话，找到默认网关所在的网卡，直接 ifdown ，flannel.1 也会丢失，并且不会重建，那为什么会出现这个问题，今天来看一看。
CNI Flannel Plugin Link to heading 我们常说的 Flannel 分为两部分：CNI Flannel Plugin 及 Flannel。
CNI Flannel Plugin 是 Flannel CNI 插件的具体接口实现， CNI 要求实现的 cmdAdd cmdDel cmdCheck 都是在这里实现的，来看看具体的调用流程：</description></item><item><title>Kubernetes 实战-Cluster API 升级流程</title><link>https://zdyxry.github.io/2019/12/22/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API-%E5%8D%87%E7%BA%A7%E6%B5%81%E7%A8%8B/</link><pubDate>Sun, 22 Dec 2019 16:53:55 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/22/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API-%E5%8D%87%E7%BA%A7%E6%B5%81%E7%A8%8B/</guid><description>背景 Link to heading 之前已经介绍过 ClusterAPI 及相应实现方式，但是针对使用 ClusterAPI 部署的 K8s 集群社区中一直没有升级方案，其中 vmware 实现了一个简单的升级工具，可以在社区没实现之前提供使用，今天来看下这个工具是如何实现的。
cluster-api-upgrade-tool Link to heading 项目地址：https://github.com/vmware/cluster-api-upgrade-tool
因为这只是一个单独的工具，因此代码结构比较简单：
yiran@t480:~/go/src/github.com/vmware/cluster-api-upgrade-tool master ✔ $ tree . . ├── CODE-OF-CONDUCT.md ├── CONTRIBUTING.md ├── Dockerfile ├── go.mod ├── go.sum ├── hack │ └── tools │ ├── go.mod │ ├── go.sum │ └── main.go ├── LICENSE.txt ├── main.go # 命令行入口 ├── Makefile ├── NOTICE.txt ├── pkg │ ├── internal │ │ └── kubernetes │ │ ├── client.</description></item><item><title>Kubernetes 实战-高可用集群部署（无LB）</title><link>https://zdyxry.github.io/2019/12/19/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%A0LB/</link><pubDate>Thu, 19 Dec 2019 21:38:16 +0000</pubDate><guid>https://zdyxry.github.io/2019/12/19/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%A0LB/</guid><description>背景 Link to heading 之前写过一篇《Kubernetes 实战-高可用集群部署》 博客讲 K8S 高可用配置，当时采用的方式是使用 keepalived 配合 HAProxy 自建 LB 的方式，但是最近发现无论是 Kubespray 还是 Rancher RKE 都没有采用这种方式，而是在 worker node 上配置 nginx/haproxy 代理 APIServer 达到目的。今天来手动配置下这种方式，了解下注意事项。
本文配置方式参考 Kubespray，主要是想加深自己对这方面的理解，如果只是单纯的想要搭建环境，那么直接使用 Kubespray 就好。
环境 Link to heading IP role 192.168.17.11 ControlPlane1 192.168.17.12 ControlPlane2 192.168.17.13 ControlPlane3 192.168.17.14 WorkerNode1 准备工作 Link to heading 软件安装 Link to heading 这部分准备工作与之前一样，没什么不同，因此不详细说明，需要安装 kubectl,kubeadm,kubelet 。
部署 Link to heading ControlPlane1 Link to heading kubeadm 配置文件如下：
[root@install1 17:55:09 tmp]$cat kubeadm.</description></item><item><title>Kubernetes 实战-Operator Finalizers 实现</title><link>https://zdyxry.github.io/2019/09/13/Kubernetes-%E5%AE%9E%E6%88%98-Operator-Finalizers-%E5%AE%9E%E7%8E%B0/</link><pubDate>Fri, 13 Sep 2019 20:24:41 +0000</pubDate><guid>https://zdyxry.github.io/2019/09/13/Kubernetes-%E5%AE%9E%E6%88%98-Operator-Finalizers-%E5%AE%9E%E7%8E%B0/</guid><description>背景 Link to heading 最近在写 k8s Operator，在看示例的时候看到 controller 都会设置 Finalizers，今天来聊一聊 Finalizers 和相关实现。
Finalizers Link to heading Finalizers 允许 Operator 控制器实现异步的 pre-delete hook。比如你给 API 类型中的每个对象都创建了对应的外部资源，你希望在 k8s 删除对应资源时同时删除关联的外部资源，那么可以通过 Finalizers 来实现。
Finalizers 是由字符串组成的列表，当 Finalizers 字段存在时，相关资源不允许被强制删除。存在 Finalizers 字段的的资源对象接收的第一个删除请求设置 metadata.deletionTimestamp 字段的值， 但不删除具体资源，在该字段设置后， finalizer 列表中的对象只能被删除，不能做其他操作。
当 metadata.deletionTimestamp 字段非空时，controller watch 对象并执行对应 finalizers 的动作，当所有动作执行完后，需要清空 finalizers ，之后 k8s 会删除真正想要删除的资源。
Operator finalizers 使用 Link to heading 介绍了 Finalizers 概念，那么我们来看看在 Operator 中如何使用，在 Operator Controller 中，最重要的逻辑就是 Reconcile 方法，finalizers 也是在 Reconcile 中实现的。要注意的是，设置了 Finalizers 会导致 k8s 的 delete 动作转为设置 metadata.</description></item><item><title>Kubernetes 实战-Leader 选举</title><link>https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/</link><pubDate>Thu, 12 Sep 2019 22:05:22 +0000</pubDate><guid>https://zdyxry.github.io/2019/09/12/Kubernetes-%E5%AE%9E%E6%88%98-Leader-%E9%80%89%E4%B8%BE/</guid><description>背景 Link to heading 最近手头上的 Cluster-API 的项目要告一段落， Cluster-API 发布了 v0.2.1 版本 ，正式放出了 YAML 配置文件，看到了点有意思的事情，觉得需要记录一下。
K8S Leader Link to heading 看过之前 K8S 实战系列的朋友应该记得我写过一篇 K8S 高可用部署的文章，在文章中只是讲了具体的操作步骤，没有提到 k8s 是如何保证自己多个组件之间协作的。
我们这里有一个 3 个master 节点的集群：
[root@node70 21:01:01 ~]$kubectl get node NAME STATUS ROLES AGE VERSION node70 Ready master 64d v1.15.0 node71 Ready master 64d v1.15.0 node72 Ready master 64d v1.15.0 我们都知道 k8s 核心组件，其中 apiserver 只用于接收 api 请求，不会主动进行各种动作，所以他们在每个节点都运行并且都可以接收请求，不会造成异常；kube-proxy 也是一样，只用于做端口转发，不会主动进行动作执行。
但是 scheduler, controller-manager 不同，他们参与了 Pod 的调度及具体的各种资源的管控，如果同时有多个 controller-manager 来对 Pod 资源进行调度，结果太美不敢看，那么 k8s 是如何做到正确运转的呢？</description></item><item><title>Kubernetes 实战-Cluster API v1alpha2</title><link>https://zdyxry.github.io/2019/08/23/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API-v1alpha2/</link><pubDate>Fri, 23 Aug 2019 20:13:50 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/23/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API-v1alpha2/</guid><description>背景 Link to heading 今天继续来聊一聊 cluster-api，在上周看 cluster-api-provider-vsphere 代码的时候吐槽过，cluster-api 最近因为 v1alpha2 版本的开发，变化太快，几乎每天都在变，那么我们就来看看 v1alpha2 具体做了什么。
Cluster-API v1alpha2 Link to heading 虽然目前 v1alpha2 还没有正式的 release，但是已经趋于稳定，且两个主要的 provider：aws 和 vsphere 都在进行 v1alpha2 版本的适配（最近每天都有 pr 更新）。我们先来了解下为啥要进行 v1alpha2 改动，改动的目的是啥。
在 v1alpha1 版本中，cluster-api 要求 provider 实现从节点置备到 k8s 部署的全套流程，cluster-api 自身只负责具体的 API 定义及相关控制，在 provider 实现上也不是一个标准的 Operator，（至少）我从概念的理解上比较吃力，每个 provider 需要实现对应 cluster 与 machine 的 actuator ，开发起来要求对 cluster-api 项目本身很熟悉。
其次，每个 provider 都包含了 k8s 集群部署的流程，虽然大部分实现最终都是使用 kubeadm 工具，但是使用方式千差万别，有 cloud-init、有 ssh 配合密钥、有 ssh 配合密码等等。这部分 provider 中的代码完全都是重复的，可以复用的。</description></item><item><title>cluster-api-provider-vsphere 源码阅读</title><link>https://zdyxry.github.io/2019/08/16/cluster-api-provider-vsphere-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Fri, 16 Aug 2019 20:57:25 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/16/cluster-api-provider-vsphere-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description>背景 Link to heading 上一篇博客讲了 Cluster-API 的相关概念，现在我们来找一个 provider 实现看看具体里面做了啥，因为对 vmware 产品中的概念比较熟悉，就找了 cluster-api-provider-vsphere 。
以下内容均对应 clusterapi v1alpha1 版本。
clusterctl 命令 Link to heading cluster-api provider 提供了命令行 clusterctl 用于给我们快速创建 bootstrap 集群用于创建目标 k8s 集群，我们来执行一下看看具体做了哪些工作：
root@yiran-workstation:~/go/src/github.com/kubernetes-sigs/cluster-api-provider-vsphere master ✗ $ clusterctl create cluster \ --provider vsphere \ --bootstrap-type kind \ --cluster ./out/management-cluster/cluster.yaml \ --machines ./out/management-cluster/machines.yaml \ --provider-components ./out/management-cluster/provider-components.yaml \ --addon-components ./out/management-cluster/addons.yaml \ --kubeconfig-out ./out/management-cluster/kubeconfig I0816 17:28:05.815156 14562 createbootstrapcluster.go:27] Preparing bootstrap cluster I0816 17:29:15.292547 14562 clusterdeployer.</description></item><item><title>Kubernetes 实战-Cluster API</title><link>https://zdyxry.github.io/2019/08/09/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API/</link><pubDate>Fri, 09 Aug 2019 19:41:49 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/09/Kubernetes-%E5%AE%9E%E6%88%98-Cluster-API/</guid><description>背景 Link to heading 在当前 Kubernetes 生态中，生命周期管理相关工具官方的有 kubeadm、kubespray（部署集群部分通过 kubeadm） ，开源社区还有很多其他的实现，我们可以通过这类工具来实现 k8s 集群的部署，升级，增删节点，但是使用一个工具的前提是：基础设施已经准备完成。只有当基础设施准备完成后，kubeadm 之类工具才可以正常工作。
当我们在部署 Kubernetes 集群时，节点可能在任何环境上，比如 AWS、OpenStack、Vsphere、Azure 等，那么想要自动化配置基础设施，通常我们根据自己的环境不同，编写不通的代码来支持我们的虚拟化（or 服务器）场景。
基础设施包括不限于：
OS 安装 Load Balance 配置 网络配置 IP 分配 … Cluster-API Link to heading Kubernetes 社区针对基础设施问题，发起了一个项目：cluster-api，目前处于 alpha1 版本，项目目标：
使用声明式API管理 Kubernetes 集群的生命周期 支持多种环境，私有云或公有云 使用社区中现有的工具完成相应功能 … 功能简述 Link to heading 无需创建额外基础设施前提下创建 bootstrap cluster 通过 bootstrap cluster 创建目标 k8s 集群 工作流程 Link to heading cluster-api 使用声明式 API 管理 k8s 集群，需要环境中先存在一个 k8s 集群，通常成为 bootstrap cluster，若不存在，也可通过提供的命令行工具 clusterctl 创建 bootstrap cluster 在 bootstrap cluster 中，部署 CRD 及相应的 cluster api 控制器及 provider 控制器 在 bootstrap cluster 中，开始创建我们真正想要创建的资源：k8s 集群 创建资源类型为 Cluster、Machine 或 MachineDeployment ，对应的控制器会自动为我们创建好虚拟机 在虚拟机创建完成后，通过 kubeadm 创建 k8s 集群 具体实现 Link to heading 虚拟机创建 Link to heading 目前看到的几个 Cluster-API Provider 项目实现，虚拟机均通过克隆的方式创建出来的。</description></item><item><title>Kubernetes 实战-平滑移除节点</title><link>https://zdyxry.github.io/2019/08/01/Kubernetes-%E5%AE%9E%E6%88%98-%E5%B9%B3%E6%BB%91%E7%A7%BB%E9%99%A4%E8%8A%82%E7%82%B9/</link><pubDate>Thu, 01 Aug 2019 23:07:29 +0000</pubDate><guid>https://zdyxry.github.io/2019/08/01/Kubernetes-%E5%AE%9E%E6%88%98-%E5%B9%B3%E6%BB%91%E7%A7%BB%E9%99%A4%E8%8A%82%E7%82%B9/</guid><description> 背景 Link to heading 自己玩 K8S 以来，搭建的环境没有10几套，也有5，6套了，当环境测试完成后，基本上直接删除掉了，也没有想着一直维护，最近在维护一个集群的时候，想要删除一个节点，发现自己一直不知道如何删除节点，特此记录。
平滑移除 Link to heading 获取节点列表 Link to heading kubectl get node 设置不可调度 Link to heading 由于节点目前处于正常工作状态，集群中新建资源还是有可能创建到该节点的，所以先将节点设置为不可调度：
kubectl cordon $node_name 将节点上资源调度到其他节点 Link to heading 目前集群已经不会分配新的资源在该节点上了，但是节点还运行着现有的业务，所以我们需要将节点上的业务分配到其他节点：
kubectl drain $node_name 注意：DaemonSet Pod 和 Static Pod 是不会在集群中其他节点重建的。
移除节点 Link to heading 当前集群中已经没有任何资源分配在节点上了，那么我们可以直接移除节点：
kubectl delete $node_name 至此，我们平滑移除了一个 k8s 节点。如果移除的是一个 master 节点，那么记得之后还要添加一个新的 master 节点到集群中，避免集群可靠性降低。
参考链接 Link to heading https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes</description></item><item><title>Kubernetes 实战-踩坑记录（持续更新）</title><link>https://zdyxry.github.io/2019/07/13/Kubernetes-%E5%AE%9E%E6%88%98-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</link><pubDate>Sat, 13 Jul 2019 09:34:45 +0000</pubDate><guid>https://zdyxry.github.io/2019/07/13/Kubernetes-%E5%AE%9E%E6%88%98-%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</guid><description>背景 Link to heading 在对现有服务进行容器话改造的过程中，随着对 K8S 使用程度越来越深，也渐渐的遇到了一些坑，所以开一篇博客，记录自己所遇到的坑，应该会长期更新。
更新记录 Link to heading 2019.07.13 02:00 来自加班中的 yiran 2019.07.19 06:52 早起不想去公司的 yiran coredns 无法解析域名 Link to heading 在 Kubernetes 环境中，使用 kubeadm 工具部署的集群，会自动部署 coredns 作为集群的域名服务，每当我们创建了自己的 service，都可以通过域名直接访问，不用再考虑自己多个 Pod 的 IP 不同如何连接的问题。
最近遇到多个环境出现无法解析域名的问题，具体现象如下：
集群部署完成后，部署 daemonset 资源，每个节点均运行一个 busybox； 在 busybox 中对 kubernetes 默认域名进行解析，查看解析结果。 正常情况应该是所有的 busybox 都可以正常解析才对，但是最近几个环境中均出现了 3 个node 中1个node 上的 pod 无法解析的问题，示例代码如下：
daemonset.yaml
apiVersion: &amp;#34;extensions/v1beta1&amp;#34; kind: &amp;#34;DaemonSet&amp;#34; metadata: name: &amp;#34;ds&amp;#34; namespace: &amp;#34;default&amp;#34; spec: template: metadata: labels: app: ds spec: tolerations: - key: node-role.</description></item><item><title>Kubernetes 实战-Pod 可用性</title><link>https://zdyxry.github.io/2019/06/26/Kubernetes-%E5%AE%9E%E6%88%98-Pod-%E5%8F%AF%E7%94%A8%E6%80%A7/</link><pubDate>Wed, 26 Jun 2019 21:57:31 +0000</pubDate><guid>https://zdyxry.github.io/2019/06/26/Kubernetes-%E5%AE%9E%E6%88%98-Pod-%E5%8F%AF%E7%94%A8%E6%80%A7/</guid><description>背景 Link to heading Kubernetes 作为一个容器编排系统，负责 Pod 生命周期管理，那么肯定会保证 Pod 的可用性，今天来说下 k8s Pod 可用性相关知识。
K8S 可用性相关参数 Link to heading k8s 核心组件有 kubelet,kube-apiserver,kube-scheduler,kube-controller-manager，通过阅读官方文档中相关参数说明，我摘取了认为跟可用性相关的参数，具体列表如下：
kubelet Link to heading &amp;ndash;housekeeping-interval duration Link to heading Default: 10s
Interval between container housekeepings.
kubelet 主动检测容器资源是否达到阈值的周期。
&amp;ndash;node-status-update-frequency duration Link to heading Default: 10s
Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller.
kubelet 上报到 kube-apiserver 频率。</description></item><item><title>Kubernetes 实战-Helm 包管理器</title><link>https://zdyxry.github.io/2019/06/21/Kubernetes-%E5%AE%9E%E6%88%98-Helm-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8/</link><pubDate>Fri, 21 Jun 2019 20:40:24 +0000</pubDate><guid>https://zdyxry.github.io/2019/06/21/Kubernetes-%E5%AE%9E%E6%88%98-Helm-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8/</guid><description>简介 Link to heading Helm 就是k8s 的包管理器 。常见的包管理器有：yum,apt,pip&amp;hellip;
包管理器基础功能有：
安装 依赖安装 升级 回滚 卸载 源管理 搜索 &amp;hellip; 基本概念 Link to heading Helm: Kubernetes的包管理工具，命令行同名
Tiller: Helmv2 的服务端，用于接收并处理 Helm 发送的请求，默认以 Deployment 形式部署在 k8s 集群中
Chart: Helm 包管理的基础单元，等同于 RPM
Repoistory: Helm的软件源仓库，是一个 Web 服务器，路径下除了响应的软件 Chart 包之外，维护了一个 index.yaml 用于索引
Release: Helm 安装在 Kubernetes 集群中的 Chart 实例
现状 Link to heading helm 截至06月20日最新稳定版本为 v2.14.1。
在05月16日发布了 v3.0 alpha 版本，根据相关文档描述，v2 无法平滑升级到 v3 版本。
注：存在部分小版本无法平滑升级情况。
helm v3 版本改进：</description></item><item><title>Kubernetes 实战-高可用集群部署</title><link>https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</link><pubDate>Sat, 15 Jun 2019 01:44:28 +0000</pubDate><guid>https://zdyxry.github.io/2019/06/15/Kubernetes-%E5%AE%9E%E6%88%98-%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</guid><description>准备工作 Link to heading 本文所有节点 OS 均为 CentOS 7.4 。
1.关闭 selinux Link to heading 所有节点执行：
[root@node211 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=disabled # SELINUXTYPE= can take one of three two values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy.</description></item><item><title>Kubernetes 实战-集群部署</title><link>https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</link><pubDate>Fri, 31 May 2019 21:22:51 +0000</pubDate><guid>https://zdyxry.github.io/2019/05/31/Kubernetes-%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</guid><description>背景 Link to heading 本来计划这周写一下如何定制 UEFI Linux 发行版的，但是计划赶不上变化，加上 UEFI 的改动比想象中的多，这周还是继续 k8s 系列好了。
说起来 k8s 写了 3 篇博客一直没有写集群部署相关的，一是当时对 k8s 了解不多，集群搭建大多是 GitHub 上的开源项目或 Rancher 快速搭建起来的；二是 k8s 官方工具 kubeadm 现在还有很多的不确定性，随着 v1.14 版本的发布，可用性大大提高，虽然还不支持 HA，但是要写一下了。
本文并不会介绍具体的部署步骤，望周知。
Kubernetes 主要组件 Link to heading 因为主要说集群部署相关的，因此只列出 Master 和 Node 的主要组件，k8s 内部资源不再罗列：
Master Link to heading apiserver： 集群中所有其他组件通过 apiserver 进行交互
scheduler： 按照 Pod 配置来对 Pod 进行节点调度
controller-manager：负责节点管理，资源的具体创建动作， desired state management 具体实行者
etcd：用于存储集群中数据的键值存储
Node Link to heading kubelet：处理 master 及其上运行的 node 之间的所有通信。它与容器运行时配合，负责部署和监控容器</description></item><item><title>Kubernetes 实战-镜像管理</title><link>https://zdyxry.github.io/2019/05/24/Kubernetes-%E5%AE%9E%E6%88%98-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</link><pubDate>Fri, 24 May 2019 07:35:08 +0000</pubDate><guid>https://zdyxry.github.io/2019/05/24/Kubernetes-%E5%AE%9E%E6%88%98-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</guid><description>镜像组织形式 Link to heading 镜像默认采用 OverlayFS 方式挂载，最终效果是将多个目录结构合并为一个。
其中 lowerdir 为只读路径，最右层级最深。最终容器运行时会将 lowerdir 和 upperdir 合并挂在为 merged，对应容器中的路径为 / 。 举例： 镜像 testadd:0.5 版本的层级挂载如下：
[root@node111 16:02:24 overlay2]$docker inspect testadd:0.5 |grep Dir &amp;#34;WorkingDir&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;WorkingDir&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;LowerDir&amp;#34;: &amp;#34;/var/lib/docker/overlay2/693c140b9c70744a7a6ce93de56d3ac7549dae84195cbfac3486062d1ceaccf1/diff&amp;#34;, &amp;#34;MergedDir&amp;#34;: &amp;#34;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/merged&amp;#34;, &amp;#34;UpperDir&amp;#34;: &amp;#34;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/diff&amp;#34;, &amp;#34;WorkDir&amp;#34;: &amp;#34;/var/lib/docker/overlay2/e2f2ad8332a9567ad28495b28342b5f5712218e235b0129435abfc3c781be957/work&amp;#34; 运行该容器后，可以看到多了一个 overlay 方式挂载的路径：
[root@node111 16:05:53 overlay2]$mount |grep overlay /dev/md127 on /var/lib/docker/overlay2 type ext4 (rw,relatime,data=ordered) overlay on /var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/3NA23BH5OMSSXWTHGPRS6YENB7:/var/lib/docker/overlay2/l/QQVS7UVPGRBVHRZOBDPMMO4EQM:/var/lib/docker/overlay2/l/E7HTYBVD5SXSZRLVTETODOIANT,upperdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/diff,workdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/work) 查看对应关系：
[root@node111 16:05:53 overlay2]$mount |grep overlay /dev/md127 on /var/lib/docker/overlay2 type ext4 (rw,relatime,data=ordered) overlay on /var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/3NA23BH5OMSSXWTHGPRS6YENB7:/var/lib/docker/overlay2/l/QQVS7UVPGRBVHRZOBDPMMO4EQM:/var/lib/docker/overlay2/l/E7HTYBVD5SXSZRLVTETODOIANT,upperdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/diff,workdir=/var/lib/docker/overlay2/e1378dc042b534fe00ca6f4565e399f30d56c81d434a30a6137cfae6b5355d00/work) [root@node111 16:06:07 overlay2]$docker inspect testadd:0.</description></item><item><title>Kubernetes 实战-日志处理</title><link>https://zdyxry.github.io/2019/05/17/Kubernetes-%E5%AE%9E%E6%88%98-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/</link><pubDate>Fri, 17 May 2019 08:50:44 +0000</pubDate><guid>https://zdyxry.github.io/2019/05/17/Kubernetes-%E5%AE%9E%E6%88%98-%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/</guid><description>基础日志处理 Link to heading 在 Kubernetes（简称 k8s）中，所有应用在 Pod（k8s 管理容器最小单位）中运行，标准处理方式为将日志打印到标准日志输出和标准错误输出，这样我们可以通过 kuberctl logs 关键字获取容器运行时日志，根据容器运行时的类型不同，日志保存路径也不同，以 Docker 为例，所有真实日志均在 /var/lib/docker/ 路径下，下面我们来看一个例子：
在 k8s 中创建一个 Pod，Pod 中指定打印当前时间到标准输出中：
apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: [/bin/sh, -c, &amp;#39;i=0; while true; do echo &amp;#34;$i: $(date)&amp;#34;; i=$((i+1)); sleep 1; done&amp;#39;] 运行该 Pod，通过 kubectl logs 获取当前 Pod 日志：
[root@node1 blog]# kubectl get pod |grep counter counter 1/1 Running 0 9s [root@node1 blog]# kubectl logs counter 0: Fri May 17 00:34:01 UTC 2019 1: Fri May 17 00:34:02 UTC 2019 2: Fri May 17 00:34:03 UTC 2019 3: Fri May 17 00:34:04 UTC 2019 4: Fri May 17 00:34:05 UTC 2019 5: Fri May 17 00:34:06 UTC 2019 6: Fri May 17 00:34:07 UTC 2019 7: Fri May 17 00:34:08 UTC 2019 8: Fri May 17 00:34:09 UTC 2019 9: Fri May 17 00:34:10 UTC 2019 10: Fri May 17 00:34:11 UTC 2019 11: Fri May 17 00:34:12 UTC 2019 12: Fri May 17 00:34:13 UTC 2019 这里如果使用 -f 选项，可以持续输出该 Pod 日志。</description></item><item><title>Kubernetes 实战-微服务</title><link>https://zdyxry.github.io/2019/05/14/Kubernetes-%E5%AE%9E%E6%88%98-%E5%BE%AE%E6%9C%8D%E5%8A%A1/</link><pubDate>Tue, 14 May 2019 22:32:43 +0000</pubDate><guid>https://zdyxry.github.io/2019/05/14/Kubernetes-%E5%AE%9E%E6%88%98-%E5%BE%AE%E6%9C%8D%E5%8A%A1/</guid><description>微服务 Link to heading 在 《Kubernetes In Action》的开始，先要了解 k8s 的需求来自于哪里，为什么我们需要 k8s。
引用维基百科解释：
微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent/Language agnostic) 的 API 集相互通信。
说一说我的理解，在项目早期，都是单体应用，随着功能越来越多，项目越来越大，虽然保证了部署运维的方便，但对于组内同学并不友好，新同学往往要在一坨代码中找自己想要的一点，本地修改提交跑 CI 也是以项目为单位的执行（前段时间 B 站不小心泄露的 Golang 代码就是这种）。当后续升级产品时，因为是以项目为最小粒度，哪怕无关代码，也要被迫进行代码升级，服务重启等操作，带来了额外的风险。
在 2014年，Martin Fowler 与 James Lewis 共同提出了微服务的概念，把单体应用改为通过接口产生的远程方法调用，将项目拆分，一个项目保证只做一件事情，独立部署和维护。
优点：
高度可维护和可测试 松散耦合 可独立部署 围绕业务能力进行组织 缺点：
服务数量大幅增加，部署维护困难 服务间依赖管理 服务故障处理 容器 Link to heading 那么我们提到了项目演进，在同一时间，容器技术的标准化统一也间接促成了微服务的推广（我猜的），Docker 在 2013.03.13 发布第一个版本，容器化技术让我们产品发布形态有了新的选择，开发直接将容器镜像发布，运维同学通过镜像进行产品上线，确保了环境的统一，无须纠结环境配置相关问题（不用吵架了）。
当我们产品发布采用容器化上线后，我们面临一些其他的问题了：
微服务追求的是将服务解耦，拆分为多个服务，那么最终发布形态对应的也是多个镜像，运维同学管理这些镜像之间的关系难度增加。 同时当镜像运行在不同的物理节点上，对计算资源和网络资源的要求是一致的，运维同学需要做到让镜像无感知。 当产品要进行升级时，镜像之间的依赖关系，故障切换等操作紧靠现有容器功能实现困难。 这么一看，与之前单体应用比也没好哪里去。于是有了各种容器编排系统，比如 Swarm，Mesos等等，但都不是很好用且各家一个标准，这时候老大哥谷歌发话了，我来把我们内部用了很多年要淘汰的东西拿出来给大家解决问题吧，于是有了 Kubernetes。
Kubernetes 功能上提供了解决微服务引入的问题，并更好的配合微服务去提供稳定高可用的统一容器化环境，具体如何解决的我们后续可以通过了解 Pod，ConfigMap，ReplicaSet 等功能去详细了解。
总结 Link to heading 可能是因为我考虑问题都是从运维角度去看的，网上的一些文章讲的带来的好处反而没看太清，可能作为 2C 产品，追求敏捷开发，产品不断快速迭代的目标比较适合，但是如果本身作为一个追求稳定可靠的 2B 产品来说，引入 k8s 带来的好处和维护 k8s 带来的成本真的要仔细的从产品层面考虑清楚，这里感觉跟具体的技术关系不大，而是说从产品面向的客户对象考虑，客户想要的是一个什么产品，而 k8s 作为一个还在不断（频繁）迭代的产品来说（可以去看看 release notes 的更新速度），后续若出现某些 API 不兼容等情况，如何去应对，感觉还是个灾难。</description></item><item><title>Kubernetes 实战-前言</title><link>https://zdyxry.github.io/2019/05/11/Kubernetes-%E5%AE%9E%E6%88%98-%E5%89%8D%E8%A8%80/</link><pubDate>Sat, 11 May 2019 21:00:55 +0000</pubDate><guid>https://zdyxry.github.io/2019/05/11/Kubernetes-%E5%AE%9E%E6%88%98-%E5%89%8D%E8%A8%80/</guid><description>Kubernetes 实战-前言 Link to heading 自从 Kubernetes 大热之后，一直没跟着版本去了解具体的功能及使用，只是大概了解其中概念。之前推特上有人推荐《Kubernetes In Action》这本书，说是对入门同学很友好，利用五一假期和这个周末，终于看完了，打算把学习过程和其中的一些想法记录下来。
《Kubernetes In Action》 Link to heading 就想推荐人说的那样，这本书作为 101 系列来说，是很称职的，你跟着官方示例做，90%以上都是可以成功的，且讲解门槛不高，推荐。
中文版是由七牛团队翻译的，虽然其中有一些小的翻译错误，但是整体读下来还是很顺畅的，不影响阅读，当然现在网上已经有原版资源，想读的同学可以去 SaltTiger 搜索下载。
本书章节较多，分为 3 部分：What？How？Why？首先讲解 k8s 及容器的基本概念，然后讲解 k8s 基本使用，最后介绍了一些 k8s 工作原理及最佳实践。
当你了解了什么是 k8s，及 k8s 能带来什么好处之后，我们去使用 k8s，从而真实的感受到 k8s 带来的便利，这种感觉是很美好的（表面美好的东西肯定会有某些限制），对我来说这种美好截止到第二部分就停止了。在第三部分中，我们在之前感受到的便利隐藏着很多没有考虑到的边界因素，也意味着我们从一个传统的单节点服务切换到微服务架构上，会新增很多需要去考虑的因素，如果 k8s 内部提供了解决方案，那么很简单，我们直接编写 YAML 就可以了，如果 k8s 没有解决方案呢？我不知道，可能当我真正体验过之后才能给出感受吧（即将发生的事）。
下一篇我们来说下什么是微服务。</description></item></channel></rss>